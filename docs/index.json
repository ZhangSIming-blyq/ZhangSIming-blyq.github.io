[{"body":"","link":"https://zhangsiming-blyq.github.io/","section":"","tags":null,"title":""},{"body":"学习内容 学习文档： 哈希表理论基础 收获总结 哈希表的基本应用: 哈希表是一种通过键值对存储数据的数据结构，具有O(1)的平均查找时间复杂度。其优势在于能够快速判断一个元素是否存在于集合中。通过哈希函数将键映射到存储位置，我们可以在常数时间内完成插入、删除和查找操作。这在需要频繁查找或去重的算法问题中非常有用。\n哈希函数的理解与设计: 哈希函数是哈希表的核心，负责将输入（键）映射到特定的存储桶位置。一个好的哈希函数应当能够均匀地分布输入数据，避免哈希碰撞的发生。哈希碰撞是指不同的输入映射到了同一存储桶，这会导致性能下降。常用的哈希函数设计包括除留余数法、乘积法和平方取中法等。\n哈希碰撞及其处理策略: 哈希碰撞是无法完全避免的，因此需要设计合理的碰撞处理策略。常见的方法有链地址法（即使用链表处理同一存储桶中的多个元素）和开放地址法（即在发生碰撞时寻找下一个可用位置存储）。链地址法的优点是简单易实现，且能处理多种数据类型；而开放地址法则可以更好地利用空间，但在负载因子较高时性能下降明显。\n数组作为特殊的哈希表: 在某些特定的算法问题中，我们可以使用数组来模拟哈希表。特别是当键值范围固定且有限时（如小写字母a-z），数组能够提供与哈希表类似的功能，但由于数组的访问速度更快且无哈希碰撞，因此在特定场景下表现更佳。例如，在统计字符频次的题目中，使用固定大小的数组可以大幅提升性能。\nset数据结构的应用: 在某些问题中，需要频繁检查某个元素是否已经存在，或需要确保数据不重复。此时，集合（set）是一种理想的数据结构。Go语言中没有原生的set结构，但可以通过map[T]struct{}来模拟实现，其中T是元素类型。由于struct{}{}在Go语言中不占用额外空间，因此这种方法既节省内存，又能够实现集合所需的所有操作（如插入、删除、查找）。\nmap作为哈希表的高级应用: Go语言中的map不仅可以用来模拟集合，还能够用于更复杂的场景，如计数器、查找表等。在处理组合问题时，map常用于记录不同组合出现的次数，并通过查找实现快速匹配。例如在\u0026quot;四数相加II\u0026quot;问题中，使用两个map分别记录前两组数的和与后两组数的和，从而在O(1)时间内完成匹配，大大提升了算法效率。\n拓展理解：哈希表在实际问题中的应用: 在实际开发中，哈希表广泛应用于缓存（如LRU缓存）、数据库索引、计数器统计等场景。学习哈希表的基础知识并理解其实现细节，能够帮助我们更好地应对这些实际问题。通过这次学习，我们不仅掌握了哈希表的理论知识，还在实践中理解了如何通过优化哈希函数、处理哈希碰撞等方法，提升算法的效率和可靠性。\n题目解析 题目1：第454题.四数相加II 题目描述: 给定四个整数数组nums1、nums2、nums3和nums4，统计有多少个四元组(i, j, k, l)使得nums1[i] + nums2[j] + nums3[k] + nums4[l] = 0。为了简化问题，假设所有的四个数组长度相同，且长度不超过500。\n示例:\n1输入: nums1 = [1, 2], nums2 = [-2,-1], nums3 = [-1, 2], nums4 = [0, 2] 2输出: 2 3解释: 两个符合条件的四元组为: 4(0, 0, 0, 1) -\u0026gt; nums1[0] + nums2[0] + nums3[0] + nums4[1] = 1 + (-2) + (-1) + 2 = 0 5(1, 1, 0, 0) -\u0026gt; nums1[1] + nums2[1] + nums3[0] + nums4[0] = 2 + (-1) + (-1) + 0 = 0 解法总结: 该题目要求我们找到所有满足条件的四元组。直接暴力枚举四个数组中的元素组合会导致O(n^4)的时间复杂度，无法在合理时间内解决问题。因此，利用哈希表的快速查找特性可以将问题转化为两两分组求和。首先，我们遍历nums1和nums2，计算每对元素的和，并将其存储在哈希表中，键为和，值为出现的次数。然后遍历nums3和nums4，计算它们的和并检查哈希表中是否存在该和的相反数，如果存在则说明找到了符合条件的四元组，结果增加该和的出现次数。通过这种方法，时间复杂度降低到了O(n^2)。\n1func fourSumCount(nums1 []int, nums2 []int, nums3 []int, nums4 []int) int { 2\tresultMap := map[int]int{} 3\tresult := 0 4 5\t// 1. 第一部分for循环循环nums1，nums2，之和作为key，value为组合次数 6\tfor _, v := range nums1 { 7\tfor _, sV := range nums2 { 8\tresultMap[v+sV]++ 9\t} 10\t} 11 12\t// 2. 第二部分for循环直接比对resultMap是否有答案 13\tfor _, v := range nums3 { 14\tfor _, sV := range nums4 { 15\tif ssV, ok := resultMap[0-(v+sV)]; ok { 16\tresult += ssV 17\t} 18\t} 19\t} 20\treturn result 21} 时间复杂度: O(n^2)，其中n是每个数组的长度。我们首先计算两两数组组合的和，这需要O(n^2)的时间，然后在第二部分查找哈希表也需要O(n^2)的时间。\n空间复杂度: O(n^2)，用于存储前两组数的和的哈希表。这个哈希表最多存储n^2个不同的和，因此空间复杂度为O(n^2)。\n题目2：383. 赎金信 题目描述: 给定一个ransomNote字符串和一个magazine字符串，判断ransomNote能否由magazine里面的字符构成。如果可以，返回true；否则返回false。magazine中的每个字符只能在ransomNote中使用一次。\n示例:\n1输入: ransomNote = \u0026#34;a\u0026#34;, magazine = \u0026#34;b\u0026#34; 2输出: false 3 4输入: ransomNote = \u0026#34;aa\u0026#34;, magazine = \u0026#34;ab\u0026#34; 5输出: false 6 7输入: ransomNote = \u0026#34;aa\u0026#34;, magazine = \u0026#34;aab\u0026#34; 8输出: true 解法总结: 这道题的关键在于判断magazine中是否有足够的字符可以构成ransomNote。解法是遍历magazine字符串并统计每个字符的出现次数，然后遍历ransomNote字符串，检查每个字符是否可以在magazine中找到并且次数足够。如果某个字符的数量不够，则直接返回false。如果所有字符都满足条件，则返回true。通过使用一个固定大小的数组来记录字符的出现次数，可以在O(1)时间内完成查找和更新操作，这种方法有效地利用了数组作为特殊哈希表的特性。\n1func canConstruct(ransomNote string, magazine string) bool { 2\tmagazineMap := make([]int, 26) 3 4\t// 1. magazine只能用一次，magazine映射到map，+1 5\tfor _, v := range magazine { 6\tmagazineMap[v-rune(\u0026#39;a\u0026#39;)]++ 7\t} 8 9\t// 2. 循环ransomNote进行-1，如果减完了小于0，则返回false；最后返回true 10\tfor _, v := range ransomNote { 11\tmagazineMap[v-rune(\u0026#39;a\u0026#39;)]-- 12\tif magazineMap[v-rune(\u0026#39;a\u0026#39;)] \u0026lt; 0 { 13\treturn false 14\t} 15\t} 16\treturn true 17} 时间复杂度: O(n + m)，其中n和m分别是ransomNote和magazine的长度。我们需要分别遍历这两个字符串来统计和检查字符的出现次数。\n空间复杂度: O(1)，因为我们使用了一个固定大小的数组（长度为26）来存储字符的频次，不随输入大小变化。\n题目3：第15题. 三数之和 题目描述: 给定一个包含n个整数的数组nums，判断nums中是否存在三个元素a，b，c，使得a + b + c = 0。请找出所有和为0且不重复的三元组。\n示例:\n1输入: nums = [-1, 0, 1, 2, -1, -4] 2输出: [[-1, 0, 1], [-1, -1, 2]] 解法总结: 该题目要求找出所有和为0的三元组，且不能有重复的三元组。暴力解法会尝试每个三元组合，时间复杂度为O(n^3)，效率低下。为提高效率，可以首先对数组进行排序，然后利用双指针法进行查找：固定一个数作为基准，剩下的两个数通过左右双指针向中间搜索，确保找到的三元组和为零。排序能够帮助我们轻松跳过重复的元素，避免产生重复的结果。最终的时间复杂度为O(n^2)，远优于暴力解法。\n暴力解法: 注意，暴力解法虽然正确，但是会超时；第二种双指针写法更优\n1func threeSum(nums []int) [][]int { 2\tcheckMap := map[int]int{} 3\tresult := [][]int{} 4\tseenMap := map[string]struct{}{} 5 6\t// 1. 第一轮循环，从头扫描到尾，把对应的值放checkMap进去，key是0-v，value是index 7\tfor index, v := range nums { 8\tcheckMap[0-v] = index 9 10\t} 11 12\t// 2. 第二轮循环，双指针循环(不重复)，同时需要相加判断checkMap是否有，index是否重合 13\tfor i := 0; i \u0026lt; len(nums); i++ { 14\tfor j := i + 1; j \u0026lt; len(nums); j++ { 15\tif v, ok := checkMap[nums[i]+nums[j]]; ok { 16\tif v != i \u0026amp;\u0026amp; v != j { 17\ttmp := []int{nums[v], nums[i], nums[j]} 18\tsort.Ints(tmp) 19\tif _, ok := seenMap[fmt.Sprintf(\u0026#34;%d%d%d\u0026#34;, tmp[0], tmp[1], tmp[2])]; !ok { 20\tresult = append(result, tmp) 21\tseenMap[fmt.Sprintf(\u0026#34;%d%d%d\u0026#34;, tmp[0], tmp[1], tmp[2])] = struct{}{} 22\t} 23\t} 24\t} 25\t} 26\t} 27\treturn result 28} 对撞双指针解法\n1import \u0026#34;sort\u0026#34; 2 3func threeSum(nums []int) [][]int { 4\tresult := [][]int{} 5 6\t// 0. 排序数组(默认由小到大)，这个是后面对撞双指针的前提 7\tsort.Ints(nums) 8 9\t// 1. 由于要找三个数，在不重复的情况下，i应该\u0026lt;len-2 10\tfor i := 0; i \u0026lt; len(nums)-2; i++ { 11\t// 2. 并且排序后如果已经\u0026gt;0了都直接跳过 12\tif nums[i] \u0026gt; 0 { 13\tbreak 14\t} 15 16\t// 3. 去重a 17\tif i \u0026gt;= 1 \u0026amp;\u0026amp; nums[i] == nums[i-1] { 18\tcontinue 19\t} 20 21\t// 4. 对撞双指针找b和c 22\tleft := i + 1 23\tright := len(nums) - 1 24 25\tfor left \u0026lt; right { 26\tb, c := nums[left], nums[right] 27\tif nums[i]+b+c == 0 { 28\ttmp := []int{nums[i], b, c} 29\tresult = append(result, tmp) 30 31\t// 去重b 32\tfor left \u0026lt; right \u0026amp;\u0026amp; nums[left] == b { 33\tleft++ 34\t} 35\t// 去重c 36\tfor left \u0026lt; right \u0026amp;\u0026amp; nums[right] == c { 37\tright-- 38\t} 39\t} else if nums[i]+b+c \u0026lt; 0 { 40\tleft++ 41\t} else { 42\tright-- 43\t} 44\t} 45\t} 46\treturn result 47} 时间复杂度: O(n^2)。排序需要O(n log n)，双指针查找需要O(n^2)。\n空间复杂度: O(1)。除了排序所需的空间外，不需要额外的空间来存储数据。\n题目4：第18题. 四数之和 题目描述: 给定一个包含n个整数的数组nums和一个目标值target，判断nums中是否存在四个元素a，b，c，d，使得a + b + c + d的和与target相等。请找出所有符合条件且不重复的四元组。\n示例:\n1输入: nums = [1, 0, -1, 0, -2, 2], target = 0 2输出: [[-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2]] 解法总结: 该题目是“三数之和”的扩展版，要求找到四个数的和等于目标值的所有不重复四元组。解法与“三数之和”类似，可以通过排序和双指针法来解决。首先对数组进行排序，然后固定两个数，剩下的两个数通过左右双指针进行搜索，确保找到的四元组和为目标值。为了避免重复解，固定的数以及双指针搜索过程中都需要进行去重处理。最终时间复杂度为O(n^3)，其中n是数组的长度。\n对撞双指针解法\n1import \u0026#34;sort\u0026#34; 2 3func fourSum(nums []int, target int) [][]int { 4\tresult := [][]int{} 5\t// 双指针解法 6\t// 0. 排序 7\tsort.Ints(nums) 8 9\t// 1. a+b+c+d 第一个循环a 10\tfor i := 0; i \u0026lt; len(nums)-3; i++ { 11\t// a去重 12\ta := nums[i] 13\tif i \u0026gt; 0 \u0026amp;\u0026amp; a == nums[i-1] { 14\tcontinue 15\t} 16 17\t// 2. 往后循环b 18\tfor j := i + 1; j \u0026lt; len(nums)-2; j++ { 19\tb := nums[j] 20\tif j \u0026gt; i+1 \u0026amp;\u0026amp; b == nums[j-1] { 21\tcontinue 22\t} 23 24\t// 3. 双指针c，d(left, right) 25\tleft := j + 1 26\tright := len(nums) - 1 27 28\tfor left \u0026lt; right { 29\tc := nums[left] 30\td := nums[right] 31\tsum := a + b + c + d 32\tif sum == target { 33\tresult = append(result, []int{a, b, c, d}) 34\t// 如果下一个数字和已经加入result的相同，就直接按照方向跳过 35\tfor left \u0026lt; right \u0026amp;\u0026amp; nums[left] == nums[left+1] { 36\tleft++ 37\t} 38\tfor left \u0026lt; right \u0026amp;\u0026amp; nums[right] == nums[right-1] { 39\tright-- 40\t} 41\t// 双指针同时移动 42\tleft++ 43\tright-- 44\t} else if sum \u0026lt; target { 45\tleft++ 46\t} else { 47\tright-- 48\t} 49\t} 50\t} 51\t} 52\treturn result 53} 时间复杂度: O(n^3)。排序需要O(n log n)，三层嵌套循环分别是O(n^3)。\n空间复杂度: O(1)。除了排序所需的空间外，不需要额外的空间来存储数据。\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/programmercarl/day7/","section":"post","tags":["algorithm","中文"],"title":"【算法刷题系列】第7天 第454题.四数相加II, 383. 赎金信, 第15题. 三数之和，第18题. 四数之和"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/algorithm/","section":"tags","tags":null,"title":"algorithm"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/algorithm/","section":"categories","tags":null,"title":"algorithm"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://zhangsiming-blyq.github.io/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/%E4%B8%AD%E6%96%87/","section":"tags","tags":null,"title":"中文"},{"body":"学习内容 学习文档：\n哈希表理论基础\n收获总结 Go中的rune类型: 在Go语言中，rune是一种别名类型，它表示一个Unicode码点，即一个整数，通常用于表示字符。Go的字符串是以字节数组的形式存储的，因此在处理多字节字符（如汉字、表情符号等）时，直接使用索引访问可能会得到不完整的字符。rune类型解决了这个问题，它能够正确处理和表示多字节的Unicode字符。这对于处理包含非ASCII字符的字符串时非常重要，尤其在国际化或多语言支持的应用中。通过这次学习，我们掌握了如何使用rune遍历字符串，确保对每个字符进行准确的操作。\n数组作为简单高效的哈希表: 在解决某些特定范围的哈希问题时，数组可以作为一种极为简单和高效的哈希表实现方式。特别是在字符计数和映射问题中，如果字符集固定（例如，字母a到z），我们可以使用一个固定长度的整数数组（如[26]int）来记录每个字符的出现次数。数组的索引直接对应字符的ASCII值减去偏移量，这使得字符查找和更新操作都可以在常数时间内完成。学习这部分内容让我们理解了如何在时间复杂度和空间复杂度上进行权衡，选择最合适的数据结构。\nGo中使用map实现set: 在Go语言中，集合(set)这一数据结构并没有直接实现，但我们可以通过map来间接实现。使用map[T]struct{}这种方式来模拟集合，其中T是元素类型，struct{}是一种不占用内存的零大小结构体。通过map键的唯一性，保证集合中的元素不重复，同时由于值类型为空结构体，节省了内存空间。我们还学习了如何利用delete函数从map中删除元素，进而动态地管理集合内容。这种技巧在需要高效去重和查找操作的场景中非常有用。\nGo中struct{}{}占用空间最小: 在Go语言中，struct{}{}是一个空的结构体类型，占用空间为零。在使用map实现集合时，我们可以将map的值类型定义为struct{}{}，这样可以在实现集合功能的同时，极大地节省内存空间。与其他语言的集合实现相比，这种方式更为轻量级，适合内存受限的场景。通过这种学习，我们进一步理解了Go语言在性能优化方面的设计思想，以及如何在日常编程中应用这些知识提高程序的效率。\n两数之和的双指针与哈希表解法: 对于两数之和问题，如果数组是有序的，我们可以利用双指针（也称为对撞指针）策略，从数组两端同时向中间移动，根据当前和与目标值的比较结果决定指针的移动方向，从而在O(n)时间复杂度内找到解。如果数组是无序的，则可以使用哈希表来记录已遍历过的数值及其索引。在遍历数组时，通过查找哈希表，快速判断是否存在与当前元素互补的数值，并在常数时间内找到目标组合。这些方法各有优劣，双指针法简单直观但只能用于有序数组，而哈希表法适用于无序数组且查找效率更高。通过学习这两种解法，我们理解了不同场景下算法选择的依据。\n扩展知识: 在这次学习中，还扩展了关于哈希表、双指针法在不同算法问题中的广泛应用。深入理解了哈希表在解决查找问题中的效率优势，以及双指针在排序数组中优化搜索过程的应用场景。这些知识不仅在理论层面增强了我们的算法理解，也为我们实际解决编程问题提供了多种思路和工具。\n题目解析 题目1：242. 有效的字母异位词 题目描述: 给定两个字符串 s 和 t，编写一个函数来判断 t 是否是 s 的字母异位词。字母异位词是指由相同的字母组成，但排列顺序不同的字符串。注意，字符串中的字母全部为小写字母。\n示例:\n1输入: s = \u0026#34;anagram\u0026#34;, t = \u0026#34;nagaram\u0026#34; 2输出: true 3 4输入: s = \u0026#34;rat\u0026#34;, t = \u0026#34;car\u0026#34; 5输出: false 解法总结: 该题目要求判断两个字符串是否为字母异位词，核心在于统计两个字符串中各字符的出现次数。如果两个字符串中各字符出现的次数完全相同，那么这两个字符串就是字母异位词。解法采用了固定长度的整数数组来记录字符的频次，其中每个字符的频次通过其ASCII码的偏移计算得出。在遍历第一个字符串时，对应字符的计数器加一；在遍历第二个字符串时，对应字符的计数器减一。最后，如果数组中的所有值都为零，则说明两个字符串是字母异位词。这个方法利用了数组的高效查找特性，使得整个过程的时间复杂度保持在O(n)。\n1func isAnagram(s string, t string) bool { 2\tvar tmp [26]int 3\t// 1. 映射s到数组+1(数组是简单高效的哈希表) 4\tfor _, c := range s { 5\ttmp[c-rune(\u0026#39;a\u0026#39;)]++ 6\t} 7\t// 2. 映射t到数组-1 8\tfor _, c := range t { 9\ttmp[c-rune(\u0026#39;a\u0026#39;)]-- 10\t} 11 12\t// 3. 检查是否有元素不为0 13\tfor _, c := range tmp { 14\tif c != 0 { 15\treturn false 16\t} 17\t} 18\treturn true 19} 时间复杂度: O(n)，其中n是字符串的长度。算法主要耗时在两次遍历字符串上，每次遍历的时间复杂度均为O(n)。\n空间复杂度: O(1)。由于使用了固定大小的数组来存储字符频次，无论字符串的长度如何变化，空间复杂度始终保持常数。\n题目2：349. 两个数组的交集 题目描述: 给定两个数组，编写一个函数来计算它们的交集。返回结果中的每个元素应是唯一的，结果可以是任意顺序。\n示例:\n1输入: nums1 = [1,2,2,1], nums2 = [2,2] 2输出: [2] 3 4输入: nums1 = [4,9,5], nums2 = [9,4,9,8,4] 5输出: [9,4] 解法总结: 该题目的目标是找出两个数组中的公共元素，并确保结果中的元素不重复。通过使用两个map结构，一个用于存储第一个数组中的元素，另一个用于存储交集结果。首先，将第一个数组中的所有元素存入一个map中，以去重。然后，遍历第二个数组，检查每个元素是否存在于第一个数组的map中，如果存在，则将该元素添加到结果集并从map中删除，以确保结果集中元素的唯一性。这种方法利用了哈希表的快速查找特性，能够高效地找出两个数组的交集。\n1func intersection(nums1 []int, nums2 []int) []int { 2\tnums1Map := map[int]struct{}{} 3\tresult := []int{} 4 5\t// 1. 将nums1编入map,并且去重 6\tfor _, v := range nums1 { 7\tif _, ok := nums1Map[v]; !ok { 8\tnums1Map[v] = struct{}{} 9\t} 10\t} 11 12\t// 2. 循环nums2，如果nums1有就放入结果集 13\tfor _, v := range nums2 { 14\tif _, ok := nums1Map[v]; ok { 15\tresult = append(result, v) 16 delete(nums1Map, v) 17\t} 18\t} 19 20\treturn result 21 22} 时间复杂度: O(n + m)，其中n和m分别是两个数组的长度。时间复杂度主要耗费在两个数组的遍历和哈希表的查找操作上。\n空间复杂度: O(min(n, m))。空间复杂度主要由存储第一个数组的map以及结果集map决定，取决于两个数组中较小的那个。\n题目3：202. 快乐数 题目描述: 编写一个算法来判断一个数n是否是快乐数。快乐数定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程，直到这个数变为1，或是无限循环但始终变不到1。如果可以变为1，则这个数是快乐数。\n示例:\n1输入: 19 2输出: true 3解释: 41² + 9² = 82 58² + 2² = 68 66² + 8² = 100 71² + 0² + 0² = 1 解法总结: 该题目通过不断迭代计算数字的平方和，判断其是否最终收敛到1。为了防止进入无限循环，使用一个map来记录每次迭代后的数字，如果某个数字已经出现过，则表示出现了循环，当前数字不可能是快乐数。具体实现上，首先定义一个辅助函数happyNum来计算平方和，然后在主函数中进行迭代判断，直到数字变为1（返回true）或者出现循环（返回false）。\n1func isHappy(n int) bool { 2\tresultMap := map[int]struct{}{} 3\t// 1. 无限循环计算快乐数 4\tfor { 5\t// 2. 计算快乐数 6\tn = happyNum(n) 7\t// 3. 如果快乐数等于1，直接返回true 8\tif n == 1 { 9\treturn true 10\t} 11 12\t// 4. 如果相同的快乐数出现了两次，返回false 13\tif _, ok := resultMap[n]; ok { 14\treturn false 15\t} 16\tresultMap[n] = struct{}{} 17\t} 18} 19 20func happyNum(n int) int { 21\tsum := 0 22\t// 只适合两位数的 23\tfor n \u0026gt; 0 { 24\tsum += (n % 10) * (n % 10) 25\tn = n / 10 26\t} 27 28\treturn sum 29} 时间复杂度: O(logn)。每次迭代中数字的位数减少，最终将收敛到1或出现循环。由于数字的位数与它的对数成正比，因此时间复杂度是O(logn)。\n空间复杂度: O(logn)。用于记录中间结果的哈希表最多存储数字的平方和的位数，空间复杂度为O(logn)。\n题目4：1. 两数之和 题目描述: 给定一个整数数组nums和一个目标值target，请你在该数组中找出和为目标值的那两个整数，并返回他们的数组下标。你可以假设每种输入只会对应一个答案。但是，数组中同一个元素不能使用两遍。\n示例:\n1输入: nums = [2,7,11,15], target = 9 2输出: [0,1] 3解释: 因为 nums[0] + nums[1] = 2 + 7 = 9 解法总结: 对于这道题目，可以采用两种解法：双指针法和哈希表法。如果数组是有序的，可以使用双指针法，通过从两端向中间移动指针，根据当前和与目标值的关系来调整指针，直到找到两个数之和等于目标值的位置。这种方法的时间复杂度为O(n)。如果数组是无序的，可以使用哈希表法。遍历数组的同时，将每个元素及其对应的索引存入哈希表。在遍历过程中，检查哈希表中是否存在目标值与当前元素的差值，如果存在，说明找到了两个数之和等于目标值的位置。这种方法的时间复杂度为O(n)，但空间复杂度也为O(n)。 双指针版本\n1func twoSum(nums []int, target int) []int { 2\t// 1. 定义对撞双指针 3\t// 2. 暴力 4\tfor i := 0; i \u0026lt; len(nums); i++ { 5\tfor j := i + 1; j \u0026lt; len(nums); j++ { 6\tsum := nums[i] + nums[j] 7\tif sum == target { 8\treturn []int{i, j} 9\t} 10\t} 11\t} 12 13\treturn []int{} 14} 哈希表版本\n1func twoSum(nums []int, target int) []int { 2\tresultMap := map[int]int{} 3 4\tfor index, v := range nums { 5\t// 将另一半索引到map里面! 6\tif tarIndex, ok := resultMap[target-v]; ok { 7\treturn []int{index, tarIndex} 8\t} else { 9\tresultMap[v] = index 10\t} 11\t} 12\treturn []int{} 13} 时间复杂度: 双指针法的时间复杂度为O(n)，哈希表法的时间复杂度也为O(n)。\n空间复杂度: 双指针法的空间复杂度为O(1)，哈希表法的空间复杂度为O(n)。\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/programmercarl/day6/","section":"post","tags":["algorithm","中文"],"title":"【算法刷题系列】第6天 242. 有效的字母异位词, 349. 两个数组的交集, 202. 快乐数, 1. 两数之和"},{"body":"收获总结 虚拟头节点的使用：虚拟头节点（Dummy Head）是处理链表问题的一大利器，尤其在增删节点操作中。通过引入虚拟头节点，可以避免处理链表头部时遇到的特殊情况，如删除第一个节点或在第一个节点前插入新节点。这不仅简化了代码，还减少了需要额外考虑的边界条件。例如，在处理 19. 删除链表的倒数第 N 个结点 时，虚拟头节点能够让双指针操作统一化，避免对头节点的单独处理。\n双指针技术：双指针技术是链表问题中的核心工具，特别是在处理链表长度不一致、链表中查找特定节点、或检测链表是否存在环时。双指针通常有两种应用方式：\n快慢指针：通过让一个指针每次走两步（快指针），另一个指针每次走一步（慢指针），这种方法能有效检测链表中的环（如 142. 环形链表 II）。当快慢指针相遇时，表明链表中存在环。随后，通过调整指针，可以精确找到环的起点。 同步指针：在链表相交问题中（如 面试题 02.07. 链表相交），同步指针的应用非常巧妙。让两个指针分别从两个链表的头开始遍历，当其中一个指针走到链表末尾时切换到另一个链表的头部，最终两个指针会在相交点相遇。这种方法的妙处在于，它平衡了链表长度的差异，使得指针在正确的位置相遇。 内置数据结构的应用：在处理链表问题时，合理使用内置的数据结构（如 map）可以大大提高解决问题的效率。例如，在检测链表是否有环时，使用哈希表可以记录已经访问过的节点，一旦再次访问到相同的节点，就可以立即判定链表中存在环，并找到环的入口。这种方法尽管增加了空间复杂度，但通常能够显著降低时间复杂度，是时间换空间的一种常见手段。\n画图和理清思路：链表操作往往涉及多步节点指针的调整，容易出现操作顺序错误导致链表断裂或死循环的问题。因此，在进行复杂链表操作之前，通过画图来理清每一步的指针变动，明确节点间的连接关系，是非常必要的。比如在解决 24. 两两交换链表中的节点 问题时，通过画图可以清晰地看到每次交换后的链表结构，有助于正确实现节点交换。\n操作的标准化和优化：在链表操作中，保持节点连接的正确性是最重要的。在实现代码时，应尽量避免无效或重复的操作。例如，在删除节点时，应确保先处理前驱节点的 next 指针，再释放目标节点的内存。同时，在实际开发中，掌握一些标准化的代码模板（如增删节点的通用代码框架）能够帮助减少错误，提高代码的复用性和开发效率。\n题目解析 题目1：24. 两两交换链表中的节点 题目描述: 给定一个链表，两两交换其中相邻的节点，并返回交换后的链表。你不能只是单纯地改变节点内部的值，而是需要实际进行节点交换。\n示例:\n1输入: head = [1,2,3,4] 2输出: [2,1,4,3] 解法总结: 这个问题要求我们在链表中两两交换相邻的节点。为了简化操作，可以引入一个虚拟头节点（dummy head），它指向原链表的头节点，这样可以统一对头节点和后续节点的处理。然后使用双指针遍历链表，分别指向当前待交换的两个节点及其前驱节点。关键在于每次交换时，需要提前保存第二个节点的下一个节点，以防止链表断裂。假设1-\u0026gt;2-\u0026gt;3-\u0026gt;4是待交换的两个节点，交换过程如下：\n将pre指向2 将2指向1 将1指向3 将pre指向1 将cur指向3 重复上述步骤，直到cur或cur.Next为空 代码实现:\n1/** 2 * Definition for singly-linked list. 3 * type ListNode struct { 4 * Val int 5 * Next *ListNode 6 * } 7 */ 8func swapPairs(head *ListNode) *ListNode { 9\t// 1. 加虚拟头，定义双指针 10\tdummy := \u0026amp;ListNode{} 11\tdummy.Next = head 12\tcur := head 13\tpre := dummy 14 15\t// 2. 退出循环条件 16\tfor cur != nil \u0026amp;\u0026amp; cur.Next != nil { 17\t// 3. pre --\u0026gt; 2 18\tpre.Next = cur.Next 19 20\t// 4. 2 --\u0026gt; 1, 这里因为cur.Next.Next被改动了，所以要提前把原来的数据保存起来 21\ttmp := cur.Next.Next 22\tcur.Next.Next = cur 23 24\t// 5. 1 --\u0026gt; 3 25\tcur.Next = tmp 26 27\t// 6. pre cur 指针移动 28\tpre = cur 29\tcur = tmp 30\t} 31 32\treturn dummy.Next 33} 时间复杂度: O(n)，因为我们需要遍历整个链表，n 是链表的长度。\n空间复杂度: O(1)，因为只使用了常量级别的额外空间。\n题目2：19. 删除链表的倒数第 N 个结点 题目描述: 给定一个链表，删除链表的倒数第 n 个节点，并返回链表的头节点。要求算法的时间复杂度为 O(n)。\n示例:\n1输入: head = [1,2,3,4,5], n = 2 2输出: [1,2,3,5] 解法总结: 这道题的核心是如何在只遍历一次链表的情况下删除倒数第 n 个节点。使用双指针技术是一个非常有效的办法。通过让其中一个指针 fast 先前进 n 步，然后再让 fast 和另一个指针 slow 同时前进。当 fast 到达链表末尾时，slow 指针正好位于需要删除节点的前一个节点。这个过程只需要一次遍历，因而满足 O(n) 的时间复杂度要求。\n具体步骤:\n初始化虚拟头节点和双指针:\n创建一个虚拟头节点 dummy，使 dummy.Next 指向 head，并初始化两个指针 fast 和 slow，均指向 dummy。 让 fast 先走 n 步:\n通过一个循环，让 fast 指针向前移动 n 步，这样 fast 和 slow 之间的距离正好是 n。 同时移动 fast 和 slow:\n当 fast 指向链表末尾（fast.Next 为 nil）时，slow 刚好指向需要删除节点的前一个节点。 删除节点:\n修改 slow.Next 指向 slow.Next.Next，从而删除目标节点。 返回结果:\n最终返回 dummy.Next 作为新的链表头。 这种方法确保了在遍历链表一次的情况下，准确找到并删除倒数第 n 个节点。\n代码实现:\n1/** 2 * Definition for singly-linked list. 3 * type ListNode struct { 4 * Val int 5 * Next *ListNode 6 * } 7 */ 8func removeNthFromEnd(head *ListNode, n int) *ListNode { 9\t// 1. 定义双指针，增加dummy头 10\tdummy := \u0026amp;ListNode{ 11\tNext: head, 12\t} 13\tfast := dummy 14\tslow := dummy 15 16\t// 2. 检查n的合法性 17\t// 3. fast先走 18\tfor i := 0; i \u0026lt; n; i++ { 19\tfast = fast.Next 20\t} 21 22\t// 4. fast和slow一起走，fast的下一个是nil的时候slow就到了该删除的值的前一位! 23\tfor fast.Next != nil { 24\tfast = fast.Next 25\tslow = slow.Next 26\t} 27 28\tslow.Next = slow.Next.Next 29\treturn dummy.Next 30} 时间复杂度: O(n)，因为需要遍历链表一次，n 是链表的长度。\n空间复杂度: O(1)，因为只使用了常量级别的额外空间。\n题目3：面试题 02.07. 链表相交 题目描述: 给定两个单链表，找出它们相交的起始节点。如果两个链表没有交点，返回 null。\n示例:\n1输入: intersectVal = 8, listA = [4,1,8,4,5], listB = [5,0,1,8,4,5] 2输出: Intersected at \u0026#39;8\u0026#39; 代码实现:\n解法1: 该解法通过分别计算两个链表的长度，然后让较长的链表先行走差值步数，最后再同步遍历两个链表，以找到它们的相交节点。\n具体步骤:\n计算两个链表的长度:\n分别遍历 headA 和 headB，计算出两个链表的长度 lengthA 和 lengthB。 对齐两个链表的起点:\n根据两个链表的长度差，调整较长链表的起始点，使两个链表在剩余长度上对齐。具体操作是让较长链表先走 |lengthA - lengthB| 步。 同步遍历两个链表:\n从新的起点开始，同时遍历两个链表，当 headA 与 headB 相等时，返回该节点，这个节点即为两个链表的相交节点。 这种方法依赖于计算链表的长度，并通过调整起点来找到相交节点。\n1/** 2 * Definition for singly-linked list. 3 * type ListNode struct { 4 * Val int 5 * Next *ListNode 6 * } 7 */ 8func getIntersectionNode(headA, headB *ListNode) *ListNode { 9\t// 无增删改查操作可以不用虚拟头 10\t// 1. 分别求出headA, headB的长度 11\ttmpHeadA := headA 12\tlengthA := 0 13\tfor tmpHeadA != nil { 14\ttmpHeadA = tmpHeadA.Next 15\tlengthA++ 16\t} 17 18\ttmpHeadB := headB 19\tlengthB := 0 20\tfor tmpHeadB != nil { 21\ttmpHeadB = tmpHeadB.Next 22\tlengthB++ 23\t} 24 25\t// 2. 长的先走差值 26\tif lengthA \u0026gt;= lengthB { 27\tfor i := 0; i \u0026lt; lengthA-lengthB; i++ { 28\theadA = headA.Next 29\t} 30\t} else { 31\tfor i := 0; i \u0026lt; lengthB-lengthA; i++ { 32\theadB = headB.Next 33\t} 34\t} 35 36\t// 3. 一起走，看值是否相同，有的话就返回起始交点 37\tfor headA != headB { 38\theadA = headA.Next 39\theadB = headB.Next 40\t} 41\treturn headA 42} 时间复杂度: O(m + n)，其中 m 和 n 分别是两个链表的长度。 空间复杂度: O(1)，只使用了常量级别的额外空间。 解法2: 该解法通过双指针技术，让两个指针分别从 headA 和 headB 开始遍历，当遍历到链表末尾时，指针切换到另一个链表的起点。这样两个指针将在相交节点处相遇，或者在没有相交时，最终指向 null。\n具体步骤:\n初始化双指针:\n初始化两个指针 i 和 j，分别指向 headA 和 headB。 遍历链表:\n在两个指针不相等的情况下，不断遍历链表。当一个指针到达链表末尾时，切换到另一个链表的头部继续遍历。最终两个指针要么在相交节点相遇，要么同时遍历完两个链表后都指向 null。 这种方法不需要预先计算链表长度，通过双指针的遍历自然对齐并找到相交节点，代码更加简洁直观。\n1/** 2 * Definition for singly-linked list. 3 * type ListNode struct { 4 * Val int 5 * Next *ListNode 6 * } 7 */ 8func getIntersectionNode(headA, headB *ListNode) *ListNode { 9\t// 无增删改查操作可以不用虚拟头 10 // 直接开始走两个链表，判断是否相等，走完了就换到另一个 11 // 定义双指针 12 i := headA 13 j := headB 14 15 for i != j { 16 // if i.Next != nil不行，因为这样少判断了最后一位 17 if i != nil { 18 i = i.Next 19 } else { 20 i = headB 21 } 22 23 if j != nil { 24 j = j.Next 25 } else { 26 j = headA 27 } 28 } 29 30 return i 31} 时间复杂度: O(m + n)，其中 m 和 n 分别是两个链表的长度。 空间复杂度: O(1)，只使用了常量级别的额外空间。 对比:\n解法1：需要计算链表长度，并做一次长度调整后再同步遍历。逻辑上清晰，但步骤较多。 解法2：使用双指针技术，不需要计算链表长度，通过自然对齐找到相交节点。代码更简洁高效，推荐使用。 题目4：142. 环形链表 II 题目描述: 给定一个链表，返回链表开始入环的第一个节点。如果链表无环，则返回 null。\n示例:\n1输入: head = [3,2,0,-4], pos = 1 2输出: 返回索引为 1 的链表节点 代码实现:\n解法1: 该解法使用哈希表记录访问过的节点，一旦再次访问到相同的节点，说明链表存在环，该节点即为环的起始节点。\n具体步骤:\n定义哈希表:\n使用一个哈希表 map 来存储已经访问过的节点。 遍历链表:\n遍历链表，对于每个节点，检查它是否已经存在于哈希表中。如果存在，则说明该节点是环的起始节点，返回该节点。 如果节点不在哈希表中，则将其加入哈希表，继续遍历。 返回结果:\n如果遍历完链表没有找到重复节点，说明链表无环，返回 null。 这种方法直接有效，但需要额外的空间来存储已经访问过的节点。\n1/** 2 * Definition for singly-linked list. 3 * type ListNode struct { 4 * Val int 5 * Next *ListNode 6 * } 7 */ 8func detectCycle(head *ListNode) *ListNode { 9\t// 1. 定义map 10 meet := map[*ListNode]bool{} 11 12 // 2. 直接Next找，每次存到meet这个map里面，如果碰到相同的就返回 13 for head != nil { 14 if _, ok := meet[head]; ok { 15 return head 16 } 17 meet[head] = true 18 head = head.Next 19 } 20 21 // 3. 不存在 22 return nil 23} 时间复杂度: O(n)，其中 n 是链表的长度。 空间复杂度: O(n)，用于存储已经访问过的节点。 解法2: 使用快慢指针技术，通过让快指针（一次走两步）和慢指针（一次走一步）同时遍历链表，若链表有环，则两个指针会在环内相遇。然后通过调整指针，找到环的起点。\n具体步骤:\n初始化快慢指针:\n初始化快指针 fast 和慢指针 slow，都指向 head。 检测环:\n快指针每次走两步，慢指针每次走一步。如果快指针与慢指针在某一点相遇，说明链表中存在环。 找到环的起点:\n当快慢指针相遇时，将其中一个指针移到链表头部，然后两个指针每次都走一步。最终它们会在环的起始节点相遇。 返回结果:\n如果快慢指针相遇，返回该节点；如果快指针遍历完链表没有相遇，返回 null。 这种方法不需要额外的存储空间，通过数学推导和遍历链表，能够在常数空间下检测环并找到环的起点。\n1/** 2 * Definition for singly-linked list. 3 * type ListNode struct { 4 * Val int 5 * Next *ListNode 6 * } 7 */ 8func detectCycle(head *ListNode) *ListNode { 9\t// 1. 定义快慢指针 10\tslow := head 11\tfast := head 12 13\t// 2. slow前进步幅为1，fast前进步幅为2；fast按照永远领先一个逼近slow；如果有环在绕了n圈之后一定会相交! 14\tfor fast != nil \u0026amp;\u0026amp; fast.Next != nil { 15\tfast = fast.Next.Next 16\tslow = slow.Next 17 18\tif fast == slow { 19\tfor slow != head { 20\thead = head.Next 21\tslow = slow.Next 22\t} 23\treturn head 24\t} 25\t} 26\treturn nil 27} 时间复杂度: O(n)，其中 n 是链表的长度。 空间复杂度: O(1)，只使用了常量级别的额外空间。 对比:\n解法1：哈希表法直接而有效，但空间复杂度较高。 解法2：快慢指针法通过数学推导实现，时间复杂度和空间复杂度都更优，在实际应用中更为推荐。 ","link":"https://zhangsiming-blyq.github.io/post/algorithm/programmercarl/day4/","section":"post","tags":["algorithm","中文"],"title":"【算法刷题系列】第4天 24. 两两交换链表中的节点, 19. 删除链表的倒数第 N 个结点, 面试题 02.07. 链表相交, 142. 环形链表 II"},{"body":"学习内容 学习文档：\n链表讲解\n收获总结 链表概述 链表是一种基础的数据结构，由一系列节点组成，每个节点包含数据部分和指向下一个节点的指针（或引用）。链表的最后一个节点指向 null，表示链表的末尾。链表动态扩展性强，适合频繁插入和删除操作。\n链表的定义 链表有多种类型，最常见的是单链表和双链表。\n单链表：每个节点包含数据和指向下一个节点的指针 next。链表的头节点指向第一个节点，最后一个节点的 next 指向 null。\nGolang 单链表定义：\n1type ListNode struct { 2 Val int 3 Next *ListNode 4} 双链表：每个节点包含数据、指向下一个节点的指针 next 和指向前一个节点的指针 prev，允许双向遍历。\nGolang 双链表定义：\n1type ListNode struct { 2 Val int 3 Next *ListNode 4 Prev *ListNode 5} 注意事项 循环的时候参考数组，初始条件是i,j; 链表就是cur, cur.Next 链表的第一个节点比较特殊，处理的时候需要特殊处理；引入一个空的头节点dummyHead，可以简化很多操作(一视同仁) 链表的指针就是ListNode本身，因为任何一个ListNode都可以根据Next进行移动; 双指针解法的时候每一个指针都应该是一个ListNode 链表元素的内存分布 链表节点的内存分布不连续，节点在内存中的位置是随机分配的。这使得链表可以灵活地增长或缩小，但查找元素的时间复杂度较高，因为需要从头节点开始逐一遍历。\n节点的插入与删除 插入节点：\n头部插入：新节点的 next 指向当前头节点，并将链表头节点更新为新节点，时间复杂度为 O(1)。 尾部插入：单链表需要遍历链表找到最后一个节点，时间复杂度为 O(n)，双链表则直接访问尾节点，时间复杂度为 O(1)。 删除节点：\n删除头节点：将头节点更新为下一个节点，时间复杂度为 O(1)。 删除指定节点：需要遍历链表找到待删除节点，时间复杂度为 O(n)。 题目解析 题目1：203.移除链表元素 题目描述: 给定一个链表的头节点 head 和一个整数 val，请删除链表中所有满足 Node.val == val 的节点，并返回新的头节点。\n示例:\n1输入: head = [1,2,6,3,4,5,6], val = 6 2输出: [1,2,3,4,5] 解法总结: 使用虚拟头节点 dummyHead 方便处理可能需要删除头节点的情况。遍历链表时，如果当前节点的下一个节点的值等于 val，则跳过该节点（即将当前节点的 next 指向下下个节点），否则继续向下遍历。\n代码实现:\n1/** 2 * Definition for singly-linked list. 3 * type ListNode struct { 4 * Val int 5 * Next *ListNode 6 * } 7 */ 8func removeElements(head *ListNode, val int) *ListNode { 9\tdummyHead := \u0026amp;ListNode{} 10\tdummyHead.Next = head 11\t// 初始为第一个元素 12\tcur := dummyHead 13 14\tfor cur != nil \u0026amp;\u0026amp; cur.Next != nil { 15\t// 如果即将要看的下一位的值等于要删除的值 16\tif cur.Next.Val == val { 17\t// 移除就是跳过两位 18\tcur.Next = cur.Next.Next 19\t} else { 20\t// 跳过一位 21\tcur = cur.Next 22\t} 23\t} 24 25\t// 移除dummyHead 26\treturn dummyHead.Next 27} 时间复杂度: O(n)，其中 n 是链表中的节点数。每个节点最多访问一次。\n空间复杂度: O(1)，只使用了常量级别的额外空间。\n题目2：707.设计链表 题目描述: 设计链表实现增删查等基本操作。需要支持在链表头、尾以及指定索引位置进行元素的添加和删除，并能够获取指定索引位置的元素值。\n示例:\n1MyLinkedList linkedList = new MyLinkedList(); 2linkedList.addAtHead(1); 3linkedList.addAtTail(3); 4linkedList.addAtIndex(1, 2); // 链表变为 1-\u0026gt;2-\u0026gt;3 5linkedList.get(1); // 返回 2 6linkedList.deleteAtIndex(1); // 现在链表是 1-\u0026gt;3 7linkedList.get(1); // 返回 3 解法总结: 通过虚拟头节点 dummyHead 简化在头部进行插入和删除操作的实现。链表的设计包括获取指定索引位置的值、在头部或尾部添加元素、在指定索引位置插入或删除元素。注意要处理索引越界的情况。\n代码实现:\n1// type ListNode struct { 2// Val int 3// Next *ListNode 4// } 5 6type MyLinkedList struct { 7\tDummyHead *ListNode 8\tSize int 9} 10 11func Constructor() MyLinkedList { 12\tDummyHead := \u0026amp;ListNode{} 13 14\tMyLinkedList := MyLinkedList{ 15\tDummyHead: DummyHead, 16\tSize: 0, 17\t} 18\treturn MyLinkedList 19} 20 21func (this *MyLinkedList) Get(index int) int { 22\t// 先判断index是否有效 23\tif this == nil || index \u0026lt; 0 || index \u0026gt;= this.Size { 24\treturn -1 25\t} 26 27\tcurNode := this.DummyHead 28\tfor i := 0; i \u0026lt;= index; i++ { 29\tcurNode = curNode.Next 30\t} 31 32\treturn curNode.Val 33} 34 35func (this *MyLinkedList) AddAtHead(val int) { 36\tcurrHead := this.DummyHead.Next 37\tthis.DummyHead.Next = \u0026amp;ListNode{ 38\tVal: val, 39\tNext: currHead, 40\t} 41\tthis.Size++ 42} 43 44func (this *MyLinkedList) AddAtTail(val int) { 45\tcurrNode := this.DummyHead 46\tfor i := 0; i \u0026lt; this.Size; i++ { 47\tcurrNode = currNode.Next 48\t} 49\tcurrNode.Next = \u0026amp;ListNode{ 50\tVal: val, 51\tNext: nil, 52\t} 53\tthis.Size++ 54} 55 56func (this *MyLinkedList) AddAtIndex(index int, val int) { 57\t// 先判断index是否有效 58\tif this == nil || index \u0026lt; 0 || index \u0026gt; this.Size { 59\treturn 60\t} 61 62\tcurNode := this.DummyHead 63\tfor i := 0; i \u0026lt; index; i++ { 64\tcurNode = curNode.Next 65\t} 66\ttmp := curNode.Next 67\tfmt.Println(tmp) 68\tcurNode.Next = \u0026amp;ListNode{ 69\tVal: val, 70\tNext: tmp, 71\t} 72\tthis.Size++ 73} 74 75func (this *MyLinkedList) DeleteAtIndex(index int) { 76\t// 先判断index是否有效 77\tif this == nil || index \u0026lt; 0 || index \u0026gt;= this.Size { 78\treturn 79\t} 80 81\tcurNode := this.DummyHead 82\tfor i := 0; i \u0026lt; index; i++ { 83\tcurNode = curNode.Next 84\t} 85 86\ttmp := curNode.Next.Next 87\tfmt.Println(tmp) 88\tcurNode.Next = tmp 89\tthis.Size-- 90} 91 92/** 93 * Your MyLinkedList object will be instantiated and called as such: 94 * obj := Constructor(); 95 * param_1 := obj.Get(index); 96 * obj.AddAtHead(val); 97 * obj.AddAtTail(val); 98 * obj.AddAtIndex(index,val); 99 * obj.DeleteAtIndex(index); 100 */ 时间复杂度:\nGet 操作: O(n)，需要遍历链表直到指定索引位置。 AddAtHead 和 AddAtTail 操作: O(1)，在链表头或尾进行插入操作。 AddAtIndex 和 DeleteAtIndex 操作: O(n)，需要遍历链表找到指定位置。 空间复杂度: O(1)，只使用了常量级别的额外空间。\n题目3：206. 反转链表 题目描述: 给定一个单链表的头节点 head，将链表反转并返回反转后的链表。\n示例:\n1输入: head = [1,2,3,4,5] 2输出: [5,4,3,2,1] 解法总结: 使用双指针法反转链表。初始化 pre 指针为 nil，cur 指针为链表的头节点。在遍历过程中，通过临时变量 tmp 保存 cur.Next，将 cur.Next 指向 pre 实现链表反转，最后将 pre 移动到 cur 位置，cur 移动到 tmp 位置，直到遍历结束。 一定要注意双指针的时候要根据下面的图解梳理清楚，先是cur后移，然后pre后移，然后cur回指到pre！\n图解: 代码实现:\n1/** 2 * Definition for singly-linked list. 3 * type ListNode struct { 4 * Val int 5 * Next *ListNode 6 * } 7 */ 8func reverseList(head *ListNode) *ListNode { 9\tcur := head 10\tvar pre *ListNode 11 12\tfor cur != nil { 13\t// 下次循环cur后移一位 14\ttmp := cur.Next 15 16\t// cur回指到前一个 17\tcur.Next = pre 18 19\t// pre后移一位 20\tpre = cur 21\tcur = tmp 22\t} 23\treturn pre 24} 时间复杂度: O(n)，其中 n 是链表的长度。每个节点被访问一次。\n空间复杂度: O(1)，只使用了常量级别的额外空间。\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/programmercarl/day3/","section":"post","tags":["algorithm","中文"],"title":"【算法刷题系列】第3天 203.移除链表元素, 707.设计链表, 206.反转链表"},{"body":"学习内容 学习文档：\n长度最小子数组讲解\n螺旋矩阵II讲解\n收获总结 滑动窗口： 在某些情况下，滑动窗口可以看作是一种特殊的双指针技术，其中两个指针 i 和 j（即左指针和右指针）从同一端开始，但移动的条件有所不同。滑动窗口的核心思想是通过右指针 j 不断向右扩展窗口，同时左指针 i 尽量向右收缩窗口，以找到满足特定条件的最小或最大窗口。\n举例来说，假设你想找到数组 arr 中两个元素之间的差值等于 diff 的一对元素索引，这可以视为滑动窗口问题，右指针 j 用来扩展窗口，左指针 i 用来收缩窗口，直到找到满足条件的子数组。初始化时可以根据具体问题选择 i, j 都从 0, 0 开始，也可以 0, 1 这种情况。\n题目解析 题目1：209. 长度最小的子数组 题目描述: 给定一个含有 n 个正整数的数组 nums 和一个正整数 target ，找出该数组中满足其和 ≥ target 的长度最小的连续子数组，并返回其长度。如果不存在符合条件的子数组，返回 0。\n示例:\n1输入: target = 7, nums = [2,3,1,2,4,3] 2输出: 2 3解释: 子数组 [4,3] 是该条件下的长度最小的子数组。 解法总结: 该题可以通过滑动窗口技术来解决。我们使用两个指针 i 和 j，分别代表窗口的左右边界。首先，右指针 j 向右移动，扩展窗口，累加窗口内的元素和 sum。当 sum 大于或等于 target 时，开始收缩窗口（移动左指针 i），同时记录当前窗口的最小长度。通过这种方式，可以高效地找到满足条件的最短子数组长度。 这里需要注意的是，因为有可能第一个数就等于target，所以j也要从0开始，同时内侧判断条件也是for!\n代码实现:\n1func minSubArrayLen(target int, nums []int) int { 2\ti, j := 0, 0 3\tsum := 0 4\tmin := len(nums) + 1 5 6\tfor j \u0026lt; len(nums) { 7\tsum += nums[j] 8 9\tfor sum \u0026gt;= target { 10\t// 只在 sum 等于 target 时更新 min 11\tmin = minValue(j-i+1, min) 12\tsum -= nums[i] 13\ti++ 14\t} 15 16\tj++ 17\t} 18 19\t// 最后检查是否找到了符合条件的子数组 20\tif min == len(nums)+1 { 21\treturn 0 22\t} else { 23\treturn min 24\t} 25} 26 27func minValue(x, y int) int { 28\tif x \u0026lt; y { 29\treturn x 30\t} 31\treturn y 32} 时间复杂度: O(n)，因为每个元素在最坏情况下只会被访问两次，分别是被加入到窗口和从窗口中移除。\n空间复杂度: O(1)，仅使用了常数空间来存储索引、和以及最小长度。\n题目2：59. 螺旋矩阵 II 题目描述: 给定一个正整数 n，生成一个包含 1 到 n^2 所有元素，且元素按顺时针顺序螺旋排列的 n x n 正方形矩阵。\n示例:\n1输入: n = 3 2输出: 3[ 4 [1, 2, 3], 5 [8, 9, 4], 6 [7, 6, 5] 7] 解法总结: 这段代码的目的是生成一个 n x n 的螺旋矩阵，矩阵中的元素从 1 开始，按顺时针方向从外向内依次填充。具体来说，代码通过控制矩阵的四个边界（上、下、左、右）来逐步缩小范围。每次循环依次填充当前边界，从左到右、从上到下、从右到左、从下到上，直到所有元素填满为止。在填充过程中，每个方向的边界都会向内收缩一行或一列，确保后续填充在正确的范围内进行。最终，当 num 超过目标值（n * n）时，填充过程结束，返回生成的螺旋矩阵。 在实现过程中，有几个关键要点和注意事项。首先是边界的初始化和更新，确保在每次填充完一个方向后正确地调整 top、bottom、left 和 right 的值，以防止重复填充或越界。其次，在循环过程中需要时刻检查 num 的值，以确保填充过程不会超出范围。此外，需要特别处理小矩阵的特殊情况，如 n = 1 或 n = 0，以保证代码的通用性和正确性。最后，由于算法的时间和空间复杂度均为 O(n^2)，这意味着该算法可以有效地处理中小规模的矩阵生成任务，但在非常大的 n 时需要考虑性能问题。\n代码实现:\n1func generateMatrix(n int) [][]int { 2\t// Init 3\ttop, bottom, left, right := 0, n-1, 0, n-1 4\tnum := 1 5\ttarget := n * n 6 7\tmatrix := make([][]int, n) 8 9\tfor i := 0; i \u0026lt;= n-1; i++ { 10\tmatrix[i] = make([]int, n) 11\t} 12 13\tfor num \u0026lt;= target { 14\tfor i := left; i \u0026lt;= right; i++ { 15\tmatrix[top][i] = num 16\tnum++ 17\t} 18\ttop++ 19 20\tfor i := top; i \u0026lt;= bottom; i++ { 21\tmatrix[i][right] = num 22\tnum++ 23\t} 24\tright-- 25 26\tfor i := right; i \u0026gt;= left; i-- { 27\tmatrix[bottom][i] = num 28\tnum++ 29\t} 30\tbottom-- 31 32\tfor i := bottom; i \u0026gt;= top; i-- { 33\tmatrix[i][left] = num 34\tnum++ 35\t} 36\tleft++ 37\t} 38 39\treturn matrix 40} 时间复杂度: O(n^2)，因为需要填充整个 n x n 的矩阵。\n空间复杂度: O(n^2)，用于存储生成的螺旋矩阵。\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/programmercarl/day2/","section":"post","tags":["algorithm","中文"],"title":"【算法刷题系列】第2天 209. 长度最小的子数组, 59. 螺旋矩阵 II"},{"body":"学习内容 学习文档：数组理论基础\n收获总结 快速排序：\n快速排序是一种基于分治法的排序算法。首先，选择一个基准元素，然后通过分区操作将数组划分为两部分，一部分元素小于基准值，另一部分元素大于基准值。递归地对这两部分进行排序，最终将数组排序完成。\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func quickSort(arr []int, low, high int) { 6 if low \u0026lt; high { 7 pi := partition(arr, low, high) 8 quickSort(arr, low, pi-1) 9 quickSort(arr, pi+1, high) 10 } 11} 12 13func partition(arr []int, low, high int) int { 14 pivot := arr[high] 15 i := low - 1 16 for j := low; j \u0026lt; high; j++ { 17 if arr[j] \u0026lt; pivot { 18 i++ 19 arr[i], arr[j] = arr[j], arr[i] 20 } 21 } 22 arr[i+1], arr[high] = arr[high], arr[i+1] 23 return i + 1 24} 二分查找：\n二分查找是一种在有序数组中查找目标值的算法。它通过不断将数组分成两半，并与中间元素进行比较，从而快速缩小查找范围，最终确定目标值的位置。如果目标值不存在，则返回 -1。\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func binarySearch(arr []int, low, high, target int) int { 6 if low \u0026lt;= high { 7 mid := low + (high-low)/2 8 if arr[mid] == target { 9 return mid 10 } else if arr[mid] \u0026gt; target { 11 return binarySearch(arr, low, mid-1, target) 12 } else { 13 return binarySearch(arr, mid+1, high, target) 14 } 15 } 16 return -1 17} 双指针，对撞指针，快慢指针常用初始化设计：\n双指针：可以从数组两端开始，常用于查找两个数的和问题。初始化 i 为数组起始位置，j 为数组结束位置，当 i \u0026lt; j 时迭代。另一种情况是从同一端开始，适用于查找具有特定差值的元素对。\n快慢指针：快慢指针常用于处理链表或数组中的问题，比如删除重复元素。i 和 j 初始化为数组或链表的起始位置，迭代条件是 j 指针不越界。\n1// 双指针 - 从两端开始 2func twoSum(arr []int, target int) []int { 3 i, j := 0, len(arr)-1 4 for i \u0026lt; j { 5 sum := arr[i] + arr[j] 6 if sum == target { 7 return []int{i, j} 8 } else if sum \u0026lt; target { 9 i++ 10 } else { 11 j-- 12 } 13 } 14 return nil 15} 16 17// 双指针 - 从同一端开始 18func findPairWithDiff(arr []int, diff int) []int { 19 i, j := 0, 1 20 for i \u0026lt; len(arr) \u0026amp;\u0026amp; j \u0026lt; len(arr) { 21 if i != j \u0026amp;\u0026amp; arr[j]-arr[i] == diff { 22 return []int{i, j} 23 } else if arr[j]-arr[i] \u0026lt; diff { 24 j++ 25 } else { 26 i++ 27 } 28 } 29 return nil 30} 31 32// 快慢指针 33func removeDuplicates(arr []int) int { 34 i, j := 0, 1 35 for j \u0026lt; len(arr) { 36 if arr[i] != arr[j] { 37 i++ 38 arr[i] = arr[j] 39 } 40 j++ 41 } 42 return i + 1 43} 题目解析 题目1：704. 二分查找 题目描述: 给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。\n示例:\n1输入: nums = [-1,0,3,5,9,12], target = 9 2输出: 4 解法总结: 使用二分查找方法。初始化左右指针 low 和 high，每次计算中间元素并与目标值进行比较，更新指针范围，直到找到目标值或确认不存在。\n代码实现:\n1func search(nums []int, target int) int { 2 low := 0 3 high := len(nums) - 1 4 for low \u0026lt;= high { 5 mid := low + (high-low)/2 6 if target == nums[mid] { 7 return mid 8 } else if target \u0026gt; nums[mid] { 9 low = mid + 1 10 } else { 11 high = mid - 1 12 } 13 } 14 return -1 15} 时间复杂度: O(log n)，因为每次查找都将搜索范围缩小一半。 空间复杂度: O(1)，只使用了常数空间。\n题目2：27. 移除元素 题目描述: 给你一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，返回剩余元素的数量。元素的顺序可能发生改变。\n示例:\n1输入: nums = [3,2,2,3], val = 3 2输出: 2, nums = [2,2] 解法总结: 使用双指针方法。i 和 j 都从数组的头部开始遍历，当 j 指向的元素不等于 val 时，将其赋值给 i 指向的位置，并递增 i，最终返回 i 的值作为新数组的长度。\n代码实现:\n1func removeElement(nums []int, val int) int { 2 i, j := 0, 0 3 count := 0 4 for j \u0026lt;= len(nums)-1 { 5 if nums[j] != val { 6 nums[i] = nums[j] 7 count++ 8 i++ 9 } 10 j++ 11 } 12 return count 13} 时间复杂度: O(n)，遍历数组一次即可完成操作。 空间复杂度: O(1)，只使用了常数空间。\n题目3：977.有序数组的平方 题目描述: 给你一个按非递减顺序排序的整数数组 nums，返回每个数字的平方组成的新数组，要求也按非递减顺序排序。\n示例:\n1输入: nums = [-4,-1,0,3,10] 2输出: [0,1,9,16,100] 解法总结: 可以采用两种方法。一种是先对所有元素平方后使用快速排序，另一种是使用双指针从数组两端向中间遍历，将较大的平方值放在结果数组的末尾。第二种方法要注意，因为给定的数组是非递减排序的，所以平方值最大的元素一定在数组的两端(中间小)，所以当我们遍历需要for循环从后往前遍历!\n代码实现:\n方法一：先平方后排序\n1func sortedSquares(nums []int) []int { 2 for i := 0; i \u0026lt; len(nums); i++ { 3 nums[i] = nums[i] * nums[i] 4 } 5 quickSort(nums, 0, len(nums)-1) 6 return nums 7} 8 9func quickSort(arr []int, low, high int) { 10 if low \u0026lt; high { 11 pivot := partition(arr, low, high) 12 quickSort(arr, low, pivot-1) 13 quickSort(arr, pivot+1, high) 14 } 15} 16 17func partition(arr []int, low, high int) int { 18 pivotValue := arr[high] 19 i := low - 1 20 for j := low; j \u0026lt; high; j++ { 21 if arr[j] \u0026lt; pivotValue { 22 i++ 23 arr[i], arr[j] = arr[j], arr[i] 24 } 25 } 26 arr[i+1], arr[high] = arr[high], arr[i+1] 27 return i + 1 28} 时间复杂度: O(n log n)，因为对所有元素进行平方操作后，再使用快速排序。 空间复杂度: O(log n)，递归调用栈的空间开销。\n方法二：双指针法\n1func sortedSquares(nums []int) []int { 2 i, j := 0, len(nums)-1 3 result := make([]int, len(nums)) 4 5 for h := len(nums) - 1; h \u0026gt;= 0; h-- { 6 if nums[i]*nums[i] \u0026gt; nums[j]*nums[j] { 7 result[h] = nums[i] * nums[i] 8 i++ 9 } else { 10 result[h] = nums[j] * nums[j] 11 j-- 12 } 13 } 14 return result 15} 时间复杂度: O(n)，因为只需要一次遍历即可完成操作。 空间复杂度: O(n)，因为使用了额外的数组来存储结果。\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/programmercarl/day1/","section":"post","tags":["algorithm","中文"],"title":"【算法刷题系列】第1天 704. 二分查找，27. 移除元素, 977.有序数组的平方"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/english/","section":"tags","tags":null,"title":"English"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/etcd/","section":"tags","tags":null,"title":"etcd"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/etcd/","section":"categories","tags":null,"title":"Etcd"},{"body":"Preface: This tutorial focuses exclusively on the Etcd v3 protocol. Throughout Etcd's history, two protocols have been employed: v2 and v3. However, v2 is considered outdated and not recommended for production environments. Furthermore, Etcd v2 and v3 have distinct data storage structures. As a result, you cannot use Etcd v2 to read data from Etcd v3 or use an Etcd v2 snapshot to restore data in Etcd v3. This document is tailored for Etcd instances containing v3 data.\nHow to Deploy an Etcd Cluster? Etcd, a widely used distributed key-value store, plays a pivotal role in Kubernetes and is employed by numerous internet corporations worldwide. For both backend engineers and operations personnel, becoming proficient in handling errors and mastering this middleware is crucial. In this section, we will explore how to deploy an Etcd cluster using container tools such as \u0026quot;Docker\u0026quot; and \u0026quot;Docker Compose.\u0026quot; Let's get started!\nI have chosen an Ubuntu virtual machine as the working platform, specifically version 20.04.2 LTS. I assume you have already installed Docker and Docker Compose on your machine. If not, you can refer to the official Docker and Docker Compose documentation for installation instructions.\nhttps://docs.docker.com/desktop/install/linux-install/\n1. Etcd Directory Structure 1$ tree 2. 3├── docker-compose.yaml 4├── etcd 5├── etcd.conf 6└── etcd-ssl 7 ├── ca.crt 8 ├── ca.key 9 ├── ca.srl 10 ├── client.crt 11 ├── client.csr 12 ├── client.key 13 ├── server.cnf 14 ├── server.crt 15 ├── server.csr 16 └── server.key As you may know, we load our Etcd cluster using Docker Compose. Therefore, we need to create a docker-compose.yaml file to describe our Etcd cluster. Additionally, we require a customized etcd.conf file because the following section will involve configuring our Etcd cluster. The \u0026quot;etcd\u0026quot; and \u0026quot;etcd-ssl\u0026quot; directories are used to store Etcd data and certificates for client authentication.\n2. Complete the docker-compose.yaml File 1version: \u0026#34;3\u0026#34; 2services: 3 etcd: 4 image: quay.io/coreos/etcd:v3.5.9 5 network_mode: \u0026#34;host\u0026#34; 6 container_name: etcd_container 7 command: /usr/local/bin/etcd --config-file /etcd.conf 8 volumes: 9 - ./etcd:/var/lib/etcd 10 - ./etcd.conf:/etcd.conf:ro 11 - ./etcd-ssl:/etcd/etcd-ssl/:ro 12 restart: always 13 ulimits: 14 nofile: 15 soft: 1048576 16 hard: 1048576 We are using the official Etcd image from quay.io, specifically version v3.5.9. The network_mode: \u0026quot;host\u0026quot; configuration allows the Etcd cluster to communicate with each other, enabling the use of the host's IP address for connecting to the Etcd cluster. The restart: always setting ensures that the Etcd cluster remains operational, fully utilizing Etcd's high availability mechanisms. 3. Customize the etcd.conf File 1#---for basic running---# 2#[basic] 3name: k8s-etcd01 4data-dir: /var/lib/etcd 5enable-v2: true 6listen-client-urls: https://0.0.0.0:2379 7listen-peer-urls: http://0.0.0.0:2380 8 9#[cluster] 10initial-advertise-peer-urls: http://10.0.24.14:2380 11advertise-client-urls: https://10.0.24.14:2379 12# for multi-cluster: k8s-etcd01=http://10.0.24.14:2380,k8s-etcd02=http://10.146.80.49:2380,k8s-etcd03=http://10.146.80.52:2380 13initial-cluster: k8s-etcd01=http://10.0.24.14:2380 14initial-cluster-state: existing 15initial-cluster-token: etcd-cluster 16 17#---for cluster election---# 18initial-election-tick-advance: true 19heartbeat-interval: 500 20election-timeout: 3000 21 22#---for performance tuner---# 23quota-backend-bytes: 8589934592 24auto-compaction-mode: \u0026#39;periodic\u0026#39; 25auto-compaction-retention: \u0026#39;1\u0026#39; 26max-request-bytes: 10485760 27snapshot-count: 50000 28max-snapshots: 5 29max-wals: 5 30 31#---for client TLS encryption---# 32client-transport-security: 33 # Path to the client server TLS cert file. 34 cert-file: /etcd/etcd-ssl/server.crt 35 # Path to the client server TLS key file. 36 key-file: /etcd/etcd-ssl/server.key 37 # Enable client cert authentication. 38 # client-cert-auth: false 39 # Path to the client server TLS trusted CA cert file. 40 trusted-ca-file: /etcd/etcd-ssl/ca.crt 41 # Client TLS using generated certificates 42 # auto-tls: false The table below provides explanations for each parameter in the etcd.conf file:\nParameter Description data-dir Specifies the directory where Etcd stores its data, including the database and transaction logs. enable-v2 Enables or disables support for the Etcd version 2 API. Typically set to \u0026quot;false\u0026quot; to encourage the use of the more feature-rich and efficient v3 API. listen-client-urls Defines the URLs on which Etcd listens for client requests, allowing clients to connect to Etcd using these URLs. listen-peer-urls Specifies the URLs on which Etcd listens for communication with other Etcd nodes in the cluster. initial-advertise-peer-urls Specifies the initial URLs used by Etcd to advertise itself to other members when forming or joining a cluster. advertise-client-urls Similar to initial-advertise-peer-urls, this parameter specifies URLs that clients should use to connect to this Etcd member. initial-cluster Provides a list of initial Etcd cluster members and their associated initial-advertise-peer-urls. Used during cluster bootstrapping. initial-cluster-state Indicates whether the cluster should be initially set to \u0026quot;new\u0026quot; or \u0026quot;existing.\u0026quot; Set to \u0026quot;new\u0026quot; for a new cluster or \u0026quot;existing\u0026quot; when adding a new member to an existing cluster. initial-cluster-token An arbitrary string used to identify the cluster. All members in the same cluster should have the same token. initial-election-tick-advance Determines whether to fast-forward initial election ticks on boot for faster election. When true, local member fast-forwards election ticks to expedite \u0026quot;initial\u0026quot; leader election. heartbeat-interval The time interval between heartbeat signals sent by an Etcd leader to maintain leadership. Expressed in milliseconds. Recommended to be around the maximum of the average round-trip time (RTT) between members. election-timeout The maximum time interval an Etcd follower can go without receiving communication from the leader before starting an election. Expressed in milliseconds. Set based on the heartbeat interval and average round-trip time between members. Highly recommended you to see this link: https://etcd.io/docs/v3.5.9/tuning/#election-timeout quota-backend-bytes Specifies the maximum number of bytes that can be used to store data in Etcd. When exceeded, Etcd performs auto-compaction or raises an alarm. auto-compaction-mode Determines when auto-compaction should be performed: \u0026quot;periodic\u0026quot; (at regular intervals) or \u0026quot;revision\u0026quot; (based on the number of revisions). auto-compaction-retention Auto-compaction retention for MVCC key-value store in hours. 0 means disabling auto-compaction. max-request-bytes Sets the maximum size in bytes of an Etcd request. Requests exceeding this size will be rejected. snapshot-count The number of committed transactions to trigger a snapshot, used for data backup and recovery. max-snapshots The maximum number of snapshots to retain; older snapshots may be deleted when this limit is reached. max-wals The maximum number of write-ahead logs (WALs) to retain; older WALs may be deleted when this limit is reached. client-transport-security Security settings for client communication, including client certificate and key files. 4. Generate Certificates for Client Authentication Enabling client authentication for the Etcd cluster is essential for preventing unauthorized access. Therefore, we need to generate certificates for client authentication. For efficiency reasons, in private environments, peer authentication is not necessary, as it may impact performance.\nFollow these commands to generate client certificates using OpenSSL:\n1# Generate ca.crt 2$ openssl genrsa -out ca.key 4096 3$ openssl req -new -x509 -key ca.key -out ca.crt -subj \u0026#34;/CN=etcd-ca\u0026#34; 4 5# Generate server.crt 6$ vim server.cnf 7[req] 8distinguished_name = req_distinguished_name 9req_extensions = v3_req 10[req_distinguished_name] 11[ v3_req ] 12subjectAltName = @alt_names 13[alt_names] 14DNS.1 = xxx.xxx.xxx.xxx 15IP.1 = xxx.xxx.xxx.xxx 16IP.2 = xxx.xxx.xxx.xxx 17IP.3 = xxx.xxx.xxx.xxx 18$ openssl genrsa -out server.key 2048 19$ openssl req -new -key server.key -out server.csr -subj \u0026#34;/CN=etcd-server\u0026#34; -config server.cnf 20$ openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 -extensions v3_req -extfile server.cnf 21 22# Generate client.crt 23$ openssl genrsa -out client.key 2048 24$ openssl req -new -key client.key -out client.csr -subj \u0026#34;/CN=etcd-client\u0026#34; 25$ openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365 5. Optimize the Etcd Environment To achieve optimal performance as an Etcd machine, it's essential to optimize both the Etcd configuration and the host's hardware environment. Here are some recommended parameters to adjust before starting your Etcd cluster:\nFile Descriptors: Linux treats everything as a file, including regular files, directories, sockets, pipes, and more. A file descriptor is a unique integer identifier used by a process to access open files or other resources. It acts as a handle that the process uses to read, write, or manipulate the resource. There are three standard file descriptors: 0 (stdin), 1 (stdout), and 2 (stderr). You can view a process's current open file count with lsof -p \u0026lt;pid\u0026gt; | wc -l. How to view a process's open file limit? cat /proc/\u0026lt;pid\u0026gt;/limits | grep \u0026quot;Max open files\u0026quot; How to view user's current file descriptor limit(per user)? ulimit -n To change the file descriptor limit for your user, modify /etc/security/limits.conf or create a new file under /etc/security/limits.d/, with the following content: 1* soft nofile 1048576 2* hard nofile 1048576 You can also adjust file descriptor limits in systemd service files or Docker Compose YAML files, for example,add the following lines to the [Service] section: 1LimitNOFILE=new_limit 2LimitNPROC=new_limit Max Open Files Count: The \u0026quot;max open files count\u0026quot; or \u0026quot;file descriptor limit\u0026quot; is a system-wide limit on the number of file descriptors a process can have open simultaneously. This limit is in place to prevent a single process from consuming excessive system resources by opening too many files or network connections. It is especially important for server applications that need to handle many concurrent clients, as well as for system daemons and background processes. You can view the current file descriptor limit for a process using the ulimit -Hn command. To change the system-wide file descriptor limit, modify /etc/sysctl.conf, by adding the following line: 1fs.file-max = new_limit Remember that increasing file descriptor limits should be done carefully and with consideration for system resources. Setting limits too high can potentially lead to resource exhaustion, so it's essential to strike a balance between accommodating your application's needs and maintaining system stability. TCP Keepalive: Enable TCP keepalive to prevent idle connections from timing out. Modify net.ipv4.tcp_keepalive_time, net.ipv4.tcp_keepalive_probes, and net.ipv4.tcp_keepalive_intvl, for example: 1net.ipv4.tcp_keepalive_time = 60 2net.ipv4.tcp_keepalive_probes = 3 3net.ipv4.tcp_keepalive_intvl = 10 I/O Scheduler: Choose an I/O scheduler that best suits your workload. For example, for SSDs, you can use the deadline or mq-deadline scheduler, and for HDDs, the cfq scheduler. To check the current scheduler, use cat /sys/block/\u0026lt;device\u0026gt;/queue/scheduler, and to change it, use echo \u0026lt;scheduler\u0026gt; \u0026gt; /sys/block/\u0026lt;device\u0026gt;/queue/scheduler. CPU: Etcd performance benefits from having a fast CPU, so choose a machine with multiple cores and high clock speeds. Memory: Etcd's performance is directly affected by available memory. Ensure you have enough RAM to accommodate your etcd workload. Storage: Etcd performance can be I/O bound, so consider using faster storage solutions like SSDs to reduce latency and increase overall performance. Bandwidth: Ensure your network has sufficient bandwidth to handle the expected etcd traffic. Latency: Minimize network latency between etcd nodes to enhance communication speed. Please note that specific values for these parameters depend on your workload, hardware, and network environment. Perform load testing and benchmarking after making changes to assess their impact on Etcd's performance and stability. Additionally, ensure proper backups and conduct changes in a controlled test environment before applying them to production systems.\n6. Bootstrapping the Etcd Cluster 1$ docker-compose up 2$ docker-compose ps 3 Name Command State Ports 4--------------------------------------------------------------- 5etcd_container /usr/local/bin/etcd --conf ... Up 6# Check logs 7$ docker-compose logs -f 8... 9 10# Check Etcd cluster status 11ETCDCTL_API=3 etcdctl --endpoints=https://10.0.24.14:2379 --cacert=./etcd-ssl/ca.crt --cert=./etcd-ssl/client.crt --key=./etcd-ssl/client.key endpoint status -w table 12+-------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 13| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | 14+-------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 15| https://10.0.24.14:2379 | c8e267dee39d90f6 | 3.5.9 | 20 kB | true | false | 6 | 38 | 38 | | 16+-------------------------+ 17 18------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ Etcd Basic Commands Here are some basic Etcd commands you can use:\nDescription Command Set a key-value pair etcdctl put key value Get the value for a key etcdctl get key Delete a key etcdctl delete key Watch changes on a key etcdctl watch key Set a key with a TTL (time-to-live) etcdctl put key value --ttl seconds List all keys in a directory (with prefix) etcdctl get dir --prefix Delete all keys in a directory (with prefix) etcdctl delete dir --prefix Create a new user (for authentication) etcdctl user add username:password Create a role and grant permissions (RBAC) etcdctl role add rolename\netcdctl role grant-permission rolename Add a new Etcd cluster member etcdctl member add --peer-urls=https://new-node:2380 List Etcd cluster members etcdctl member list Remove an Etcd cluster member etcdctl member remove member-id Take a snapshot of the Etcd data etcdctl snapshot save snapshot.db Restore Etcd data from a snapshot etcdctl snapshot restore snapshot.db Display cluster health status etcdctl endpoint status -w table Etcd Common Issues And How To Fix Them 1. Disaster Recovery Taking a Snapshot of Etcd Data You can take a snapshot of the etcd data while it's running using the following command:\n1$ ETCDCTL_API=3 etcdctl --endpoints=https://10.0.24.14:2379 --cacert=./etcd-ssl/ca.crt --cert=./etcd-ssl/client.crt --key=./etcd-ssl/client.key snapshot save ./snapshot.db 2{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-09-06T00:20:31.982857+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:65\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;created temporary db file\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;./snapshot.db.part\u0026#34;} 3{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-09-06T00:20:31.992423+0800\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;client\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;v3@v3.5.9/maintenance.go:212\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;opened snapshot stream; downloading\u0026#34;} 4{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-09-06T00:20:31.992476+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:73\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;fetching snapshot\u0026#34;,\u0026#34;endpoint\u0026#34;:\u0026#34;https://10.0.24.14:2379\u0026#34;} 5{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-09-06T00:20:32.003568+0800\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;client\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;v3@v3.5.9/maintenance.go:220\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;completed snapshot read; closing\u0026#34;} 6{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-09-06T00:20:32.008874+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:88\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;fetched snapshot\u0026#34;,\u0026#34;endpoint\u0026#34;:\u0026#34;https://10.0.24.14:2379\u0026#34;,\u0026#34;size\u0026#34;:\u0026#34;20 kB\u0026#34;,\u0026#34;took\u0026#34;:\u0026#34;now\u0026#34;} 7{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-09-06T00:20:32.008952+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:97\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;saved\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;./snapshot.db\u0026#34;} 8Snapshot saved at ./snapshot.db The command will produce output indicating the snapshot has been saved.\nRestoring Etcd Data from a Snapshot To restore etcd data from a snapshot, ensure that the following parameters in your restore command match the ones in your etcd.conf file, such as initial-cluster-token:\n1$ ETCDCTL_API=3 etcdctl --endpoints=https://10.0.24.14:2379 --cacert=./etcd-ssl/ca.crt --cert=./etcd-ssl/client.crt --key=./etcd-ssl/client.key snapshot restore --skip-hash-check ./snapshot.db --initial-cluster=k8s-etcd01=http://10.0.24.14:2380 --initial-cluster-token=etcd-cluster --initial-advertise-peer-urls=http://10.0.24.14:2380 --name k8s-etcd01 --data-dir=./data 2Deprecated: Use `etcdutl snapshot restore` instead. 3 42023-09-06T00:25:53+08:00\tinfo\tsnapshot/v3_snapshot.go:248\trestoring snapshot\t{\u0026#34;path\u0026#34;: \u0026#34;./snapshot.db\u0026#34;, \u0026#34;wal-dir\u0026#34;: \u0026#34;data/member/wal\u0026#34;, \u0026#34;data-dir\u0026#34;: \u0026#34;./data\u0026#34;, \u0026#34;snap-dir\u0026#34;: \u0026#34;data/member/snap\u0026#34;, \u0026#34;stack\u0026#34;: \u0026#34;go.etcd.io/etcd/etcdutl/v3/snapshot.(*v3Manager).Restore\\n\\tgo.etcd.io/etcd/etcdutl/v3@v3.5.9/snapshot/v3_snapshot.go:254\\ngo.etcd.io/etcd/etcdutl/v3/etcdutl.SnapshotRestoreCommandFunc\\n\\tgo.etcd.io/etcd/etcdutl/v3@v3.5.9/etcdutl/snapshot_command.go:147\\ngo.etcd.io/etcd/etcdctl/v3/ctlv3/command.snapshotRestoreCommandFunc\\n\\tgo.etcd.io/etcd/etcdctl/v3/ctlv3/command/snapshot_command.go:129\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tgithub.com/spf13/cobra@v1.1.3/command.go:856\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tgithub.com/spf13/cobra@v1.1.3/command.go:960\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tgithub.com/spf13/cobra@v1.1.3/command.go:897\\ngo.etcd.io/etcd/etcdctl/v3/ctlv3.Start\\n\\tgo.etcd.io/etcd/etcdctl/v3/ctlv3/ctl.go:107\\ngo.etcd.io/etcd/etcdctl/v3/ctlv3.MustStart\\n\\tgo.etcd.io/etcd/etcdctl/v3/ctlv3/ctl.go:111\\nmain.main\\n\\tgo.etcd.io/etcd/etcdctl/v3/main.go:59\\nruntime.main\\n\\truntime/proc.go:250\u0026#34;} 52023-09-06T00:25:53+08:00\tinfo\tmembership/store.go:141\tTrimming membership information from the backend... 62023-09-06T00:25:53+08:00\tinfo\tmembership/cluster.go:421\tadded member\t{\u0026#34;cluster-id\u0026#34;: \u0026#34;7cadd2cbf250eee7\u0026#34;, \u0026#34;local-member-id\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;added-peer-id\u0026#34;: \u0026#34;c8e267dee39d90f6\u0026#34;, \u0026#34;added-peer-peer-urls\u0026#34;: [\u0026#34;http://10.0.24.14:2380\u0026#34;]} 72023-09-06T00:25:53+08:00\tinfo\tsnapshot/v3_snapshot.go:269\trestored snapshot\t{\u0026#34;path\u0026#34;: \u0026#34;./snapshot.db\u0026#34;, \u0026#34;wal-dir\u0026#34;: \u0026#34;data/member/wal\u0026#34;, \u0026#34;data-dir\u0026#34;: \u0026#34;./data\u0026#34;, \u0026#34;snap-dir\u0026#34;: \u0026#34;data/member/snap\u0026#34;} Alternatively, you can restore specific keys or directories from a snapshot:\n1$ etcdctl snapshot restore \u0026lt;snapshot-file\u0026gt; --data-dir \u0026lt;data-directory\u0026gt; --restore-from-key \u0026lt;key\u0026gt; 2# or 3$ etcdctl snapshot restore /path/to/snapshot.db --data-dir /var/lib/etcd --restore-from-key /mydir/mykey After adjusting etcd's data directory in the docker-compose.yaml file, start the restored etcd cluster:\n1$ docker-compose up -d 2$ ETCDCTL_API=3 etcdctl --endpoints=https://10.0.24.14:2379 --cacert=./etcd-ssl/ca.crt --cert=./etcd-ssl/client.crt --key=./etcd-ssl/client.key endpoint status -w table 3+-------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 4| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | 5+-------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 6| https://10.0.24.14:2379 | c8e267dee39d90f6 | 3.5.9 | 20 kB | true | false | 2 | 4 | 4 | | 7+-------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 2. Issue: \u0026quot;etcdserver: mvcc: database space exceeded\u0026quot; Related links: https://etcd.io/blog/2023/how_to_debug_large_db_size_issue/\nIf you encounter the \u0026quot;etcdserver: mvcc: database space exceeded\u0026quot; issue, you can follow these steps to resolve it:\n1$ ETCDCTL_API=3 etcdctl --endpoints localhost:2379 alarm list/disarm 2$ ETCDCTL_API=3 etcdctl --endpoints localhost:2379 endpoint status -w table 3$ ETCDCTL_API=3 etcdctl --endpoints localhost:2379 compact 4$ ETCDCTL_API=3 etcdctl --endpoints localhost:2379 defrag 5 6# Check the etcd revision to identify keys with the most revisions 7$ ETCDCTL_API=3 etcdctl --endpoints localhost:2379 endpoint status -w json 8$ ETCDCTL_API=3 etcdctl --endpoints localhost:2379 get \u0026#34;\u0026#34; --prefix --keys-only | grep -v ^$ | awk -F \u0026#39;/\u0026#39; \u0026#39;{ h[$3]++ } END {for (k in h) print h[k], k}\u0026#39; | sort -nr 9 10# Modify the docker-compose.yaml file and restart the etcd cluster 11$ vim docker-compose.yaml In the docker-compose.yaml file, ensure that you specify \u0026quot;--quota-backend-bytes 8589934592\u0026quot; and any other settings for your etcd cluster. Then restart the cluster:\n1$ docker-compose up -d Let's delve into the etcd's revision mechanism:\nIn etcd, the revision represents a continually increasing integer value assigned to each modification made to the etcd key-value store. It serves as a global version number that tracks the historical changes within the etcd cluster. The revision number is incremented each time there is a modification, such as creating, updating, or deleting a key-value pair in etcd. Each modification operation increments the revision by one.\nThe etcd revision number carries several crucial implications:\nConsistency and Order: The revision number ensures the consistency and order of operations within the etcd cluster. It guarantees that operations applied at a higher revision number have occurred after operations at lower revision numbers. Read Operations: When conducting a read operation on etcd, you can specify a revision number to retrieve the state of the key-value store at a specific point in time. This enables you to access a consistent snapshot of the data at a particular revision. Change Detection: By monitoring the revision number, you can identify changes or updates to the etcd key-value store. You can periodically fetch the latest revision number and compare it to a previously recorded value to ascertain if any modifications have taken place. Database Compaction: Etcd offers a compaction mechanism that allows you to remove older or redundant revisions from the database. By compacting the database, you can reclaim storage space and enhance performance. The revision number plays a pivotal role in the compaction process. 3. Adding a New Etcd Cluster Member To add a new etcd cluster member, follow these steps:\nConfigure the new node's etcd.conf file with the following essential parameters:\nname: A unique name for the new node. data-dir: The directory where etcd will store its data. initial-advertise-peer-urls: The URL at which the new node advertises itself to other members. listen-peer-urls: The URLs on which the new node listens for communication with other etcd nodes. listen-client-urls: The URLs on which the new node listens for client requests. advertise-client-urls: The URLs that clients should use to connect to the new node. initial-cluster: A list of the initial etcd cluster members and their associated initial-advertise-peer-urls, which should match those in other nodes' etcd.conf. initial-cluster-state: Set it to \u0026quot;existing\u0026quot; if you're adding a new member to an existing cluster. Use etcdctl to add the new member:\n1$ ETCDCTL_API=3 etcdctl --endpoints=https://ETCDCLUSTER --cacert=./etcd-ssl/ca.crt --cert=./etcd-ssl/client.crt --key=./etcd-ssl/client.key member add ETCDNAME --peer-urls=https://NEW-NODE:2380 Start the new node and check its status: 1$ docker-compose up -d 2$ ETCDCTL_API=3 etcdctl --endpoints=https://ETCDCLUSTER --cacert=./etcd-ssl/ca.crt --cert=./etcd-ssl/client.crt --key=./etcd-ssl/client.key endpoint status -w table Interacting With Kubernetes's Etcd Installing etcdhelper To interact with Kubernetes's etcd, you can install etcdhelper using the following steps:\n1# Clone the etcdhelper repository 2$ git clone https://github.com/openshift/origin.git 3 4# Set up an alias for etcdhelper 5$ vim ~/.zshrc 6alias ectl=\u0026#39;ETCDCTL_API=3 etcdhelper -endpoint=172.31.27.61:2379 -cacert /home/k8s/code/etcd-prod-27-61/cert/ca.pem -cert /home/k8s/code/etcd-prod-27-61/cert/etcd.pem -key /home/k8s/code/etcd-prod-27-61/cert/etcd.key\u0026#39; Accessing Kubernetes's Etcd You can use etcdhelper to access Kubernetes's etcd and retrieve data:\n1$ ectl get /registry/secrets/test/harbor-kubernetes \u0026amp;\u0026amp; echo Please note that Kubernetes stores data in the following order: /registry/{resource_name}/{namespace}/{resource_instance}. Secrets' content is base64 encoded before being stored in etcd, so you need to decode it to retrieve the actual content.\nAdditional Information To explore etcd's API paths, you can use the provided script:\n1#!/bin/bash 2# Get Kubernetes keys from etcd 3export ETCDCTL_API=3 4keys=`ETCDCTL_API=3 etcdctl --endpoints=172.31.27.61:2379 --cacert /etc/ssl/etcd/ssl 5 6/ca.pem --cert /etc/ssl/etcd/ssl/member-ip-172-31-27-61.cn-north-1.compute.internal.pem --key /etc/ssl/etcd/ssl/member-ip-172-31-27-61.cn-north-1.compute.internal-key.pem get /registry --prefix -w json|python -m json.tool|grep key|cut -d \u0026#34;:\u0026#34; -f2|tr -d \u0026#39;\u0026#34;\u0026#39;|tr -d \u0026#34;,\u0026#34;` 7for x in $keys;do 8 echo $x|base64 -d|sort 9done This script helps you retrieve and decode Kubernetes-related keys stored in etcd.\nPlease keep in mind that the most reliable and accurate source for tutorials is the official documentation (https://etcd.io/docs/v3.5/). If you have any questions, please consult the official documentation first. Wishing you the best of luck!\n","link":"https://zhangsiming-blyq.github.io/post/linux/etcd-tutorial/","section":"post","tags":["Etcd","English"],"title":"Etcd Tutorial"},{"body":"一、Java环境安装 Java是一门强大的编程语言，首先需要在您的计算机上安装Java开发环境。以下是安装Java环境的步骤：\n1. 选择Java版本 对于初学者，建议选择Java 8或Java 11版本。您可以从Oracle官方网站下载这些版本。\nJava 8下载链接：https://www.oracle.com/java/technologies/javase/jdk8-archive-downloads.html Java 11下载链接：https://www.oracle.com/java/technologies/javase/jdk11-archive-downloads.html 2. 下载示例代码 您可以下载示例代码，以便学习和实践Java编程。示例代码通常包含了一些基础的Java程序，可以帮助您快速入门。\n示例代码下载链接（以wget为例）：\n1$ wget https://horstmann.com/corejava/corejava11.zip 3. 配置环境变量 将Java主目录添加到系统的PATH环境变量中，这样您就可以在任何位置运行Java命令。您可以使用以下命令来编辑配置文件：\n1$ vim ~/.zshrc 在配置文件中添加以下行：\n1export PATH=/your/java/installation/path/bin:$PATH 确保将/your/java/installation/path替换为您的Java安装路径, 比如\u0026quot;/Users/simingzhang/siming/jdk-11.0.17/bin\u0026quot;。\n4. 验证安装 corejava实例代码和javasrc内置库代码都放到java主目录下：\n1$ tree -L 1 2. 3├── README.html 4├── bin 5├── conf 6├── corejava 7├── include 8├── javasrc 9├── jmods 10├── legal 11├── lib 12├── man 13└── release 使用以下命令验证您的Java安装是否成功：\n1$ javac --version 2javac 11.0.17 3 4$ java --version 5java 11.0.17 2022-10-18 LTS 6Java(TM) SE Runtime Environment 18.9 (build 11.0.17+10-LTS-269) 7Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.17+10-LTS-269, mixed mode) 5. 运行Welcome程序 您可以尝试运行一个简单的Java程序，比如\u0026quot;Welcome\u0026quot;程序。以下是示例代码和运行步骤：\n1$ cat Welcome.java 2/** 3 * This program displays a greeting for the reader. 4 * @version 1.30 2014-02-27 5 * @author Cay Horstmann 6 */ 7public class Welcome 8{ 9 public static void main(String[] args) 10 { 11 String greeting = \u0026#34;Welcome to Core Java!\u0026#34;; 12 System.out.println(greeting); 13 for (int i = 0; i \u0026lt; greeting.length(); i++) 14 System.out.print(\u0026#34;=\u0026#34;); 15 System.out.println(); 16 } 17} 18 19$ ls 20Welcome.java 21$ javac Welcome.java 22$ ls 23Welcome.class Welcome.java 24$ java Welcome 25Welcome to Core Java! 26===================== 这个示例演示了一个简单的Java程序，欢迎您进入Java编程的世界。\n二、基础数据类型 Java具有丰富的数据类型，让我们首先了解一些基本的概念：\n2.1 一个简单的Java应用程序 在Java中，一个简单的应用程序通常由以下几个要素组成：\n使用关键字public作为访问修饰符。 区分大小写。 类名必须以字母开头，遵循大驼峰命名规则。 源代码文件名必须与公共类的名字相同。 运行已编译的程序时，Java虚拟机总是从指定类的main方法开始执行。\n2.2 注释 在Java中，注释是非常重要的。它们用于提供关于代码的说明和文档。\nJava注释示例：\n1// This is a single-line comment. 2 3/* 4 * This is a multi-line comment. 5 * This is the second line. 6 */ 2.3 数据类型 Java是一种强类型语言，每个变量都必须声明一个数据类型。\n整型 Java提供了不同大小的整数类型，例如int、short、long和byte。每种类型都有不同的取值范围。\nint：4字节，取值范围为-2147483648到2147483647。 short：2字节，取值范围为-32768到32767。 long：8字节，取值范围为-9223372036854775808到9223372036854775807。 byte：1字节，取值范围为-128到127。 浮点类型 Java提供了float和double两种浮点类型。double类型的精度是float的两倍。\n浮点类型还有特殊值，如正无穷大、负无穷大和NaN（非数值）。\nchar类型 char类型用于表示字符，字符字面值需要用单引号括起来。\nUnicode和char类型 Java中的char类型使用Unicode编码来表示字符。\nboolean类型 boolean类型只有两个值：true和false。它主要用于条件判断。\n2.4 变量与常量 在Java中，变量用于存储数据，常量用于存储不可变的值。\n2.5 运算符 Java提供了各种运算符，包括算术运算符、比较运算符、逻辑运算符等。\n整数被0除会产生一个异常，浮点数被0除会得到无穷大或者NaN结果; 而且在使用运算符时要小心，避免出现除以零等异常情况。 几个小tips:\n不要在boolean类型与任何数值类型之间进行强制类型转换 可以在赋值中使用二元运算符，这是一种很简便的简写形式; 比如x += 1等价于x = x + 1 建议不要在表达式中使用++，因为这样的代码很容易让人困惑，而且会带来烦人的bug 1public class Variable { 2 // const 需要全大写 3 public static final double CM_PER_INCH = 2.331; 4 5 // 枚举类型在java 11必须要local不能public 6 enum Size {SMALL, MEDIUM, LARGE, EXTRA_LARGE} 7 8 public static void main(String[] args) { 9 // 定义变量并且打印变量 10 double salary; 11 int vacationDays; 12 long earthPopulation; 13 boolean done; 14 int i, j; 15 salary = 1.2; 16 vacationDays = 10; 17 earthPopulation = 20; 18 done = true; 19 double money = 1.15; 20 var test = \u0026#39;Z\u0026#39;; 21 System.out.println(salary); 22 System.out.println(vacationDays); 23 System.out.println(earthPopulation); 24 System.out.println(done); 25 System.out.println(money); 26 System.out.println(test); 27 28 System.out.println(CM_PER_INCH); 29 30 Size s = Size.LARGE; 31 System.out.println(s); 32 /* 33 1.2 34 10 35 20 36 true 37 1.15 38 Z 39 2.331 40 LARGE 41 */ 42 43 // 变量的运算符 44 System.out.println(salary * vacationDays); 45 System.out.println(money + vacationDays); 46 System.out.println(money % vacationDays); 47 /* 48 12.0 49 11.15 50 1.15 51 */ 52 53 test(); 54 convert(); 55 rela_operator(); 56 } 57 58 public static void test() { 59 // 变量的数学计算 60 System.out.println(\u0026#34;This is test part\u0026#34;); 61 double x = 4; 62 double y = Math.sqrt(x); 63 System.out.println(y); 64 System.out.println(y * Math.PI); 65 /* 66 This is test part 67 2.0 68 6.283185307179586 69 */ 70 } 71 72 public static void convert() { 73 // 变量的类型转换 74 System.out.println(\u0026#34;This is convert part\u0026#34;); 75 int n = 1289; 76 float y = n; 77 System.out.println(y); 78 double x = 9.997; 79 int nx = (int) Math.round(x); 80 System.out.println(nx); 81 /* 82 This is convert part 83 1289.0 84 10 85 */ 86 } 87 88 public static void rela_operator() { 89 // 变量的关系运算符 90 System.out.println(\u0026#34;This is relation operator part\u0026#34;); 91 int n = 1; 92 n += 4; 93 n++; 94 System.out.println(n); 95 96 // 前缀形式会先完成+1，而后缀形式会使用变量原来的值, 所以下面的答案都是14(++只是作用于相连的变量上) 97 int a = 2 * ++n; 98 int b = 2 * n++; 99 System.out.println(n); 100 System.out.println(a); 101 System.out.println(b); 102 System.out.println(n); 103 104 int x = 2; 105 int y = 3; 106 boolean b1 = x != 0 \u0026amp;\u0026amp; x \u0026gt; 1; 107 if (b1) { 108 // 三元运算符 109 System.out.println(x \u0026lt; y ? x : y); 110 } 111 /* 112 This is relation operator part 113 6 114 8 115 14 116 14 117 8 118 2 119 */ 120 } 121} 2.6 字符串 Java中的字符串是不可变的，意味着一旦创建，就不能修改。字符串类具有许多有用的方法，如equals()、isEmpty()等。\n1public class Character { 2 public static void main(String[] args) { 3 // 定义字符串 4 String greeting = \u0026#34;Hello\u0026#34;; 5 // 截取字符串，左闭右开 6 String s = greeting.substring(0, 3); 7 System.out.println(s); 8 9 // 拼接字符串 10 String expletive = \u0026#34;Expletive\u0026#34;; 11 String sum = expletive + s; 12 System.out.println(sum); 13 System.out.println(sum.repeat(3)); 14 /* 15 Hel 16 ExpletiveHel 17 ExpletiveHelExpletiveHelExpletiveHel 18 */ 19 20 // 一定不要使用 == 运算符检测两个字符串是否相等，因为这个运算符只能确定两个字符串内存是否相同 21 if (s.equals(\u0026#34;Hel\u0026#34;)) { 22 System.out.println(\u0026#34;s is equal to Hel\u0026#34;); 23 } 24 25 // 空串和 null 26 if (s != null \u0026amp;\u0026amp; s.length() != 0) { 27 System.out.println(\u0026#34;s is not empty\u0026#34;); 28 } 29 30 // 字符串构建 31 StringBuilder builder = new StringBuilder(); 32 builder.append(s); 33 builder.append(\u0026#34;siminghahaha\u0026#34;); 34 String result = builder.toString(); 35 System.out.println(result); 36 /* 37 s is equal to Hel 38 s is not empty 39 Helsiminghahaha 40 */ 41 } 42} 2.7 输入输出 Java提供了多种方式进行输入和输出操作。标准输入输出流是常用的方式之一。\n不过需要注意，因为输入是可见的，所以不适合从控制台读取密码，密码要放入字符数组中，而不是字符串中；比如Console cons = System.console(); char[] passwd = cons.readPassword();。\n1import java.io.*; 2import java.nio.charset.StandardCharsets; 3import java.nio.file.Path; 4import java.util.*; 5 6public class FiFOPart { 7 public static void main(String[] args) throws IOException { 8 // 获取输入 9 Scanner in = new Scanner(System.in); 10 System.out.println(\u0026#34;What\u0026#39;s your name?\u0026#34;); 11 String name = in.nextLine(); 12 13 Scanner ain = new Scanner(System.in); 14 System.out.println(\u0026#34;Your age?\u0026#34;); 15 int age = ain.nextInt(); 16 17 System.out.println(name + \u0026#34; is \u0026#34; + age + \u0026#34; years old.\u0026#34;); 18 19 // 打印输出 20 System.out.printf(\u0026#34;%s is %d years old.\\n\u0026#34;, name, age); 21 /* 22 What\u0026#39;s your name? 23 ZhangSiming 24 Your age? 25 26 26 ZhangSiming is 26 years old. 27 ZhangSiming is 26 years old. 28 */ 29 30 // 读写文件 31 write_file(); 32 read_file(); 33 } 34 35 public static void read_file() throws IOException { 36 Scanner in = new Scanner(Path.of(\u0026#34;test.txt\u0026#34;), StandardCharsets.UTF_8); 37 while (in.hasNext()) { 38 System.out.println(in.nextLine()); 39 } 40 } 41 42 public static void write_file() throws IOException { 43 PrintWriter out = new PrintWriter(\u0026#34;test.txt\u0026#34;, StandardCharsets.UTF_8); 44 out.println(\u0026#34;Hello, world!\u0026#34;); 45 out.close(); 46 } 47} 2.8 控制流程 控制流程用于控制程序的执行顺序。Java支持条件语句、循环语句、switch语句等。\n请注意，在循环中使用for语句时要小心循环变量的范围。\n块作用域：在块内部定义的变量只能在块内部使用，块外部不能使用块内部定义的变量。\n1public class Blockcontrol { 2 public static void main(String[] args) { 3 // if条件判断 4 int x = 10; 5 if (x \u0026lt; 20) { 6 System.out.println(\u0026#34;This is if statement\u0026#34;); 7 } else { 8 System.out.println(\u0026#34;This is else statement\u0026#34;); 9 } 10 // This is if statement 11 12 // while循环 13 int y = 20; 14 while (y \u0026lt; 25) { 15 System.out.println(\u0026#34;value of y : \u0026#34; + y); 16 y++; 17 } 18 /* 19 value of y : 20 20 value of y : 21 21 value of y : 22 22 value of y : 23 23 value of y : 24 24 */ 25 26 // do和while组合表示, 先执行一次do, 然后判断while条件是否成立, 如果成立则继续执行do, 否则退出循环 27 int z = 30; 28 do { 29 System.out.println(\u0026#34;value of z : \u0026#34; + z); 30 z++; 31 } while (z \u0026lt; 25); 32 // value of z : 30 33 34 // for循环 35 for (int i = 0; i \u0026lt; 10; i++) { 36 System.out.println(\u0026#34;value of i : \u0026#34; + i); 37 } 38 /* 39 value of i : 0 40 value of i : 1 41 value of i : 2 42 value of i : 3 43 value of i : 4 44 value of i : 5 45 value of i : 6 46 value of i : 7 47 value of i : 8 48 value of i : 9 49 */ 50 51 // switch语句，如果没有break语句，会一直执行到break语句或者switch语句结束 52 int a = 6; 53 switch (a) { 54 case 1: 55 System.out.println(\u0026#34;value is 1\u0026#34;); 56 break; 57 case 2: 58 System.out.println(\u0026#34;value is 2\u0026#34;); 59 break; 60 case 3: 61 System.out.println(\u0026#34;value is 3\u0026#34;); 62 break; 63 default: 64 System.out.println(\u0026#34;value is not 1, 2 or 3\u0026#34;); 65 } 66 // value is not 1, 2 or 3 67 68 // goto语句，跳转到指定的标签处 69 int gt = 20; 70 goto_exam: 71 while (gt \u0026lt; 25) { 72 // 跳过22 73 if (gt == 22) { 74 gt++; 75 continue; 76 } 77 if (gt == 24) { 78 for (int i = 0; i \u0026lt; 10; i++) { 79 if (i == 5) { 80 break goto_exam; 81 } 82 System.out.println(\u0026#34;value of i : \u0026#34; + i); 83 } 84 } 85 System.out.println(\u0026#34;value of gt : \u0026#34; + gt); 86 gt++; 87 } 88 /* 89 value of gt : 20 90 value of gt : 21 91 value of gt : 23 92 value of i : 0 93 value of i : 1 94 value of i : 2 95 value of i : 3 96 value of i : 4 97 */ 98 } 99} 2.9 大数 对于大数计算，Java提供了BigInteger类，可以处理大整数; 特别大的数比如彩票中奖的几率。\n1import java.math.*; 2import java.util.*; 3public class BigIntegerTest { 4 public static void main(String[] args) { 5 // 这个程序演示了如何使用大数值BigInteger类, 以及如何使用大数值BigInteger类来计算概率 6 // 通俗点说就是计算概率的时候, 用int或者long都会溢出, 所以用BigInteger类来计算 7 Scanner in = new Scanner(System.in); 8 9 System.out.println(\u0026#34;How many numbers do you need to draw?\u0026#34;); 10 int k = in.nextInt(); 11 12 System.out.println(\u0026#34;What is the highest number you can draw?\u0026#34;); 13 int n = in.nextInt(); 14 15 BigInteger lotteryOdds = BigInteger.valueOf(1); 16 for (int i = 1; i \u0026lt;= k; i++) { 17 lotteryOdds = lotteryOdds.multiply(BigInteger.valueOf(n - i + 1)).divide(BigInteger.valueOf(i)); 18 } 19 System.out.println(\u0026#34;Your odds are 1 in \u0026#34; + lotteryOdds + \u0026#34;. Good luck!\u0026#34;); 20 /* 21 How many numbers do you need to draw? 22 111 23 What is the highest number you can draw? 24 1111 25 Your odds are 1 in 228162770660494339523962629246420402033509870004236362946558722063262768528532360822215561441512138917100676811940334385612061873122251943150491920143450176. Good luck! 26 */ 27 } 28} 2.10 数组 数组是一种用于存储多个相同类型的元素的数据结构。Java中，数组的长度是固定的。\n多维数组允许您创建表格或矩阵等数据结构。\n注意：\n长度为0的数组和null并不相同 1import java.util.Arrays; 2 3public class ArrayTest { 4 public static void main(String[] args) { 5 // 定义并初始化数组的几种方法 6 int[] a; 7 a = new int[100]; 8 int[] b = new int[100]; 9 int[] c = {1, 2, 3, 4, 5,}; 10 int[] d = {}; 11 12 // 打印数组的长度 13 System.out.println(\u0026#34;a\u0026#39;s length is \u0026#34; + a.length); 14 System.out.println(\u0026#34;b\u0026#39;s length is \u0026#34; + b.length); 15 // 数组的长度是固定的，不能改变 16 // c[5] = 6; 17 System.out.println(\u0026#34;c\u0026#39;s length is \u0026#34; + c.length); 18 System.out.println(\u0026#34;d\u0026#39;s length is \u0026#34; + d.length); 19 /* 20 a\u0026#39;s length is 100 21 b\u0026#39;s length is 100 22 c\u0026#39;s length is 5 23 d\u0026#39;s length is 0 24 */ 25 26 // 访问数组元素的方式1 27 System.out.println(\u0026#34;c[0] is \u0026#34; + c[0]); 28 29 for (int i = 0; i \u0026lt; c.length; i++) { 30 System.out.println(\u0026#34;c[\u0026#34; + i + \u0026#34;] is \u0026#34; + c[i]); 31 } 32 // 访问数组元素的方式2 33 for (int i : c) { 34 System.out.println(\u0026#34;i is \u0026#34; + i); 35 } 36 // 打印数组元素最快的方式 37 System.out.println(java.util.Arrays.toString(c)); 38 /* 39 a\u0026#39;s length is 100 40 b\u0026#39;s length is 100 41 c\u0026#39;s length is 5 42 d\u0026#39;s length is 0 43 c[0] is 1 44 c[0] is 1 45 c[1] is 2 46 c[2] is 3 47 c[3] is 4 48 c[4] is 5 49 i is 1 50 i is 2 51 i is 3 52 i is 4 53 i is 5 54 [1, 2, 3, 4, 5] 55 */ 56 57 // 数组是引用类型，c和cc指向同一个数组 58 int[] cc = c; 59 cc[0] = 9999; 60 System.out.println(\u0026#34;c[0] is \u0026#34; + c[0]); 61 62 // 数组的拷贝 63 int[] cc_long = Arrays.copyOf(c, c.length + 10); 64 System.out.println(\u0026#34;cc_long\u0026#39;s length is \u0026#34; + cc_long.length); 65 System.out.println(java.util.Arrays.toString(c)); 66 System.out.println(\u0026#34;cc_long[14] is \u0026#34; + cc_long[14]); 67 68 // 快排 69 Arrays.sort(c); 70 System.out.println(java.util.Arrays.toString(c)); 71 /* 72 c[0] is 9999 73 cc_long\u0026#39;s length is 15 74 [9999, 2, 3, 4, 5] 75 cc_long[14] is 0 76 [2, 3, 4, 5, 9999] 77 */ 78 79 MultiArray(); 80 } 81 82 public static void MultiArray() { 83 // define a two-dimensional array 84 double[][] balances; 85 balances = new double[10][10]; 86 balances[0][0] = 1.0; 87 balances[0][1] = 2.0; 88 balances[1][0] = 3.0; 89 balances[1][1] = 4.0; 90 91 double[][] bb = { 92 {1, 2, 3, 4, 5}, 93 {1, 2, 3, 4, 5} 94 }; 95 96 // 打印二维数组 97 for (double[] row : bb) { 98 for (double value : row) { 99 // Print with column and row line 100 System.out.printf(\u0026#34;%10.2f\u0026#34;, value); 101 } 102 } 103 104 // 或者直接用deepToString打印多维数组 105 System.out.println(Arrays.deepToString(balances)); 106 /* 107 1.00 2.00 3.00 4.00 5.00 1.00 2.00 3.00 4.00 5.00[[1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]] 108 */ 109 110 // 打印三角数组 111 int[][] triang = triangular(); 112 for (int[] row : triang) { 113 for (int value : row) { 114 // Print with column and row line 115 System.out.printf(\u0026#34;%2d\u0026#34;, value); 116 } 117 System.out.println(); 118 } 119 /* 120 1 121 1 1 122 1 2 1 123 1 3 3 1 124 1 4 6 4 1 125 1 51010 5 1 126 1 6152015 6 1 127 1 721353521 7 1 128 1 82856705628 8 1 129 1 936841261268436 9 1 130 110451202102522101204510 1 131 */ 132 } 133 134 public static int[][] triangular() { 135 // 打印杨辉三角形 136 // 杨辉三角形：第n行有n个元素，第n行的第k个元素等于第n-1行的第k-1个元素和第k个元素之和 137 final int NMAX = 10; 138 int[][] odds = new int[NMAX + 1][]; 139 for (int n = 0; n \u0026lt;= NMAX; n++) { 140 odds[n] = new int[n + 1]; 141 } 142 143 for (int n = 0; n \u0026lt; odds.length; n++) { 144 for (int k = 0; k \u0026lt; odds[n].length; k++) { 145 int lotteryOdds = 1; 146 for (int i = 1; i \u0026lt;= k; i++) { 147 lotteryOdds = lotteryOdds * (n - i + 1) / i; 148 } 149 150 odds[n][k] = lotteryOdds; 151 } 152 } 153 return odds; 154 } 155} 三、对象与类 算法+数据结构=程序, 算法是第一位的，数据结构是第二位的,这就明确地表述了程序员的工作方式。\n3.1 面向对象程序设计概述 面向对象程序设计是一种重要的编程范式，它强调程序的结构是由对象和对象之间的交互所定义的。\n类是构造对象的模板，对象是类的实例。类包含数据字段和方法，用于描述对象的属性和行为。\n注意：\nJava编写的所有代码都位于某个类里面 对象的数据称为实例字段，操作数据的过程称为方法 程序只能通过对象的方法与对象进行数据交互，封装给对象赋予了\u0026quot;黑盒\u0026quot;特征 所有的类都有一个共同的超类Object，通过扩展一个类来建立另一个类的过程称为继承 对象的三个主要特性：\n对象的行为：可以对对象完成哪些操作，或者可以对对象应用哪些方法 对象的状态：当调用那些方法时，对象会如何响应 对象的标识：如何区分具有相同行为与状态的不同对象 类之间最常见的关系：\n依赖 聚合 继承 3.2 使用预定义类 Java提供了许多预定义类，您可以直接使用它们来完成各种任务。例如，LocalDate类用于处理日期。\n在使用预定义类时，通常需要创建类的实例，并调用其方法来执行操作。\n注意：\nJava中，使用构造器(或称构造函数)构造新实例，构造器的名字应该与类名相同，构造出来的对象需要存到一个变量中 在Java中，任何对象变量的值都是对存储在另外一个地方的某个对象的引用，new操作符的返回值也是一个引用; null表示这个对象变量目前没有引用任何对象, 比如Date deadline = new Date; 下面的程序使用预定义类\u0026quot;LocalDate\u0026quot;打印了当月的日历，并且用星号标识了所在的具体某一天(Java类库中的LocalDate类：Date类使用距离一个固定时间点的毫秒数表示，这个时间点就是所谓的纪元(ephch), 他是UTC时间1970年1月1日00:00:00)。 1/* 2Mon Tue Wed Thu Fri Sat Sun 3 1 2 3 4 5 6 4 7 8 9 10 11 12 13 5 14 15 16 17 18 19 20 6 21* 22 23 24 25 26 27 7 28 29 30 31 8 */ 9 10import java.time.*; 11 12public class CalendarTest { 13 public static void main(String[] args) { 14 // java.time.LocalDate 15 LocalDate date = LocalDate.now(); 16 int month = date.getMonthValue(); 17 int today = date.getDayOfMonth(); 18 // 由当前日期算出月初第一天日期 19 date = date.minusDays(today - 1); 20 DayOfWeek weekday = date.getDayOfWeek(); 21 int value = weekday.getValue(); // 1 = Monday, ... 7 = Sunday 22 System.out.println(\u0026#34;Mon Tue Wed Thu Fri Sat Sun\u0026#34;); 23 24 // 月初打印多少个空格取决于月初第一天是星期几 25 for (int i = 1; i \u0026lt; value; i++) 26 System.out.print(\u0026#34; \u0026#34;); 27 28 // 打印每一天，直到月末，根据是否为周一判断是否换行；并且在当前日期打出*标识 29 while (date.getMonthValue() == month) { 30 System.out.printf(\u0026#34;%3d\u0026#34;, date.getDayOfMonth()); 31 if (date.getDayOfMonth() == today) 32 System.out.print(\u0026#34;*\u0026#34;); 33 else 34 System.out.print(\u0026#34; \u0026#34;); 35 date = date.plusDays(1); 36 if (date.getDayOfWeek().getValue() == 1) 37 System.out.println(); 38 } 39 if (date.getDayOfWeek().getValue() != 1) 40 System.out.println(); 41 } 42} 3.3 用户自定义类 您可以创建自己的类来建模和解决特定问题。自定义类通常包括字段、构造器和方法。\n在类中，使用private关键字来保护数据字段，同时提供公共的访问器和更改器方法。\n构造器用于初始化对象的状态，可以有多个构造器以适应不同的需求。\nJava中最简单的类定义形式为：\n1 class ClassName 2 { 3 field1; 4 field2; 5 ... 6 constructor1; 7 constructor2; 8 ... 9 method1; 10 method2; 11 ... 12 } 关于构造器:\n构造器与类同名 每个类可以有一个以上的构造器 构造器可以有0个、1个或者多个参数 构造器没有返回值 构造器总是伴随着new操作符一起调用 注意:\n关键字public意味着任何类的任何方法都可以调用这些方法; private确保只有Employee类自身的方法能够访问这些实例字段 不要在构造器中定义与实例字段同名的局部变量 不要编写返回可变对象引用的访问器方法，因为如果是引用的话，在外界也可以更改，就违背了封装的设计原则(这个对象只能在方法中修改返回), 会让程序变得不可控 final修饰符(不可修改，并且确保每一个构造器执行之后这个字段已经设置)：如果一个类不希望被继承，就用final修饰符来修饰这个类，如果一个方法不希望被重写，就用final修饰符来修饰这个方法，如果一个变量不希望被修改，就用final修饰符来修饰这个变量 var声明局部变量，Java10以后才会有 1/* 2name=Carl Cracker,salary=78750.0,hireDay=1987-12-15 3name=Harry Hacker,salary=52500.0,hireDay=1989-10-01 4name=Tony Tester,salary=42000.0,hireDay=1990-03-15 5 */ 6 7import java.time.*; 8 9public class EmployeeTest { 10 public static void main(String[] args) { 11 // 创建三个Employee对象 12 Employee[] staff = new Employee[3]; 13 staff[0] = new Employee(\u0026#34;Carl Cracker\u0026#34;, 75000, 1987, 12, 15); 14 staff[1] = new Employee(\u0026#34;Harry Hacker\u0026#34;, 50000, 1989, 10, 1); 15 staff[2] = new Employee(\u0026#34;Tony Tester\u0026#34;, 40000, 1990, 3, 15); 16 // 给每一个Employee对象加薪5% 17 for (Employee e : staff) 18 e.raiseSalary2(5); 19 // 打印出每一个Employee对象的信息 20 for (Employee e : staff) 21 System.out.println(\u0026#34;name=\u0026#34; + e.getName() + \u0026#34;,salary=\u0026#34; + e.getSalary() + \u0026#34;,hireDay=\u0026#34; + e.getHireDay()); 22 } 23} 24 25class Employee { 26 private final String name; 27 private double salary; 28 private LocalDate hireDay; 29 30 public Employee(String n, double s, int year, int month, int day) { 31 name = n; 32 salary = s; 33 hireDay = LocalDate.of(year, month, day); 34 } 35 36 public String getName() { 37 return name; 38 } 39 40 public double getSalary() { 41 return salary; 42 } 43 44 public LocalDate getHireDay() { 45 return hireDay; 46 } 47 48 public void raiseSalary(double byPercent) { 49 double raise = salary * byPercent / 100; 50 // 不能这样操作，因为name被设置成了final：name += \u0026#34;test\u0026#34;; 51 salary += raise; 52 } 53 54 public void raiseSalary2(double byPercent) { 55 double raise = this.salary * byPercent / 100; 56 this.salary += raise; 57 } 58} 3.4 静态域与静态方法 静态域和静态方法属于类而不是对象，它们可以被所有对象共享; 换言之如果将一个字段定义为static，每个类只有一个这样的字段, 并且从这个类实例化出来的每一个对象都共享这个字段的值。\n静态常量: 是不可修改的值，通常使用public static final修饰, 比如public static final double PI = 3.1415926;\n静态方法: 是没有隐式参数(是没有this参数的方法)的方法，可以直接通过类名调用, 无需实例化对象，比如Math.pow(2, 3);\nmain方法也是静态方法，实际上启动程序的时候还没有任何对象，静态的main方法将执行并构造程序所需要的对象\n1/* 2name=Tom,id=1,salary=40000.0 3name=Dick,id=2,salary=60000.0 4name=Harry,id=3,salary=65000.0 5Next available id=4 6 */ 7public class StaticTest { 8 public static void main(String[] args) { 9 EmployeeNew[] staff = new EmployeeNew[3]; 10 staff[0] = new EmployeeNew(\u0026#34;Tom\u0026#34;, 40000); 11 staff[1] = new EmployeeNew(\u0026#34;Dick\u0026#34;, 60000); 12 staff[2] = new EmployeeNew(\u0026#34;Harry\u0026#34;, 65000); 13 14 for (EmployeeNew e : staff) { 15 e.setId(); 16 System.out.println(\u0026#34;name=\u0026#34; + e.getName() + \u0026#34;,id=\u0026#34; + e.getId() + \u0026#34;,salary=\u0026#34; + e.getSalary()); 17 } 18 int n = EmployeeNew.getNextId(); 19 System.out.println(\u0026#34;Next available id=\u0026#34; + n); 20 } 21} 22 23class EmployeeNew { 24 // 所有这个类实例化出来的对象都共享这个nextId 25 private static int nextId = 1; 26 private String name; 27 private double salary; 28 private int id; 29 30 public EmployeeNew(String n, double s) { 31 name = n; 32 salary = s; 33 id = 0; 34 } 35 36 public String getName() { 37 return name; 38 } 39 40 public double getSalary() { 41 return salary; 42 } 43 44 public int getId() { 45 return id; 46 } 47 48 public void setId() { 49 id = nextId; 50 nextId++; 51 } 52 53 public static int getNextId() { 54 return nextId; 55 } 56 57 public static void main(String[] args) { 58 EmployeeNew e = new EmployeeNew(\u0026#34;Harry\u0026#34;, 50000); 59 System.out.println(e.getName() + \u0026#34; \u0026#34; + e.getSalary()); 60 } 61} 3.5 方法参数 程序语言设计汇总关于将参数传递给方法：按值调用表示方法接收的是调用者提供的值，而按引用调用表示方法接收的是调用者提供的变量地址\nJava程序设计语言总是采用按值调用，也就是说，方法得到的是所有参数值的一个拷贝，也就是说，方法不能修改传递给它的任何参数变量的内容\nJava中对方法参数能做什么和不能做什么\n方法不能修改基本数据类型的参数 方法可以改变对象参数的状态 方法不能让一个对象参数引用一个新的对象 3.6 对象构造 重载：有些类有多个构造器，这种情况下，编译器会挑选出一个\u0026quot;最近匹配\u0026quot;的构造器，如果没有找到任何匹配的构造器，编译器就会报错\n默认字段初始化：如果再构造器中没有显示地给字段赋值，那么就会自动地将他们初始化为0、false或者null\n显式字段初始化：比如\n1private String name = \u0026#34;\u0026#34;; 2private double salary; 3private LocalDate hireDay; 调用另一个构造器：关键字this只是一个方法的隐式参数，当然还可以在一个构造器中用this调用另一个构造器，这个调用必须是构造器的第一个语句，例如：\n1public Employee(double s) 2{ 3 this(\u0026#34;Employee #\u0026#34; + nextId, s); 4} 注意：\n由于Java会完成自动的垃圾回收，不需要人工回收内存，所以Java不支持析构器；警告不要使用finalize方法来完成清理 1/* 2name=Harry,id=9794,salary=40000.0 3name=Employee #9795,id=9795,salary=60000.0 4name=,id=9796,salary=0.0 5 */ 6 7import java.util.*; 8 9public class ConstructorTest { 10 public static void main(String[] args) { 11 // 创建一个长度为3的EmployeeCt数组 12 EmployeeCt[] staff = new EmployeeCt[3]; 13 // 重载构造器，创建三个EmployeeCt对象 14 staff[0] = new EmployeeCt(\u0026#34;Harry\u0026#34;, 40000); 15 staff[1] = new EmployeeCt(60000); 16 staff[2] = new EmployeeCt(); 17 18 // 打印对象信息 19 for (EmployeeCt e : staff) { 20 System.out.println(\u0026#34;name=\u0026#34; + e.getName() + \u0026#34;,id=\u0026#34; + e.getId() + \u0026#34;,salary=\u0026#34; + e.getSalary()); 21 } 22 } 23} 24 25class EmployeeCt { 26 private static int nextId; 27 private int id; 28 private String name = \u0026#34;\u0026#34;; // instance field initialization 29 private double salary; 30 31 // static initialization block 32 // 所有的类，不管是不是实例化，都只执行最开始的一次 33 static { 34 Random generator = new Random(); 35 // set nextId to a random number between 0 and 9999 36 nextId = generator.nextInt(10000); 37 } 38 39 // object initialization block 40 { 41 id = nextId; 42 nextId++; 43 } 44 45 // 下面是三个重载的构造器 46 public EmployeeCt(String n, double s) { 47 name = n; 48 salary = s; 49 } 50 51 public EmployeeCt(double s) { 52 // calls the Employee(String, double) constructor 53 this(\u0026#34;Employee #\u0026#34; + nextId, s); 54 } 55 56 public EmployeeCt() { 57 // name initialized to \u0026#34;\u0026#34; --see above 58 // salary not explicitly set--initialized to 0 59 // id initialized in initialization block 60 } 61 62 public String getName() { 63 return name; 64 } 65 66 public double getSalary() { 67 return salary; 68 } 69 70 public int getId() { 71 return id; 72 } 73} 3.7 包 使用包的主要原因是确保类名的唯一性；类的导入示例：\n1import java.util.*; 2import java.sql.*; 有一种import语句允许导入静态方法和静态字段，而不只是类：\n1import static java.lang.System.*; 注意：\n如果没有在源文件中放置package语句，这个源文件中的类就属于无名包 编译器处理文件(带有文件分隔符和扩展名.java的文件), 而Java解释器(虚拟机)执行类(带有点号和.class扩展名的文件) 标记为public的部分可以由任意类使用，标记为private的部分只能由定义他们的类使用；如果没有指定访问修饰符，那么这个类只能被同一个包中的其他类访问 类存储在文件系统的子目录中，类的路径必须与包名匹配 类文件也可以存储在JAR(Java归档)文件中，在一个JAR文件中，可以包含多个压缩形式的类文件和子目录 1$ tree . 2. 3├── TestPackage.java 4├── com 5│ └── horstmann 6│ └── corejava 7│ └── TestClass.java 8 9$ cat TestPackage.java 10import com.horstmann.corejava.*; 11 12public class TestPackage { 13 public static void main(String[] args) { 14 // 不需要使用全限定名，因为有import语句 15 TestClass.main(args); 16 } 17} 18 19$ cat com/horstmann/corejava/TestClass.java 20package com.horstmann.corejava; 21 22public class TestClass { 23 public static void main(String[] args) { 24 System.out.println(\u0026#34;Hello, World!\u0026#34;); 25 } 26} 3.8 JAR文件 JAR文件是Java应用程序或库的打包方式，它将所有相关的类和资源打包到一个文件中，方便分发和部署。\nJAR的清单文件：清单文件是一个包含应用程序或者库的主清单属性的特殊文件，清单文件的名字是META-INF/MANIFEST.MF，清单文件的第一行必须是Manifest-Version: 1.0，清单文件中的每一行都是一个属性-值对，属性名和值之间用冒号分隔，属性名区分大小写，属性值前后可以有空格，属性值可以跨越多行，如果以空行开头，那么它就是前一个属性值的一部分，清单文件的最后一行必须以换行符结束\n3.9 文档注释 类注释：类注释必须放在import语句之后，类定义之前，比如：\n1/** 2* @version 1.01 2004-02-21 3* @author 4*/ 5public class ClassName 6{ 7 . . . 8} 方法注释：每一个方法注释必须放在所描述的方法之前：\n@param：参数的名字和说明 @return：返回值的说明 @exception：抛出的异常 比如：\n1/** 2* Computes the square root of a number. 3* @param x a nonnegative number 4* @return the square root of x 5* @throws IllegalArgumentException if x is negative 6*/ 7public static double sqrt(double x) 8{ 9 if (x \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;x must be nonnegative\u0026#34;); 10 return Math.sqrt(x); 11} 字段注释：只需要对公共字段建立\n1/** 2* The name of the employee. 3*/ 4private String name; 通用注释：\n@version：版本号 @since：自从哪个版本 @see：参考 @deprecated：不建议使用 3.10 类设计技巧 设计良好的类是编程的基础，以下是一些类设计的技巧：\n保证数据私有性。 对数据进行初始化。 不要过多使用基本数据类型: 如果一个类中有很多基本类型，那么就应该考虑将它分解成多个类 不要为每个字段都提供独立的访问器和更改器。 将职责过多的类进行分解。 类名和方法名应该能够清晰地反映其职责。 ","link":"https://zhangsiming-blyq.github.io/post/java/java%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF1/","section":"post","tags":["Java","中文"],"title":"【Java入门系列】Java基础入门 --- 安装，数据类型，类"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/java/","section":"tags","tags":null,"title":"Java"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/java/","section":"categories","tags":null,"title":"Java"},{"body":"Neovim, a powerful editor often hailed as a third-party extension for Vim, boasts an impressive 70k stars on GitHub. This robust tool makes for an excellent default editor choice. This guide will take you through the installation process and equip you with effective typing techniques.\nInstallation GitHub Link\nNeovim Installation On MacOS 1$ brew install neovim 2 3$ which nvim 4/usr/local/bin/nvim 5 6$ vim ~/.zshrc 7alias vim=\u0026#39;nvim\u0026#39; 8alias old_vim=\u0026#39;vim\u0026#39; 9 10# replace the normal vim with nvim 11$ which vim 12vim: aliased to nvim 13 14$ vim --version 15NVIM v0.7.2 16Build type: Release 17LuaJIT 2.1.0-beta3 18Compiled by brew@Monterey 19 20Features: +acl +iconv +tui 21See \u0026#34;:help feature-compile\u0026#34; 22 23 system vimrc file: \u0026#34;$VIM/sysinit.vim\u0026#34; 24 fall-back for $VIM: \u0026#34;/usr/local/Cellar/neovim/0.7.2_1/share/nvim\u0026#34; 25 26Run :checkhealth for more info Essential Components Installing a Plugin Manager If you haven't already, it's crucial to set up a plugin manager for seamless plugin installation and management in Neovim. Some popular options include vim-plug, dein.vim, and packer.nvim. In this guide, we'll demonstrate with vim-plug as the example.\nInstall vim-plug by following the instructions provided on its GitHub repository: https://github.com/junegunn/vim-plug\n1sh -c \u0026#39;curl -fLo \u0026#34;${XDG_DATA_HOME:-$HOME/.local/share}\u0026#34;/nvim/site/autoload/plug.vim --create-dirs \\ 2 https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\u0026#39; Configuring nvim's Initialization File Locate the configuration file at ~/.config/nvim/init.vim. This file plays a pivotal role in configuring Neovim and loading plugins effectively.\nPlugin Installation After saving your init.vim file, restart Neovim or run :source ~/.config/nvim/init.vim to apply the changes. Inside Neovim, input the following command to install the plugins:\n1:PlugInstall This command initiates the download and installation of all configured plugins mentioned in your configuration, simultaneously generating output in your vim console.\nCrafting Your Personalized Neovim Setup Let's delve into the intricacies of configuring each line in your initialization file:\n1set paste 2set number 3set relativenumber The initial three lines comprise simple settings for Neovim. The set paste command activates \u0026quot;paste mode,\u0026quot; effectively disabling auto-indentation and other automatic formatting adjustments when pasting text. This prevents unintentional formatting changes when pasting code from external sources. set number introduces line numbering, revealing line numbers on the left side of the buffer. set relativenumber displays relative line numbers instead of absolute ones. This approach designates the current line as 0, with other lines denoted by positive or negative offsets relative to the current line. This feature streamlines navigation within the buffer. 1set background=dark This line establishes the background color of the editor. While this configuration adopts dark mode, you can easily switch to a light-themed background by using light. 1set nocompatible This line eliminates compatibility with the old-time vi editor. As Neovim represents an enhanced and extended iteration of Vim, disabling compatibility ensures the availability of Neovim-specific features. You'll always need to include this line in your configuration file. 1set showmatch 2set ignorecase 3set mouse=v 4set hlsearch 5set incsearch set showmatch activates the highlighting of matching parentheses, brackets, and similar pairs when the cursor hovers over them. set ignorecase enforces case-insensitivity during search operations, allowing matches for both lowercase and uppercase instances. set mouse=v enables mouse support, facilitating navigation, split resizing, and middle-click pasting via the mouse. set hlsearch highlights search matches in real-time while entering the search query, enhancing visibility of match locations. set incsearch showcases incremental search outcomes as the search query evolves. The feature highlights partial matches while typing. 1set tabstop=4 2set softtabstop=4 3set expandtab 4set shiftwidth=4 5set autoindent 6set number set tabstop=4 establishes the count of columns occupied by a tab character, fixed at 4 in this case. set softtabstop=4 recognizes a group of spaces as a tab for indentation purposes, enabling Backspace to function as expected. set expandtab converts tab characters into spaces, ensuring uniform indentation, particularly when collaborating with others who might have varied tab settings. set shiftwidth=4 designates the number of spaces used for auto-indentation after pressing Enter. This setting governs the cursor's rightward movement during auto-indentation. set autoindent aligns new lines with the indentation of the previous line, maintaining consistency in indentation levels. 1set wildmode=longest,list 2set cc=80 set wildmode=longest,list simulates bash-like tab completion behavior. If only one match exists, it's automatically completed. With multiple matches, a list prompts you to choose. set cc=80 introduces a color column at the 80th column, serving as a visual guide to maintain code within a defined width, promoting clean coding practices. 1filetype plugin indent on 2syntax on 3set mouse=a 4set clipboard=unnamedplus 5filetype plugin on filetype plugin indent on enables filetype detection, thereby activating corresponding plugins and indent settings based on the file type. syntax on triggers syntax highlighting, offering color-coded visuals for diverse programming languages and file types. set mouse=a empowers mouse support, encompassing mouse clicks and scrolling functionality. set clipboard=unnamedplus directs Neovim to employ the system clipboard (if available) for copy and paste tasks, streamlining interaction with the system clipboard. 1set cursorline 2set ttyfast set cursorline accentuates the line where the cursor resides, simplifying tracking of the active line. set ttyfast optimizes scrolling performance within the terminal, leading to smoother scrolling within terminal-based Neovim. 1\u0026#34; set spell \u0026#34; enabling spell check (may require language package download) 2\u0026#34; set noswapfile \u0026#34; preventing swap file creation 3\u0026#34; set backupdir=~/.cache/vim \u0026#34; Directory for storing backup files. Lines commencing with \u0026quot; entail commented-out options, currently inactive within the configuration(I suggest not active these three configuration lines). set spell empowers spell checking functionality, highlighted by identifying misspelled words. set noswapfile inhibits the generation of swap files. While these files aid in recovering unsaved changes after crashes, deactivation implies potential loss of unsaved edits upon unexpected termination. set backupdir=~/.cache/vim specifies the directory for storing backup files. These duplicates of original files are generated before saving changes. Although the line mentions ~/.cache/vim, it remains commented out. 1call plug#begin(\u0026#39;~/.local/share/nvim/site/plugged\u0026#39;) 2 Plug \u0026#39;dracula/vim\u0026#39; 3 Plug \u0026#39;gruvbox-community/gruvbox\u0026#39; 4 Plug \u0026#39;ryanoasis/vim-devicons\u0026#39; 5 Plug \u0026#39;honza/vim-snippets\u0026#39; 6 Plug \u0026#39;scrooloose/nerdtree\u0026#39; 7 Plug \u0026#39;preservim/nerdcommenter\u0026#39; 8 Plug \u0026#39;mhinz/vim-startify\u0026#39; 9call plug#end() These lines leverage vim-plug to facilitate plugin management. The segment lies between call plug#begin() and call plug#end(). Each Plug line designates a plugin for installation and management. This configuration includes Dracula and Gruvbox themes, Devicons for file icons, Vim Snippets, NERDTree, NerdCommenter, Startify, and more. 1autocmd BufReadPost * 2 \\ if line(\u0026#34;\u0026#39;\\\u0026#34;\u0026#34;) \u0026gt; 0 \u0026amp;\u0026amp; line(\u0026#34;\u0026#39;\\\u0026#34;\u0026#34;) \u0026lt;= line(\u0026#34;$\u0026#34;) | 3 \\ exe \u0026#34;normal! g`\\\u0026#34;\u0026#34; | 4 \\ endif Remember the last cursor position when opening a file 1if has(\u0026#39;persistent_undo\u0026#39;) 2 \u0026#34; Set a directory to store undo files 3 let g:undo_dir = $HOME . \u0026#39;/.vim/undodir\u0026#39; 4 if !isdirectory(g:undo_dir) 5 call mkdir(g:undo_dir, \u0026#39;p\u0026#39;) 6 endif 7 8 \u0026#34; Store undo history in the specified directory 9 set undodir=$HOME/.vim/undodir 10 11 \u0026#34; Enable persistent undo 12 set undofile 13endif Enable undo history across sessions 1colorscheme gruvbox Set the theme to Gruvbox, it's also my favorite vim theme. 1\u0026#34; move line or visually selected block - alt+j/k 2inoremap \u0026lt;A-j\u0026gt; \u0026lt;Esc\u0026gt;:m .+1\u0026lt;CR\u0026gt;==gi 3inoremap \u0026lt;A-k\u0026gt; \u0026lt;Esc\u0026gt;:m .-2\u0026lt;CR\u0026gt;==gi 4vnoremap \u0026lt;A-j\u0026gt; :m \u0026#39;\u0026gt;+1\u0026lt;CR\u0026gt;gv=gv 5vnoremap \u0026lt;A-k\u0026gt; :m \u0026#39;\u0026lt;-2\u0026lt;CR\u0026gt;gv=gv 6 7\u0026#34; move split panes to left/bottom/top/right 8 nnoremap \u0026lt;A-h\u0026gt; \u0026lt;C-W\u0026gt;H 9 nnoremap \u0026lt;A-j\u0026gt; \u0026lt;C-W\u0026gt;J 10 nnoremap \u0026lt;A-k\u0026gt; \u0026lt;C-W\u0026gt;K 11 nnoremap \u0026lt;A-l\u0026gt; \u0026lt;C-W\u0026gt;L 12 13\u0026#34; move between panes to left/bottom/top/right 14 nnoremap \u0026lt;C-h\u0026gt; \u0026lt;C-w\u0026gt;h 15 nnoremap \u0026lt;C-j\u0026gt; \u0026lt;C-w\u0026gt;j 16 nnoremap \u0026lt;C-k\u0026gt; \u0026lt;C-w\u0026gt;k 17 nnoremap \u0026lt;C-l\u0026gt; \u0026lt;C-w\u0026gt;l 18 19\u0026#34; Press i to enter insert mode, and ii to exit insert mode. 20:inoremap ii \u0026lt;Esc\u0026gt; 21:inoremap jk \u0026lt;Esc\u0026gt; 22:inoremap kj \u0026lt;Esc\u0026gt; 23:vnoremap jk \u0026lt;Esc\u0026gt; 24:vnoremap kj \u0026lt;Esc\u0026gt; Some key-mapping tips for you, can be used according to personal habits. A Sneak Peek Now here is our ultimate Neovim configuration, Take a look at it!\nSome Valuable Features I Prefer Your history endures even after file closure, enabling convenient use of Ctrl + R or u for undoing changes. Cursor placement is preserved upon reopening files, proving particularly advantageous when working on substantial projects. Pasting maintains its original structure, preventing inadvertent indentation issues and code disruption. The chosen theme is Dracula, an impressive Vim theme renowned for its aesthetics and functionality. Now you've got a gorgeous and up-to-coming enhanced Vim, embrace Productive Coding, and wish You a Wonderful Day.\n","link":"https://zhangsiming-blyq.github.io/post/linux/neovim/","section":"post","tags":["vim","English"],"title":"Comprehensive Guide to Neovim"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/vim/","section":"tags","tags":null,"title":"vim"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/vim/","section":"categories","tags":null,"title":"vim"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/git/","section":"tags","tags":null,"title":"git"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/git/","section":"categories","tags":null,"title":"git"},{"body":" 参考链接: https://www.atlassian.com/git/tutorials/cherry-pick/ git 的 cherry-pick 操作简单来讲就是可以把具体的commit从一个分支，直接嫁接(复制)到另一个分支, 下面看一个例子: 1$ git branch 2* feat/siming 3 master 4$ git log 5commit 1d7df64add47be9891efa6469f663e78acf3982f (HEAD -\u0026gt; feat/siming, origin/feat/siming) 6Author: zhangsiming \u0026lt;zhangsiming@360.cn\u0026gt; 7Date: Fri Feb 10 20:18:59 2023 +0800 8 9 test2 10 11commit a272f807df9f22c58aa2a970ff26a13a66abec4d 12Author: zhangsiming \u0026lt;zhangsiming@360.cn\u0026gt; 13Date: Fri Feb 10 19:59:06 2023 +0800 14 15 test 16... 17 18# 可以看到feat/siming分支最近两个commit一个是test，一个是test2，我们现在记录一下test的commitId，然后把他cherry-pick到master分支 19$ git checkout master 20$ git cherry-pick a272f807df9f22c58aa2a970ff26a13a66abec4d 21 22# 大功告成，test部分的变更已经追加到了master分支，我们看一下git log graph(注意看HEAD指针位置) 23$ git log --pretty=oneline --graph --decorate --all 24 25* 1d7df64add47be9891efa6469f663e78acf3982f (origin/feat/siming, feat/siming) test2 26* a272f807df9f22c58aa2a970ff26a13a66abec4d test 27* 627f296be9f64418d4f6dfe99d2fcf6881196f30 (HEAD -\u0026gt; master, origin/master, origin/HEAD) Merge branch \u0026#39;feat/siming\u0026#39; into \u0026#39;master\u0026#39; 28|\\ 29| * 3a64ef1e9c78dba97b775ac6fcf3a1ecf0c7e925 fix: 优化gpu-alarmer 30| * 08358da0a98490e9e87a990342d13dbeffb7758f add: k8s-weekly-report 31| * 7231fc0a739fae55a5800e883d2e811b6c58e7f3 fix: 修改eventsinformer时间展示 32 33# 如果不想要这个commit了，可以reset回退(HEAD后面有几个^就回退几个commit，或者采用HEAD~n) 34$ git reset --hard HEAD^ ","link":"https://zhangsiming-blyq.github.io/post/linux/git%E7%9A%84cherry-pick%E6%93%8D%E4%BD%9C/","section":"post","tags":["shorthand","git","中文"],"title":"git的cherry-pick操作"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/kubernetes/","section":"tags","tags":null,"title":"kubernetes"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/kubernetes/","section":"categories","tags":null,"title":"kubernetes"},{"body":" kubernetes的cordon打上的SchedulingDisabled仅仅影响调度，也就是直接打上nodeName不会受到该参数的影响 kubernetes的QosClass判断pod内的全部container，包括init-container，也就是如果init-container不进行限制，其他container无论怎么配置仍然不是Guaranteed kubernetes集群新版本如果cordon打上unschedule，会默认追加Taint；旧版本不会 kubernetes集群对于unschedule的节点不会走入调度环节，只有可以正常调度的节点才会走到后面判断Toleration，label等；特别地，对于daemonset的pod，schedulingDisable无效，但是tolerance等有效(v1.17版本中，Damoneset 的 pod 的调度从 daemonset controller 迁移到 kube-scheduler 来做调度，从而支持 PodAffnity、PodAntiAffinity 等能力) Error(不再重启)，Completed状态的podip会显示，但是实际不占用podip，真实podip已经分配给其他服务使用 kubernetes中，kubelet限制的max-pod数量是限制的具体的pod数量，超出会报错Outofpods 每次pod重启，kubelet会给他分配新的cgroup目录路径，而不会使用原来的；新的pod启动之后间隔一小段时间会删除旧的cgroup路径 kubernetes 推荐使用 systemd 来代替 cgroupfs; 因为systemd是kubernetes自带的cgroup管理器, 负责为每个进程分配cgroups; 但docker的cgroup driver默认是cgroupfs,这样就同时运行有两个cgroup控制管理器；可以使用docker info查看docker使用的cgroup driver，然后从\u0026quot;/etc/docker/daemon.json\u0026quot;中修改成systemd ","link":"https://zhangsiming-blyq.github.io/post/kubernetes/kubernetes%E5%B8%B8%E8%A7%84%E9%97%AE%E9%A2%98/","section":"post","tags":["shorthand","kubernetes","中文"],"title":"kubernetes常规问题"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/linux/","section":"tags","tags":null,"title":"linux"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/linux/","section":"categories","tags":null,"title":"linux"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/shorthand/","section":"tags","tags":null,"title":"shorthand"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/shorthand/","section":"categories","tags":null,"title":"shorthand"},{"body":" 中断是什么？中断是一种电信号，由硬件产生并直接送到中断控制器上，再由中断控制器向CPU发送中断信号，CPU检测到信号后，中断当前工作转而处理中断信号；其实准确的说这种算硬中断 如果不像让这种中断，或者系统中断和网络中断和一些业务的中断在同一个cpu上面互相影响；可以把某个中断绑定到某几个特定的cpu核心，来达到目的 默认情况systemctl status irqbalance服务会平衡所有中断均衡地是用cpu 可以用echo cpu号 \u0026gt; /proc/irq/中断号/smp_affinity或者使用taskset来绑定中断到具体的cpu核心 ethtool -l eth0可以看到，一些通道信息(这个通道是可以触发网络中断的队列数量), 设置多了会影响内存等资源，设置小了可能会称为高流量瓶颈 参考链接：https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/interrupt_and_process_binding ","link":"https://zhangsiming-blyq.github.io/post/linux/%E4%B8%AD%E6%96%AD%E7%BB%91%E5%AE%9Acpu%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98/","section":"post","tags":["shorthand","linux","中文"],"title":"中断绑定cpu核心问题"},{"body":"\n该svc作为集群内部服务连接api-server的媒介(这三个信息会被注入到每个集群内部的pod中: KUBERNETES_SERVICE_HOST=10.96.0.1、KUBERNETES_SERVICE_PORT=443、KUBERNETES_SERVICE_PORT_HTTPS=443) 永远使用\u0026quot;--service-cluster-ip-range\u0026quot;定义的CIDR的第一个ip地址 svc以及对应的endpoints都是由master controller(api-server二进制文件在最开始启动的controller之一)管控 RunKubernetesService()是一个循环，里面的逻辑包含支持UpdateKubernetesService()更新这个svc信息，ReconcileEndpoints(...endpointPorts []corev1.EndpointPort...)来更新endpoint，也就是一般3个master的信息；不过有时候看到的endpoint可能只有一个ip，这可能是云厂商传入的master lb层 参考链接: https://networkop.co.uk/post/2020-06-kubernetes-default/ ","link":"https://zhangsiming-blyq.github.io/post/kubernetes/kubernetes-default-svc/","section":"post","tags":["shorthand","kubernetes","中文"],"title":"在default命名空间下的svc, kubernetes"},{"body":" For people new to Golang and Python, this article explains what closures are as well as sometips that need to know about when using closures in these two programming languages. To make it easier for you to copy down and run on your own server, the code section would provide the package, import, and other repeated parts. The negative aspect is that this results in some redundancy, so please understand if this causes any inconvenience.\nWhat Is Closures? a closure is a record storing a function together with an environment.\nClosure's two factors: function and environment\nfunction：Refers to the fact that when closures are actually implemented, they are often done by calling an external function that returns its internal function. An internal function may be an internal real-name function, an anonymous function, or a lambda expression. environment：In practice, the referenced environment is the environment of the external function and the closure holds/records all the environment of the external function at the time when it's being generated. To summarize, a closure is a function that extends the scope by retaining the bindings of free variables (variables not bound in the local scope) that existed when the function was defined, so that when the function is called, the definition scope is no longer available, but those bindings can still be used. Here is a look at the common usage of closures:\n1// golang version 2package main 3 4import \u0026#34;fmt\u0026#34; 5 6func outer() func(v int) { 7 x := 1 8 return func(v int) { 9 y := x + v 10 fmt.Println(x) 11 fmt.Println(y) 12 } 13} 14 15func main() { 16 a := outer() 17 a(1) 18 a(2) 19} 20 21// result 221 232 241 253 In the logic inside the golang example, \u0026quot;a\u0026quot; is a closure, the closure function is the internal func(v int){}, the closure environment is the external \u0026quot;x\u0026quot;, since the external environment is \u0026quot;captured\u0026quot;, so each execution of the closure \u0026quot;x\u0026quot; is 1, and the final output result is 1,2,1,3.\n1# python version 2def outer(): 3 x = 1 4 5 def inner(v): 6 y = x + v 7 print(x) 8 print(y) 9 return inner 10 11a = outer() 12a(1) 13a(2) 14 15# result 161 172 181 193 When comes to Python example, the same closure function is inner(v), the closure environment is \u0026quot;x\u0026quot;, so each time the closure is executed \u0026quot;x\u0026quot; is 1 (because the closure environment is captured), and then the logic inside the closure is to add \u0026quot;x\u0026quot; to the parameters passed in by the closure, so the final output is 1,2,1,3.\nClosures In Golang Anonymous Function Since anonymous functions are used very frequently in golang, let's start with this: Anonymous functions and the free variables they \u0026quot;capture\u0026quot; are called closures, and they are closed whether used normally, in a for loop, or in a defer:\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func main() { 6 x := 1 7 a := func(v int) { 8 y := x + v 9 fmt.Println(x) 10 fmt.Println(y) 11 } 12 a(2) 13 a(3) 14} 15 16// result 171 183 191 204 In the above example a is a closure, the closure function is the anonymous function func(v int){}, the closure environment is x, so the result is 1,3,1,4.\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func main() { 6 y := 2 7 8 defer func() { 9 fmt.Println(y) 10 }() 11} 12 13// result 142 The same goes for the anonymous function in defer, which is func(){} in defer, and the closure environment is \u0026quot;y\u0026quot;, so the output is 2.\nModifies The Closure Environment First of all, all reference passing in golang is a value passing, if there is something like reference passing (which will also be mentioned later in this article for convenience), it is actually the \u0026quot;value\u0026quot; of the underlying pointer that is passed, thus achieving the so-called reference passing.\nIf you want to modify the closure environment inside the closure (in the closure function), golang is easy to do that. Owing to the fact that golang is a declarative language, assignment and declaration are written differently(:= for declaration, = for assignment); and golang closure \u0026quot;captures\u0026quot; the essence of the closure environment is reference passing rather than value passing, so directly modify it is ok, see the following example: 1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func make_avg() func(v int) { 6 count := 0 7 total := 0 8 return func(v int) { 9 count += 1 10 total += v 11 fmt.Println(float32(total)/float32(count)) 12 } 13} 14 15func main() { 16 a := make_avg() 17 a(1) 18 a(2) 19 a(3) 20} 21 22// result 231 241.5 252 The example is to calculate the average value, the closure is \u0026quot;a\u0026quot;, the closure function is the internal func(v int){} anonymous function, the closure environment is \u0026quot;count\u0026quot; and \u0026quot;total\u0026quot;; count += 1, total += v are direct modifications to the behavior of the closure environment and get the desired effect.\nSpecifically, you can use golang closures to \u0026quot;catch\u0026quot; the essence of the closure environment is reference passing this feature inside the anonymous function (inside the closure function) modify the global variables (closure environment), see the following example:\n1package main 2 3import ( 4 \u0026#34;fmt\u0026#34; 5) 6 7var x int = 1 8 9func main() { 10 a := func() { 11 x += 1 12 } 13 fmt.Println(x) 14 a() 15 fmt.Println(x) 16} 17 18// result 191 202 Modify the closure environment outside the closure You must have some questions, \u0026quot;outside the closure\u0026quot; can modify the closure environment? In fact, it is possible in golang, remember the following two sentences:\nIf all the variables of the external function are local, that is, the life cycle ends when the external function ends, then the environment of the closure is also closed. Conversely, then the closure is no longer closed, and changes to globally visible variables will have an effect on the variables within the closure. Generally speaking, if the environment of a closure can be modified by a pointer, then the environment of the closure can be modified from outside the closure, see the following example:\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func foo1(x *int) func() { 6 return func() { 7 *x = *x + 1 8 fmt.Println(*x) 9 } 10} 11func foo2(x int) func() { 12 return func() { 13 x = x + 1 14 fmt.Println(x) 15 } 16} 17 18func main() { 19 x := 133 20 f1 := foo1(\u0026amp;x) 21 f2 := foo2(x) 22 f1() 23 f1() 24 f2() 25 f2() 26 27 x = 233 28 f1() 29 f1() 30 f2() 31 f2() 32 33 foo1(\u0026amp;x)() 34 foo1(\u0026amp;x)() 35 foo2(x)() 36 foo2(x)() 37} The logic inside the two closures needs to be analyzed, one is the sum of pointer variables, according to what we said earlier, \u0026quot;the closure environment can be modified by pointers\u0026quot;, each time the closure is executed or directly assigned outside will really change the value of the variable, while the \u0026quot;foo2\u0026quot;, which does not use pointers, is the normal closure, that is, the closure environment is only inside the closure; so the first four groups of output as follows:\n1134 2135 3134 4135 The middle four groups are being accumulated by the \u0026quot;f1\u0026quot; closure on the modified 233 because the value of \u0026quot;x\u0026quot; is forced externally, and the \u0026quot;f2\u0026quot; closure is being accumulated in its own environment, so the output is:\n1234 2235 3136 4137 The last four groups generate four new closures, so the \u0026quot;foo1\u0026quot; part is still cumulative based on the current \u0026quot;x\u0026quot; value, and the cumulative value actually acts in the global variable \u0026quot;x\u0026quot;; the cumulative in foo2 is still inside its own closure, so the output is:\n1236 2237 3238 4238 With this example we distinguish between modifying the closure environment from inside the closure and from outside the closure.\nDelayed Binding Of Closures This problem is every golang newbie will encounter, very puzzling problem; please remember the following sentence: when executing the closure, the closure environment declaration cycle is guaranteed, and will go to the external environment to find the latest closure environment (value), the following example when executing the closure \u0026quot;i\u0026quot; is the closure environment, when executing the closure the latest value is already 10, so all will output 10.\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func main() { 6 var handlers []func() 7 for i := 0; i \u0026lt; 10; i++ { 8 handlers = append(handlers, func() { 9 fmt.Println(i) 10 }) 11 } 12 for _, handler := range handlers { 13 handler() 14 } 15} 16 17// result 1810 1910 2010 2110 2210 2310 2410 2510 2610 2710 The solution is to copy an environment variable in the for loop that is not referenced by the closure, and then use that value instead of the closure environment, with the following modified version:\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func main() { 6 var handlers []func() 7 for i := 0; i \u0026lt; 10; i++ { 8 // a is not a closure environment because it is redeclared each time 9 a := i 10 handlers = append(handlers, func() { 11 fmt.Println(a) 12 }) 13 } 14 for _, handler := range handlers { 15 handler() 16 } 17} 18 19// result 200 211 222 233 244 255 266 277 288 299 In fact, the principle is clear, it is not about the for loop, normal use, defer use will be bound by this principle, the execution of the closure, the closure environment declaration cycle is guaranteed, and will go to the external environment to find the latest closure environment (value)\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func main() { 6 x, y := 1, 2 7 8 defer func(a int) { 9 fmt.Println(a, y) 10 }(x) 11 12 x += 100 13 y += 100 14} 15 16// The output, y, is the closure environment, so the execution of the closure will go to the latest value, while a is not the closure environment, copying the value of x so it is not implicated 171 102 The use of anonymous functions in go routines is a common scenario and suffers from this problem, see the following example:\n1package main 2 3import ( 4 \u0026#34;fmt\u0026#34; 5 \u0026#34;time\u0026#34; 6) 7 8func show(val int) { 9 fmt.Println(val) 10} 11 12func main() { 13 values := []int{1, 2, 3, 5} 14 for _, val := range values { 15 go show(val) 16 } 17 time.Sleep(time.Second) 18} 19 20// The four outputs 1,2,3,5 will be output each time, although in a different order, because no anonymous function is used, and it is not a closure 215 221 233 242 The four outputs 1,2,3,5 will be output each time, although in a different order, because no anonymous function is used, and it is not a closure.\n1package main 2 3import ( 4 \u0026#34;fmt\u0026#34; 5 \u0026#34;time\u0026#34; 6) 7 8func main() { 9 values := []int{1, 2, 3, 5} 10 for _, val := range values { 11 go func(){ 12 fmt.Println(val) 13 }() 14 } 15 time.Sleep(time.Second) 16} 17 18// output 195 205 215 225 The modification method is the same as inside the for loop example, using passing variables to avoid the closure environment.\n1package main 2 3import ( 4 \u0026#34;fmt\u0026#34; 5 \u0026#34;time\u0026#34; 6) 7 8func main() { 9 values := []int{1, 2, 3, 5} 10 for _, val := range values { 11 go func(val int){ 12 fmt.Println(val) 13 }(val) 14 } 15 time.Sleep(time.Second) 16} 17 18// output 191 205 213 222 Closures In Python Modifies The Closure Environment Modifying the closure environment from within Since python is not a declarative language, a \u0026quot;=\u0026quot; eats all, we need to use the nonlocal parameter to explicitly declare the variable as a closure environment, and not a local variable, look at the following example, if the same way as golang directly change, sometimes there will be a problem.\nuse Python list as a closure environment\n1def make_avg(count, total): 2 count = [] 3 4 def avg(v): 5 count.append(v) 6 total = sum(count) 7 print(sum(count)/len(count)) 8 return avg 9 10 11a = make_avg(0, 0) 12a(1) 13a(2) 14a(3) 15 16 17# output 181.0 191.5 202.0 use Python character as a closure environment\n1def make_avg(count, total): 2 count = 0 3 total = 0 4 5 def avg(v): 6 count += 1 7 total += v 8 print(total/count) 9 return avg 10 11 12a = make_avg(0, 0) 13a(1) 14a(2) 15a(3) 16 17 18# error 19Traceback (most recent call last): 20 File \u0026#34;/root/code/linux/blog/aa.py\u0026#34;, line 14, in \u0026lt;module\u0026gt; 21 a(1) 22 File \u0026#34;/root/code/linux/blog/aa.py\u0026#34;, line 7, in avg 23 count += 1 24UnboundLocalError: local variable \u0026#39;count\u0026#39; referenced before assignment That is because python has two types of data, one for mutable data types, one for immutable data types, when passing variable data types use reference passing, this and golang closure directly modified characteristics match, but immutable data types such as the above string will be a problem, because python will take him as a newly generated local variables, so the solution is to use nonlocal to tell the python interpreter, This variable is for the closure environment, so that it can run properly, the correct way to write the following:\n1def make_avg(count, total): 2 count = 0 3 total = 0 4 5 def avg(v): 6 nonlocal count, total 7 count += 1 8 total += v 9 print(total/count) 10 return avg 11 12 13a = make_avg(0, 0) 14a(1) 15a(2) 16a(3) 17 18# output 191.0 201.5 212.0 Closures And Decorators I was going to talk about python decorators, but I decided to leave it for another time because there is already a lot of space here.\nSummary There are many examples above, after reading each example, to understand why, then the closure problem will be solved;\nKeep in mind, cuz the closure is not commonly used, and it is not very practical, you don't have to use closures if the work is not necessary to use. Or may trigger some memory leaks, which is not a small problem.\ntwo main elements of the closure: function and environment closure is a function that extends the scope, it will retain the bindings of free variables (variables not bound in the local scope) that existed when the function was defined, so when the function is called, so the definition scope is not available, but can still use those bindings the use of anonymous functions in golang is actually a closure the closure environment variables can be modified from outside the closure by pointers golang closures delayed binding problem: when executing a closure, the declaration cycle of the closure environment is guaranteed, and it will go to the external environment to find the latest closure environment (value) python closures use nonlocal to declare variables as closure environment, instead of local variables The above analysis is my understanding of the closure, I hope golang, python newcomers can avoid a must-step pit after reading, if you have questions about the environment feel free to comment or email contact, thank you.\nReference Links\nhttps://juejin.cn/post/6844904133208604679\nhttps://www.jianshu.com/p/fa21e6fada70\nhttps://zhuanlan.zhihu.com/p/92634505\n","link":"https://zhangsiming-blyq.github.io/post/golang/closure/","section":"post","tags":["golang","python","English"],"title":"Closures In Golang And Python"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/golang/","section":"tags","tags":null,"title":"golang"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/golang/","section":"categories","tags":null,"title":"golang"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/python/","section":"tags","tags":null,"title":"python"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/python/","section":"categories","tags":null,"title":"python"},{"body":" 近期配置了uber家的zap日志库，觉得性能比较强，展示比较美观，在这里做一个分享，代码在第三部分可以自取。\n为什么不选择原生log? 说起golang如何优雅的打印日志，任何一个golang的初学者大概都是用的原生log库，或者直接fmt.Println()...但是这种方式并不优雅，并且有以下缺点：\n对于基础日志：不能细粒度区分info和debug级别的日志; 对于错误日志: 不支持除了fatal或者panic的普通error级别告知。 log示例 1package main 2 3import \u0026#34;log\u0026#34; 4 5func main() { 6 log.Print(\u0026#34;info or debug\u0026#34;) 7 log.Fatal(\u0026#34;fatal\u0026#34;) 8 log.Panic(\u0026#34;panic\u0026#34;) 9} 10 11// 输出如下 122022/11/23 22:23:32 info or debug 132022/11/23 22:23:32 fatal 14exit status 1 为什么不选择logrus? logrus也是比较常用的自定义日志库，不过因为Go语言是一门强类型的静态语言，而logrus需要知道数据的类型来打印日志，怎么办呢？实现方案是使用反射，这导致大量分配计数。虽然通常不是一个大问题（取决于代码），但是在大规模、高并发的项目中频繁的反射开销影响很大，所以这里不进行采用。\n仓库链接: logrus\nlogrus示例 1package main 2 3import log \u0026#34;github.com/sirupsen/logrus\u0026#34; 4 5var logger = log.New() 6 7func main() { 8 // 这里可以通过WithFields来附加字段 9 logger.WithFields(log.Fields{\u0026#34;testfield\u0026#34;: \u0026#34;test\u0026#34;}).Info(\u0026#34;test info\u0026#34;) 10 logger.Info(\u0026#34;info\u0026#34;) 11 logger.Error(\u0026#34;Error\u0026#34;) 12 logger.Fatal(\u0026#34;Error\u0026#34;) 13 logger.Panic(\u0026#34;Panic\u0026#34;) 14} 15 16// 输出如下 17INFO[0000] test info testfield=test 18INFO[0000] info 19ERRO[0000] Error 20FATA[0000] Error 21exit status 1 聊一聊zap日志库 仓库链接：zap\n1. zap支持的六种日志级别 名称 作用 Debug 打印调试时候的debug日志 Info 正常输出普通信息 Warn 警告，可能有部分出现问题但是不影响程序运行 Error 错误，不会中断程序运行但是程序可能已经不正常 Fatal 输出信息，然后调用os.Exit Panic 调用panic 个人建议如果有错就在初始化检查中可以panic掉，之后程序运行期间就不要碰到小错误就panic掉了，能容忍就抛出Error，这样方便程序员主动停掉服务排查而不是已经工作起来的程序异常挂掉。\n2. zap的性能问题及benchmark 官方github在Performance模块中明确说道:\nFor applications that log in the hot path, reflection-based serialization and string formatting are prohibitively expensive — they're CPU-intensive and make many small allocations. Put differently, using encoding/json and fmt.Fprintf to log tons of interface{}s makes your application slow.\nZap takes a different approach. It includes a reflection-free, zero-allocation JSON encoder, and the base Logger strives to avoid serialization overhead and allocations wherever possible. By building the high-level SugaredLogger on that foundation, zap lets users choose when they need to count every allocation and when they'd prefer a more familiar, loosely typed API.\n也就是说，基于反射的序列化，或者字符串格式化这种是很吃cpu资源的，严重了会导致程序变慢(logrus存在这个问题); zap这里定义了一个无反射的，无分配的json encoder来优化这一部分，并且在此基础上提供了Sugar可以舍弃部分性能换取更简单的配置这一特点，我们下面的部分会演示。\n通过benckmark可以看出, zap和zap-sugar在性能上还是非常有优势的!\n3. 代码展示 下面分段展示完整代码，第一部分是各种包的导入；值得注意的是定义了默认的DefaultLog，用于把Info级别以上的日志人性化可读地输出到控制台; 还支持通过给InitLogger函数传递参数自定义想要的日志形式，支持(假设库名叫logger)：\n初始化方式 日志形式 logger.DefaultLog 人性化输出到控制台, 输出高于Info级别的日志 logger.InitLogger(\u0026quot;\u0026quot;, \u0026quot;console\u0026quot;, \u0026quot;debug\u0026quot;).Sugar() 人性化输出到控制台, 输出高于Debug级别的日志 logger.InitLogger(\u0026quot;\u0026quot;, \u0026quot;file\u0026quot;, \u0026quot;debug\u0026quot;).Sugar() 人性化输出到文件(默认当前目录下的log目录，会自动创建), 输出高于Debug级别的日志 logger.InitLogger(\u0026quot;json\u0026quot;, \u0026quot;console\u0026quot;, \u0026quot;debug\u0026quot;).Sugar() json格式输出到控制台, 输出高于Debug级别的日志 1package logger 2 3import ( 4\t\u0026#34;os\u0026#34; 5 6\t\u0026#34;github.com/natefinch/lumberjack\u0026#34; 7\t\u0026#34;go.uber.org/zap\u0026#34; 8\t\u0026#34;go.uber.org/zap/zapcore\u0026#34; 9) 10 11var ( 12\tMyLogger = InitLogger() 13\tDefaultLog = MyLogger.Sugar() 14) 15 16func InitLogger(logArgs ...string) *zap.Logger { 17 ... 18} InitLogger函数中大概分为四个部分:\n接收参数初始化变量 生成encoder 定义日志级别和输出形式 根据指定配置生成并返回日志实例 代码 1func InitLogger(logArgs ...string) *zap.Logger { 2\tvar logger *zap.Logger 3\tvar coreArr []zapcore.Core 4\tvar format, logType, priority string 5 6\t// get the parameters 7\tswitch { 8\tcase len(logArgs) \u0026gt;= 3: 9\tformat = logArgs[0] 10\tlogType = logArgs[1] 11\tpriority = logArgs[2] 12\tcase len(logArgs) == 2: 13\tformat = logArgs[0] 14\tlogType = logArgs[1] 15\tcase len(logArgs) == 1: 16\tformat = logArgs[0] 17\t} 18 19\t// get encoder 20\tencoderConfig := zap.NewProductionEncoderConfig() 21\tencoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder // time format 22\tencoderConfig.EncodeLevel = zapcore.CapitalColorLevelEncoder // use different color for various log levels 23\t// uncomment next line to show full path of the code 24\t// encoderConfig.EncodeCaller = zapcore.FullCallerEncoder 25\t// NewJSONEncoder() for json，NewConsoleEncoder() for normal 26\tif format == \u0026#34;\u0026#34; { 27\tformat = \u0026#34;normal\u0026#34; 28\t} 29\tencoder := zapcore.NewConsoleEncoder(encoderConfig) 30\tif format == \u0026#34;json\u0026#34; { 31\tencoder = zapcore.NewJSONEncoder(encoderConfig) 32\t} 33 34\t// log levels 35\terrorPriority := zap.LevelEnablerFunc(func(lev zapcore.Level) bool { 36\treturn lev \u0026gt;= zap.ErrorLevel 37\t}) 38\tinfoPriority := zap.LevelEnablerFunc(func(lev zapcore.Level) bool { 39\treturn lev \u0026lt; zap.ErrorLevel \u0026amp;\u0026amp; lev \u0026gt;= zap.InfoLevel 40\t}) 41\tdebugPriority := zap.LevelEnablerFunc(func(lev zapcore.Level) bool { 42\treturn lev \u0026lt; zap.InfoLevel \u0026amp;\u0026amp; lev \u0026gt;= zap.DebugLevel 43\t}) 44\tif logType == \u0026#34;\u0026#34; { 45\tlogType = \u0026#34;console\u0026#34; 46\t} 47\t// writeSyncer for debug file 48\tdebugFileWriteSyncer := zapcore.AddSync(\u0026amp;lumberjack.Logger{ 49\tFilename: \u0026#34;./log/debug.log\u0026#34;, // will create if not exist 50\tMaxSize: 128, // max size for log file, unit:MB 51\tMaxBackups: 3, // max backup\u0026#39;s count 52\tMaxAge: 10, // max reserved days for log file 53\tCompress: false, // whether to compress or not 54\t}) 55\tdebugFileCore := zapcore.NewCore(encoder, os.Stdout, debugPriority) 56\tif logType == \u0026#34;file\u0026#34; { 57\tdebugFileCore = zapcore.NewCore(encoder, zapcore.NewMultiWriteSyncer(debugFileWriteSyncer, zapcore.AddSync(os.Stdout)), debugPriority) 58\t} 59\t// writeSyncer for info file 60\tinfoFileWriteSyncer := zapcore.AddSync(\u0026amp;lumberjack.Logger{ 61\tFilename: \u0026#34;./log/info.log\u0026#34;, 62\tMaxSize: 128, 63\tMaxBackups: 3, 64\tMaxAge: 10, 65\tCompress: false, 66\t}) 67\tinfoFileCore := zapcore.NewCore(encoder, os.Stdout, infoPriority) 68\tif logType == \u0026#34;file\u0026#34; { 69\tinfoFileCore = zapcore.NewCore(encoder, zapcore.NewMultiWriteSyncer(infoFileWriteSyncer, zapcore.AddSync(os.Stdout)), infoPriority) 70\t} 71\t// writeSyncer for error file 72\terrorFileWriteSyncer := zapcore.AddSync(\u0026amp;lumberjack.Logger{ 73\tFilename: \u0026#34;./log/error.log\u0026#34;, 74\tMaxSize: 128, 75\tMaxBackups: 5, 76\tMaxAge: 10, 77\tCompress: false, 78\t}) 79\terrorFileCore := zapcore.NewCore(encoder, os.Stdout, errorPriority) 80\tif logType == \u0026#34;file\u0026#34; { 81\terrorFileCore = zapcore.NewCore(encoder, zapcore.NewMultiWriteSyncer(errorFileWriteSyncer, zapcore.AddSync(os.Stdout)), errorPriority) 82\t} 83 84\tswitch priority { 85\tcase \u0026#34;\u0026#34;: 86\tcoreArr = append(coreArr, infoFileCore) 87\tcoreArr = append(coreArr, errorFileCore) 88\tcase \u0026#34;info\u0026#34;: 89\tcoreArr = append(coreArr, infoFileCore) 90\tcoreArr = append(coreArr, errorFileCore) 91\tcase \u0026#34;error\u0026#34;: 92\tcoreArr = append(coreArr, errorFileCore) 93\tcase \u0026#34;debug\u0026#34;: 94\tcoreArr = append(coreArr, debugFileCore) 95\tcoreArr = append(coreArr, infoFileCore) 96\tcoreArr = append(coreArr, errorFileCore) 97\t} 98\tlogger = zap.New(zapcore.NewTee(coreArr...), zap.AddCaller()) //zap.AddCaller() is to show the line number 99\treturn logger 100} 4. 效果演示 1package main 2 3import ( 4\t\u0026#34;xxx/pkg/logger\u0026#34; 5) 6 7// var log = logger.DefaultLog 8var log = logger.InitLogger(\u0026#34;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;debug\u0026#34;).Sugar() 9 10func main() { 11\tlog.Debug(\u0026#34;debug\u0026#34;) 12\tlog.Info(\u0026#34;info\u0026#34;) 13\tlog.Warn(\u0026#34;warn\u0026#34;) 14\tlog.Error(\u0026#34;error\u0026#34;) 15\tlog.Fatal(\u0026#34;fatal\u0026#34;) 16\tlog.Panic(\u0026#34;panic\u0026#34;) 17} 输出 在性能开销很小的情况下，还可以清晰的展示日志级别，并且使用颜色区分, 十分强大美观，欢迎读者拷贝并使用我个人这份配置，有疑问我们下面评论区随时沟通探讨。\n","link":"https://zhangsiming-blyq.github.io/post/golang/gozap/","section":"post","tags":["golang","中文"],"title":"使用zap打造你的golang日志库"},{"body":"Requirements There is a integer array arr and a window of size w that slides from the leftmost to the rightmost part of the array, with the window sliding one position at a time to the right. For example, the array is [4,3,5,4,3,3,6,7], and the window size is 3.\nIf the length of the array is n and the window size is w, then a total of n-w+1 window maxima are generated. Please implement a function:\nInput: a integer array arr with window size w output: an array res of length n-w+1, res[i] denotes the maximum value in each window state, requiring time complexity O(N*w) Solution 整体逻辑：\n创建一个双端数组，我们需要根据不同情况从两端分别push和pop 核心就是循环这个arr，把最大的值在arr中的下标放到双端数组的最左端(对应下文程序中\u0026quot;for !bq.Empty() \u0026amp;\u0026amp; arr[i] \u0026gt; arr[bq.Back().(int)] {}\u0026quot;的处理) 当窗口走过对应数值区域的时候，把相应的数据进行过期(对应下文程序中\u0026quot;if bq.Front().(int) == i-w {}\u0026quot;) 注意，因为要输出长度为 n-w+1, 所以循环当\u0026quot;i \u0026gt;= w -1\u0026quot;的时候，才开始输出结果到res中 Overall logic:\ncreate a double-ended array, we need to push and pop from both ends according to different situations The core is to loop through the arr and put the largest value subscript in arr at the leftmost end of the double-ended array (corresponding to the following procedure \u0026quot;for !bq.Empty() \u0026amp;\u0026amp; arr[i] \u0026gt; arr[bq. (int)] {}\u0026quot;) expire the corresponding data when the window goes through the corresponding value area (corresponding to the following program \u0026quot;if bq.Front(). (int) == i-w {}\u0026quot;) Note that since the length of the output is n-w+1, the loop starts when \u0026quot;i \u0026gt;= w -1\u0026quot; before outputting the result into res Golang Implementation 1package main 2 3func Getmaxwindow(arr []int, w int) (res []int) { 4\tres = []int{} 5\t// 1. create a bidirectional queue to store the result 6\tbq := newCustomQueue() 7\t// 2. for i \u0026lt; length(arr) 8\tfor i := 0; i \u0026lt; len(arr); i++ { 9\t// 3. for !empty or value(in queue) \u0026gt; value(in bqueue), popback bqueue 10\tfor !bq.Empty() \u0026amp;\u0026amp; arr[i] \u0026gt; arr[bq.Back().(int)] { 11\tbq.PopBack() 12\t} 13\t// 4. pushback id(in queue) 14\tbq.Pushback(i) 15\t// 5. if bqueue\u0026#39;s id \u0026gt;= i - w, expire the front value(in bqueue) 16\tif bq.Front().(int) == i-w { 17\tbq.PopFront() 18\t} 19\t// 6. record results to res slice 20\tif i \u0026gt;= w-1 { 21\tres = append(res, arr[bq.Front().(int)]) 22\t} 23\t} 24\treturn res 25} Test Cases 1package main 2 3import ( 4\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 5\t\u0026#34;testing\u0026#34; 6) 7 8func TestGetmaxwindow(t *testing.T) { 9\ttests := []struct { 10\tname string 11 12\tarr []int 13\tw int 14 15\twantRes []int 16\t}{ 17\t{ 18\tname: \u0026#34;getmaxwindow\u0026#34;, 19\tarr: []int{4, 3, 5, 4, 3, 3, 6, 7}, 20\tw: 3, 21\twantRes: []int{5, 5, 5, 4, 6, 7}, 22\t}, 23\t{ 24\tname: \u0026#34;getmaxwindow1\u0026#34;, 25\tarr: []int{4, 3, 5, 4, 3, 3, 6, 7}, 26\tw: 4, 27\twantRes: []int{5, 5, 5, 6, 7}, 28\t}, 29\t{ 30\tname: \u0026#34;getmaxwindow2\u0026#34;, 31\tarr: []int{4, 3, 9, 8, 3, 1, 6, 7}, 32\tw: 3, 33\twantRes: []int{9, 9, 9, 8, 6, 7}, 34\t}, 35\t} 36\tfor _, tt := range tests { 37\tt.Run(tt.name, func(t *testing.T) { 38\t// test case 39\tres := Getmaxwindow(tt.arr, tt.w) 40\tassert.Equal(t, tt.wantRes, res) 41\t}) 42\t} 43} Result:\n1$ go test -run ^TestSortStark$ . -v 2$ go test -run ^TestGetmaxwindow$ . -v 3=== RUN TestGetmaxwindow 4=== RUN TestGetmaxwindow/getmaxwindow 5=== RUN TestGetmaxwindow/getmaxwindow1 6=== RUN TestGetmaxwindow/getmaxwindow2 7--- PASS: TestGetmaxwindow (0.00s) 8 --- PASS: TestGetmaxwindow/getmaxwindow (0.00s) 9 --- PASS: TestGetmaxwindow/getmaxwindow1 (0.00s) 10 --- PASS: TestGetmaxwindow/getmaxwindow2 (0.00s) 11PASS The test is problem-free and the output is as expected.\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/6/","section":"post","tags":["algorithm","English"],"title":"ALGORITHM SERIES | Generate an array of window maximums"},{"body":" kubernetes各个组件都是加密通信的, 那么都有哪些证书、各个证书怎么交互、这些证书什么时候过期，这个就变得至关重要; 本文引用了一些其他网络内容(均已附上原文链接)，并适当补充完善，用于让新手完善熟悉kubernetes证书体系(如有侵权联系邮箱可以删除)。\n一、数字证书原理 1.1 传统非对称加密 1message --\u0026gt; (公钥加密) --\u0026gt; || 传输 || --\u0026gt; (私钥解密) --\u0026gt; message 注意:\n1.这里与数字证书认证相反，是公钥加密私钥解密\n2.公钥私钥需要是一个秘钥对\n1.2 哈希函数 1message --\u0026gt; H(message) --\u0026gt; Hash message 处理加入一个随机数，然后得出结果(加盐); 可以有效缓解在输入值是一个有效的集合，哈希值也是固定长度被别人‘试’出来的几率\n1message --\u0026gt; H(R|message) --\u0026gt; Hash message 1.3 数字证书 1.3.1 数字签名 数字签名；把数据根据私钥/哈希进行加密，然后必须要对应的公钥来进行解密认证才能确保数据安全。前半句的加密过程就叫做 '数字签名'\n1.3.2 数字证书认证过程 Alice 想要通过证书加密让 Bob 安全读到自己的信息流程如下：\nAlice 在本地生成 Private Key 和 CSR（Certificate Signing Request）。CSR 中包含了 Alice 的公钥和姓名，机构、地址等身份信息。 Alice 使用该 CSR 向证书机构发起数字证书申请。 证书机构验证 Alice 的身份后，使用 CSR 中的信息生成数字证书，并使用自己的 CA 根证书对应的私钥对该证书签名。 Alice 使用自己的 Private Key 对合同进行签名，然后将签名后的合同和自己的证书一起并发送给 Bob。 Bob 使用操作系统中自带的证书机构根证书中的公钥来验证 Alice 证书中的签名，以确认 Alice 的身份和公钥。(使用内置根证书确认身份并获取Alice证书) Alice 的证书验证成功后，Bob 使用 Alice 证书中的公钥来验证合同中数字签名。(使用刚刚获取的Alice证书(公钥)解析Alice发送的内容) 合同数字签名通过验证，可以证明该合同为 Alice 本人发送，并且中间未被第三方篡改过。 注意：\n1.自签发证书：证书签发商和证书持有者是同一个人，缺点是没有人能告诉你\u0026quot;这个人就是这个人(我就是我)\u0026quot;; 所以需要上面的信任证书机构的介入，多一环进行认证证明\n2.证书链：但是信任的证书机构的根证书是很机密的，一旦被盗取可能会导致很多人都无法证明\u0026quot;我就是我\u0026quot;, 所以一般会引入一些中间证书商，多一层上面的解析循环；这样也保证了万一中间商证书泄露，不至于全部使用证书机构的人都陷于危险之中\n3.证书签发一般都是有一定时期的, 过期了就如同废纸\n参考链接：\nhttps://zhaohuabing.com/post/2020-03-19-pki/\n二、双向 TLS 认证原理 双向 TLS 认证需要的场景是，服务端和客户端都需要确认，这样就是正反都走一遍上面的流程，来完成双向的数据加密，保证双向的数据安全\n参考链接：\nhttps://medium.com/sitewards/the-magic-of-tls-x509-and-mutual-authentication-explained-b2162dec4401/\n三、Kubernetes 中的证书工作机制 3.1 Kubernetes 中使用到的主要证书 etcd 集群中各个节点之间相互通信使用的证书。由于一个 etctd 节点既为其他节点提供服务，又需要作为客户端访问其他节点，因此该证书同时用作服务器证书和客户端证书 etcd 集群向外提供服务使用的证书。该证书是服务器证书 kube-apiserver 作为客户端访问 etcd 使用的证书。该证书是客户端证书 kube-apiserver 对外提供服务使用的证书。该证书是服务器证书 kube-controller-manager 作为客户端访问 kube-apiserver 使用的证书,该证书是客户端证书 kube-scheduler 作为客户端访问 kube-apiserver 使用的证书,该证书是客户端证书 kube-proxy 作为客户端访问 kube-apiserver 使用的证书,该证书是客户端证书 kubelet 作为客户端访问 kube-apiserver 使用的证书,该证书是客户端证书 管理员用户通过 kubectl 访问 kube-apiserver 使用的证书,该证书是客户端证书 kubelet 对外提供服务使用的证书。该证书是服务器证书 kube-apiserver 作为客户端访问 kubelet 采用的证书。该证书是客户端证书 kube-controller-manager 用于生成和验证 service-account token 的证书。该证书并不会像其他证书一样用于身份认证，而是将证书中的公钥/私钥对用于 service account token 的生成和验证。kube-controller-manager 会用该证书的私钥来生成 service account token，然后以 secret 的方式加载到 pod 中。pod 中的应用可以使用该 token 来访问 kube-apiserver， kube-apiserver 会使用该证书中的公钥来验证请求中的 token 注意:\n只有当你运行 kube-proxy 并要支持 扩展 API 服务器 时，才需要 front-proxy 证书 3.2 etcd 证书 1$ ssh 10.20.11.120 2$ sudo systemctl status etcd 3● etcd.service - etcd docker wrapper 4 Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled) 5 Active: active (running) since Mon 2020-08-03 15:42:36 CST; 1 months 18 days ago 6$ cat /etc/etcd.env | tail -13 7 8# TLS settings 9ETCD_TRUSTED_CA_FILE=/etc/ssl/etcd/ssl/ca.pem # etcd验证访问 etcd 服务器的客户端证书的 CA 根证书, 服务端签名证书和服务端私钥 10ETCD_CERT_FILE=/etc/ssl/etcd/ssl/member-ip-10-20-11-120.pem 11ETCD_KEY_FILE=/etc/ssl/etcd/ssl/member-ip-10-20-11-120-key.pem 12ETCD_CLIENT_CERT_AUTH=true 13 14ETCD_PEER_TRUSTED_CA_FILE=/etc/ssl/etcd/ssl/ca.pem # etcd peer之间验证访问 etcd 服务器的客户端证书的 CA 根证书, 服务端签名证书和服务端私钥 15ETCD_PEER_CERT_FILE=/etc/ssl/etcd/ssl/member-ip-10-20-11-120.pem 16ETCD_PEER_KEY_FILE=/etc/ssl/etcd/ssl/member-ip-10-20-11-120-key.pem 17ETCD_PEER_CLIENT_CERT_AUTH=True 18 19# 验证etcd证书过期时间 20$ sudo openssl x509 -in /etc/ssl/etcd/ssl/ca.pem -noout -dates 21notBefore=Aug 6 05:07:43 2019 GMT 22notAfter=Jul 13 05:07:43 2119 GMT 3.3 kube-apiserver 证书 1$ ssh 10.20.11.120 2$ cd /etc/kubernetes 3$ cat manifests/kube-apiserver.yaml 4apiVersion: v1 5kind: Pod 6metadata: 7 creationTimestamp: null 8 labels: 9 component: kube-apiserver 10 tier: control-plane 11 name: kube-apiserver 12 namespace: kube-system 13spec: 14 containers: 15 - command: 16 - kube-apiserver 17 - --advertise-address=10.20.11.120 18 - --etcd-cafile=/etc/ssl/etcd/ssl/ca.pem # 用于验证 etcd 客户端证书的 CA 根证书, 用于访问 etcd 的客户端证书和私钥 19 - --etcd-certfile=/etc/ssl/etcd/ssl/node-ip-10-20-11-120.pem 20 - --etcd-keyfile=/etc/ssl/etcd/ssl/node-ip-10-20-11-120-key.pem 21 - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver-kubelet-client.crt # 用于访问 kubelet 的客户端证书和私钥 22 - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-kubelet-client.key 23 - --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.crt # 只有当你运行 kube-proxy 并要支持 扩展 API 服务器 时，才需要 front-proxy 证书 24 - --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client.key 25 - --client-ca-file=/etc/kubernetes/ssl/ca.crt # 用于验证访问 kube-apiserver 的客户端的证书的 CA 根证书 26 - --service-account-key-file=/etc/kubernetes/ssl/sa.pub # 用于验证 service account token 的公钥 27 - --tls-cert-file=/etc/kubernetes/ssl/apiserver.crt # 用于对外提供服务的服务器证书和私钥 28 - --tls-private-key-file=/etc/kubernetes/ssl/apiserver.key 29 ... 3.4 controller-manager, kubelet, scheduler 证书 上面三个 apiserver 的客户端组件的证书都写在了对应的 KUBECONFIG 中,名为 controller-manager.conf, kubelet.conf, scheduler.conf, 就不一一展示, 随便列举一个如下:\n1# 包含验证apiserver证书的ca证书, 还有请求时候的看客户端证书和私钥 2$ cat scheduler.conf 3apiVersion: v1 4clusters: 5- cluster: 6 certificate-authority-data: ca证书 7 server: https://10.20.11.120:6443 8 name: kubernetes 9contexts: 10- context: 11 cluster: kubernetes 12 user: system:kube-scheduler 13 name: system:kube-scheduler@kubernetes 14current-context: system:kube-scheduler@kubernetes 15kind: Config 16preferences: {} 17users: 18- name: system:kube-scheduler 19 user: 20 client-certificate-data: server证书 21 client-key-data: 秘钥 3.5 Service Account 秘钥对 Service account 主要被 pod 用于访问 kube-apiserver。 在为一个 pod 指定了 service account 后，kubernetes 会为该 service account 生成一个 JWT token，并使用 secret 将该 service account token 挂载到 pod 上。pod 中的应用可以使用 service account token 来访问 api server。service account 证书被用于生成和验证 service account token。\n注意:\nService account加密过程是文章开头说的\u0026quot;非对称加密\u0026quot;, 也就是说只是controller-manager私钥加密数据, 然后kube-apiserver的公钥进行解密而已 由于是非对称加密, 也就是说没有\u0026quot;双向 tls 认证\u0026quot; istio的做法就是为每个 service account 生成一个证书, 之后就可以\u0026quot;双向 tls 认证\u0026quot; 1$ cat manifests/kube-controller-manager.yaml | grep service-account 2 - --service-account-private-key-file=/etc/kubernetes/ssl/sa.key 3 - --use-service-account-credentials=true 4$ cat manifests/kube-apiserver.yaml | grep service-account 5 - --service-account-key-file=/etc/kubernetes/ssl/sa.pub 四、监控kubernetes证书 比较好用的是开源的x509 exporter\n开源链接：\nhttps://github.com/enix/x509-certificate-exporter/\n1$ gc https://github.com/enix/x509-certificate-exporter.git 2$ cd deploy/charts/x509-certificate-exporter 3$ vim values.yaml 4... 5 daemonSets: 6 master: 7 nodeSelector: 8 \u0026#34;node-role.kubernetes.io/controlplane\u0026#34;: \u0026#34;true\u0026#34; 9 tolerations: 10 - effect: NoSchedule 11 operator: Exists 12 - effect: NoExecute 13 operator: Exists 14 watchDirectories: 15 - /etc/kubernetes/ssl/ 16 nodes: 17 tolerations: 18 - effect: NoSchedule 19 operator: Exists 20 watchKubeconfFiles: 21 - /etc/kubernetes/ssl/kubecfg-kube-node.yaml 22 - /etc/kubernetes/ssl/kubecfg-kube-proxy.yaml 23 24$ helm install x509-certificate-exporter . 25$ k get daemonset 26NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE 27x509-certificate-exporter-master 3 3 3 3 3 node-role.kubernetes.io/controlplane=true 18d 28x509-certificate-exporter-nodes 202 202 197 202 197 \u0026lt;none\u0026gt; 18d 29... 在daemonSet配置里面支持将核心的控制节点证书路径，或者kubeconfig文件写进去，然后部署即可；开源社区提供配套dashboard:\nFAQ 1. 在安装 Kubernetes 时，我们需要为每一个工作节点上的 Kubelet 分别生成一个证书。由于工作节点可能很多，手动生成 Kubelet 证书的过程会比较繁琐怎么办? Kubernetes 提供了一个 certificates.k8s.io API，可以使用配置的 CA 根证书来签发用户证书 Kubernetes 提供了 TLS bootstrapping 的方式来简化 Kubelet 证书的生成过程 过程如下:(需要 apiserver 启用--enable-bootstrap-token-auth)\n调用 kube-apiserver 生成一个 bootstrap token 将该 bootstrap token 写入到一个 kubeconfig 文件中，作为 kubelet 调用 kube-apiserver 的客户端验证方式 通过 --bootstrap-kubeconfig 启动参数将 bootstrap token 传递给 kubelet 进程 Kubelet 采用 bootstrap token 调用 kube-apiserver API，生成自己所需的服务器和客户端证书 证书生成后，Kubelet 采用生成的证书和 kube-apiserver 进行通信，并删除本地的 kubeconfig 文件，以避免 bootstrap token 泄漏风险 参考链接:\nhttps://zhaohuabing.com/post/2020-05-19-k8s-certificate/\nhttps://kubernetes.io/zh/docs/tasks/tls/managing-tls-in-a-cluster/\n2. 如何升级 kubernetes 证书但是不让 serviceaccout 轮换？ 我们都知道 serviceaccount 的 token 是依赖于 sa.key 和 sa.pub 的，也就是说如果这俩秘钥更换了，所有的 serviceaccout 就都失效了，会重新创建(kube-system 命名空间的会自动轮换，其他 ns 的需要手动); 但是如果只是更换证书的话，其实是不需要更换 sa 秘钥对的，因为 sa 秘钥对采用的是\u0026quot;非对称加密\u0026quot;, 也就是说永不过期，除非删除秘钥对；\n事实上我们只要手动更换其他证书即可：\n1$ cp -R /etc/kubernetes/ssl /etc/kubernetes/ssl.backup 2$ cp /etc/kubernetes/admin.conf /etc/kubernetes/admin.conf.backup 3$ cp /etc/kubernetes/controller-manager.conf /etc/kubernetes/controller-manager.conf.backup 4$ cp /etc/kubernetes/kubelet.conf /etc/kubernetes/kubelet.conf.backup 5$ cp /etc/kubernetes/scheduler.conf /etc/kubernetes/scheduler.conf.backup 6 7$ kubeadm alpha certs renew apiserver-kubelet-client 8$ kubeadm alpha certs renew apiserver 9$ kubeadm alpha certs renew front-proxy-client 10$ kubeadm alpha kubeconfig user --client-name system:kube-controller-manager \u0026gt; /etc/kubernetes/controller-manager.conf 11$ kubeadm alpha kubeconfig user --client-name system:kube-scheduler \u0026gt; /etc/kubernetes/scheduler.conf 12$ kubeadm alpha kubeconfig user --client-name system:node:{nodename} --org system:nodes \u0026gt; /etc/kubernetes/kubelet.conf 13 14$ kubeadm alpha kubeconfig user --client-name kubernetes-admin --org system:masters \u0026gt; /etc/kubernetes/admin.conf 15$ cp /etc/kubernetes/admin.conf ~/.kube/config 上述过程每个 master 都需要做(如果有多个 master), 最后重启基础组件即可(嫌麻烦可以直接 master 节点关机重启)\n参考链接：\nhttps://github.com/kubernetes-sigs/kubespray/issues/5464/\n3. kubernetes kubelet 证书自动 renew 1# kubelet配置 2--feature-gates=RotateKubeletServerCertificate=true 3--feature-gates=RotateKubeletClientCertificate=true 4# 1.8版本以上包含1.8都支持证书更换自动重载，以下版本只能手动重启服务 5--rotate-certificates 6 7 8# kube-controller-manager配置 9# 证书有效期为10年 10--experimental-cluster-signing-duration=87600h0m0s 11--feature-gates=RotateKubeletServerCertificate=true 创建自动批准相关 CSR 请求的 ClusterRole:\n1kind: ClusterRole 2apiVersion: rbac.authorization.k8s.io/v1 3metadata: 4 name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver 5rules: 6- apiGroups: [\u0026#34;certificates.k8s.io\u0026#34;] 7 resources: [\u0026#34;certificatesigningrequests/selfnodeserver\u0026#34;] 8 verbs: [\u0026#34;create\u0026#34;] 自动批准 kubelet-bootstrap 用户 TLS bootstrapping 首次申请证书的 CSR 请求\n1kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --user=kubelet-bootstrap 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求\n1kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求\n1kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes 参考链接：\nhttps://www.leiyawu.com/2020/10/11/Untitled/\n","link":"https://zhangsiming-blyq.github.io/post/kubernetes/kubernetes-certificate/","section":"post","tags":["kubernetes","中文"],"title":"谈谈kubernetes 证书认证那些事儿"},{"body":" ingress在将流量发往后端的时候是不经过kube-proxy的，ingress controller会直接和kube-apiserver进行交互，然后获取pod endpoints和service的对应关系，进行轮询，负载均衡到后端节点。\n一、验证试验 从集群中删除kube-proxy，查看通过ingress的方式是否可以访问成功?\n1# 实验中选择的是\u0026#34;iptables模式的kube-proxy\u0026#34; 2# 首先关闭kube-proxy 3$ sudo systemctl stop kube-proxy 4 5# 查看服务，ClusterIP的端口是8080，NodePort的端口是30948 6$ ks 7NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 8example-test NodePort 10.0.0.226 \u0026lt;none\u0026gt; 8080:30948/TCP 18h 9traefik NodePort 10.0.0.85 \u0026lt;none\u0026gt; 9000:30001/TCP,80:30002/TCP,443:31990/TCP 19h 10 11# 查看与\u0026#34;10.0.0.226\u0026#34;有关的iptables条目，得知访问\u0026#34;10.0.0.226\u0026#34;且目的端口是\u0026#34;8080\u0026#34;的流量会发送给KUBE-SVC-KNYNFDNL67C7KAZZ链 12$ sudo iptables -S -t nat | grep 10.0.0.226 13-A KUBE-SERVICES ! -s 10.0.0.0/24 -d 10.0.0.226/32 -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080 cluster IP\u0026#34; -m tcp --dport 8080 -j KUBE-MARK-MASQ 14-A KUBE-SERVICES -d 10.0.0.226/32 -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080 cluster IP\u0026#34; -m tcp --dport 8080 -j KUBE-SVC-KNYNFDNL67C7KAZZ 15 16# 查看KUBE-SVC-KNYNFDNL67C7KAZZ链发现，采用的是\u0026#34;--probability\u0026#34;策略，进行后端两个pod的负载均衡 17$ sudo iptables -S -t nat | grep KUBE-SVC-KNYNFDNL67C7KAZZ 18-N KUBE-SVC-KNYNFDNL67C7KAZZ 19-A KUBE-NODEPORTS -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080\u0026#34; -m tcp --dport 30948 -j KUBE-SVC-KNYNFDNL67C7KAZZ 20-A KUBE-SERVICES -d 10.0.0.226/32 -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080 cluster IP\u0026#34; -m tcp --dport 8080 -j KUBE-SVC-KNYNFDNL67C7KAZZ 21-A KUBE-SVC-KNYNFDNL67C7KAZZ -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080\u0026#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-O6MCO5C4JB4A4VEJ 22-A KUBE-SVC-KNYNFDNL67C7KAZZ -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080\u0026#34; -j KUBE-SEP-KREF3VGT6NHFYTVH 23 24# 我们再看后面转发的\u0026#34;KUBE-SEP-KREF3VGT6NHFYTVH\u0026#34;, 得知这一步就发送给了真实的pod 25$ sudo iptables -t nat -L KUBE-SEP-KREF3VGT6NHFYTVH --line-numbers 26Chain KUBE-SEP-KREF3VGT6NHFYTVH (1 references) 27num target prot opt source destination 281 KUBE-MARK-MASQ all -- 10.244.2.4 anywhere /* traffic-dispatcher/example-test:port-8080 */ 292 DNAT tcp -- anywhere anywhere /* traffic-dispatcher/example-test:port-8080 */ tcp to:10.244.2.4:8080 30 31# 现在我们要模拟不能走kube-proxy添加的规则访问到服务，于是我们删除刚刚\u0026#34;50%/50%\u0026#34;规则进行负载均衡转发的那两条规则(kube-proxy本质就是负载均衡) 32$ sudo iptables -t nat -L KUBE-SVC-KNYNFDNL67C7KAZZ --line-numbers 33Chain KUBE-SVC-KNYNFDNL67C7KAZZ (2 references) 34num target prot opt source destination 351 KUBE-SEP-O6MCO5C4JB4A4VEJ all -- anywhere anywhere /* traffic-dispatcher/example-test:port-8080 */ statistic mode random probability 0.50000000000 362 KUBE-SEP-KREF3VGT6NHFYTVH all -- anywhere anywhere /* traffic-dispatcher/example-test:port-8080 */ 37 38# 删除规则 39$ sudo iptables -t nat -D KUBE-SVC-KNYNFDNL67C7KAZZ 1 2 现在以ClusterIP+Port的方式访问服务应该走不通了，测试访问:\n1$ ks 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3example-test NodePort 10.0.0.226 \u0026lt;none\u0026gt; 8080:30948/TCP 18h 4traefik NodePort 10.0.0.85 \u0026lt;none\u0026gt; 9000:30001/TCP,80:30002/TCP,443:31990/TCP 19h 5$ curl 10.0.0.226:8080 6访问失败 7$ curl 10.20.13.90:30948 8访问失败 9$ curl 10.20.13.90:30002 -H \u0026#39;Host:testfile.shannonai.com\u0026#39; 10访问成功, 由此证明ingress是不走kube-proxy的! 二、结论 kube-proxy的工作是附加一些转发规则，所以说单单的\u0026quot;systemctl stop kube-proxy\u0026quot;是不会影响已经存在的规则的，也就是原来可以走通的服务只要没有变化，就依旧可以走通；只是我们再添加新的服务，就不能走通了(因为没有kube-proxy为我们添加这些规则了); 另外多节点的集群，只要其他节点的kube-proxy是ok的，就已经可以从其他节点访问服务; 我们做了上述那么多破坏如果想要恢复的话，也只是需要重新\u0026quot;systemctl restart kube-proxy\u0026quot;就会自动把我们破坏掉的规则加回来\n如何理解我们集群的ClusterIP和他对应的端口？我们了解过kube-proxy的规则了之后就很明显的可以得出结论：\n1）pod的ip是pod所处的networkns的真实ip，通过veth和网桥和我们宿主机的真实ip通信\n2）clusterip仅仅为kube-proxy的iptables或者ipvs规则匹配所用，也就是并不会出现在本机进行路由\n1# 本机的路由表只能看到真实的节点ip段和网络插件附加的pod ip段, 没有clusterip的地址段 2$ route -n 3内核 IP 路由表 4目标 网关 子网掩码 标志 跃点 引用 使用 接口 50.0.0.0 10.20.13.1 0.0.0.0 UG 100 0 0 ens5 610.20.13.0 0.0.0.0 255.255.255.0 U 0 0 0 ens5 710.20.13.1 0.0.0.0 255.255.255.255 UH 100 0 0 ens5 810.244.1.0 10.244.1.0 255.255.255.0 UG 0 0 0 flannel.1 910.244.2.0 10.244.2.0 255.255.255.0 UG 0 0 0 flannel.1 10172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 11 12# iptables和ipvs的规则中可以看到clusterip的地址段 13$ sudo iptables -S -t nat | grep 10.0.0.226 14-A KUBE-SERVICES ! -s 10.0.0.0/24 -d 10.0.0.226/32 -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080 cluster IP\u0026#34; -m tcp --dport 8080 -j KUBE-MARK-MASQ 15-A KUBE-SERVICES -d 10.0.0.226/32 -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080 cluster IP\u0026#34; -m tcp --dport 8080 -j KUBE-SVC-KNYNFDNL67C7KAZZ 由此得出，pod的targetport在对于不同的pod的情况下是可以重复的；nodeport如果是处于同一个节点是不能重复的；clusterip在clusterip的地址不同(也就是iptables和ipvs的规则中不冲突/或者简单理解为不同服务)的情况下是可以重复的\n参考链接: https://xuxinkun.github.io/2016/07/22/kubernetes-proxy/\n","link":"https://zhangsiming-blyq.github.io/post/kubernetes/ingress-mechanism/","section":"post","tags":["kubernetes","中文"],"title":"浅谈kubernetes ingress机制"},{"body":"","link":"https://zhangsiming-blyq.github.io/archives/","section":"","tags":null,"title":"Archives"},{"body":"Requirements A stack whose elements are of type integer now wants to sort the stack from top to bottom in order from smallest to largest, and only one stack is allowed to be requested. Other than that, new variables can be requested, but no additional data structures can be requested. How to complete the sorting?\nSolution apply for a new help stack, keep getting data from the original stack, and compare it with the top data of the new help stack; if it meets the sorting requirements, then push it to the help stack if it does not meet the sorting requirements, pop the top data from the help stack and push it to the original stack until it meets the sorting requirements Finally, the original stack is emptied, and the help stack is the stack that is all sorted, so write it back again to complete the stack sorting. Golang Implementation 1package main 2 3// SortStark stark top --\u0026gt; stark bottom(from small to big) 4func SortStark(stk *Stack) { 5\thelpStack := NewStack() 6\t// 1. while go through target stack until stack is empty 7\tfor stk.Len() \u0026gt; 0 { 8\tcur := stk.Pop() 9\t// 2. if cur \u0026lt; peek, push helpStack.pop() to target stack until cur \u0026gt; peek or helpStack is empty 10\tfor (helpStack.Len() \u0026gt; 0) \u0026amp;\u0026amp; (cur.(int) \u0026lt; helpStack.Peek().(int)) { 11\tstk.Push(helpStack.Pop()) 12\t} 13\t// 3. if helkStack is not empty, compare cur with helpStack\u0026#39;s peek value 14\t// 4. while cur \u0026gt; peek, push cur to helpStack 15\thelpStack.Push(cur) 16\t} 17\t// 5. push back helpStack to target stack 18\tfor helpStack.Len() \u0026gt; 0 { 19\tstk.Push(helpStack.Pop()) 20\t} 21} Test Cases 1package main 2 3import ( 4\t\u0026#34;testing\u0026#34; 5 6\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 7) 8 9func TestSortStark(t *testing.T) { 10\ttests := []struct { 11\tname string 12 13\tval *Stack 14 15\twantRes []int 16\t}{ 17\t{ 18\tname: \u0026#34;sortstark\u0026#34;, 19\tval: NewStack(), 20\t// transpose once, then return to the original order of last-in first-out 21\twantRes: []int{0, 1, 2, 3, 5, 8}, 22\t}, 23\t} 24\tfor _, tt := range tests { 25\tt.Run(tt.name, func(t *testing.T) { 26\t// test case 27\tstk := tt.val 28\tstk.Push(2) 29\tstk.Push(8) 30\tstk.Push(1) 31\tstk.Push(5) 32\tstk.Push(0) 33\tstk.Push(3) 34\tSortStark(stk) 35\tres := []int{} 36\tfor stk.length \u0026gt; 0 { 37\tres = append(res, tt.val.Pop().(int)) 38\t} 39\tassert.Equal(t, tt.wantRes, res) 40\t}) 41\t} 42} Result:\n1$ go test -run ^TestSortStark$ . -v 2=== RUN TestSortStark 3=== RUN TestSortStark/sortstark 4--- PASS: TestSortStark (0.00s) 5 --- PASS: TestSortStark/sortstark (0.00s) 6PASS 7 8Process finished with the exit code 0 Tested with no problems and successfully completed sorting.\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/5/","section":"post","tags":["algorithm","English"],"title":"ALGORITHM SERIES | Use One Stack To Sort Another Stack"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/apigateway/","section":"tags","tags":null,"title":"apigateway"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/apigateway/","section":"categories","tags":null,"title":"apigateway"},{"body":" kong、apisix是当前比较火的两款开源api网关，本文对比了二者的部署、使用方式；提供一个简单的参考; 对于kong，大家都比较熟悉，但是对于apisix可能熟悉的并不多，那么kong、apisix在使用方式，功能命名上是否有相似，还是理念不同，请看下文。\n一、kong 1.1 安装 1# 安装kong 2$ helm repo add kong https://charts.konghq.com 3$ helm repo update 4$ helm fetch kong/kong 5$ tar xf kong-2.5.0.tgz 6$ cd kong 7$ ls 8CHANGELOG.md Chart.yaml FAQs.md README.md UPGRADE.md charts ci crds example-values requirements.lock requirements.yaml templates values.yaml 9...需要配置 101. postgresql作为存储 112. 允许plain text调用admin API 12 13# 安装konga 14$ gc https://github.com/pantsel/konga.git 15$ ls konga 16Chart.yaml templates values.yaml 17...需要配置 181. 获取postgresql的secret写入连接信息 19 20# 部署 21$ helm install kong . 22$ helm install konga . 23$ kp 24NAME READY STATUS RESTARTS AGE 25kong-kong-85d4dfd88b-hjkwt 2/2 Running 2 111s 26kong-postgresql-0 1/1 Running 0 15m 27konga-5b8c899c9-9zbd6 1/1 Running 0 14m 28vagrant@node1:~/kong/kong$ ks 29NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 30kong-kong-admin NodePort 172.30.46.68 \u0026lt;none\u0026gt; 8001:31055/TCP 15m 31kong-postgresql ClusterIP 172.30.194.189 \u0026lt;none\u0026gt; 5432/TCP 15m 32kong-postgresql-headless ClusterIP None \u0026lt;none\u0026gt; 5432/TCP 15m 33konga NodePort 172.30.111.78 \u0026lt;none\u0026gt; 80:32001/TCP 14m 访问konga UI：localhost:32001\n配置kong admin连接地址：http://kong-kong-admin.kong.svc.cluster.local:8001/\n1.2 配置通过kong访问服务 配置service；service在kong中表示实际要访问的服务，这里配置协议、域名、端口、路径、重试等\nroute必须在service创建，这里创建一个多path路由，并指定规定代理的Host\n1# 测试访问 2$ curl -XGET http://172.16.166.149:8000/api/test/ -H \u0026#34;Host: www.kongtest.com\u0026#34; -H \u0026#34;name: siming\u0026#34; -I 3HTTP/1.1 200 OK 4Content-Type: text/html; charset=UTF-8 5Content-Length: 2381 6Connection: keep-alive 7Accept-Ranges: bytes 8Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform 9Date: Fri, 29 Oct 2021 03:05:59 GMT 10Etag: \u0026#34;588604c8-94d\u0026#34; 11Last-Modified: Mon, 23 Jan 2017 13:27:36 GMT 12Pragma: no-cache 13Server: bfe/1.0.8.18 14Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/ 15X-Kong-Upstream-Latency: 31 16X-Kong-Proxy-Latency: 109 17Via: kong/2.6.0 1.3 consumers and plugin consumers抽象表示一组相同的请求，plugin可以实现认证、限流等多种控制功能，这里展示一个jwt认证插件\n在route中的plugin里面enable jwt认证，配置获取jwt的token方式为从uri param \u0026quot;jwt\u0026quot;中获取，并且jwt的校验key为client_id jwt可以再jwt官网(https://jwt.io/)生成，填写对应的加密方式和PAYLOAD(之前route里面约定的key) 然后把上面的信息填入consumers的jwt plugin中(key的名字，加密算法，公钥)并保存 测试访问 1$ curl -XGET http://172.16.166.149:8000/api/test/?username=zhangsiming -H \u0026#34;Host: www.kongtest.com\u0026#34; -H \u0026#34;name: siming\u0026#34; 2{\u0026#34;message\u0026#34;:\u0026#34;Unauthorized\u0026#34;} 3 4$ curl -XGET http://172.16.166.149:8000/api/test/?jwt=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbGllbnRfaWQiOiJzaW1pbmcifQ.OciIJI2HRNa6CteoCn3D87q7pjJZ3u7vrXp0TaKWTuCgwyUJfCoC2c1RSKTz0Eg2GjOrP-u74hVMhBPZzCK1T9ChEOlFqmmS-CnKmh_jlC8RPeGJ2AhJDk7yOos176xgu11jt14nFVFAKzaTaKI4YkmXJ7eTx7TB3WYG0HpNDAgIl6q3UluHERMO-5DT4n3-ev5xHCe-H6InHmGzKkR2t02_lUbbR7EDz2M_YDWJu8enXgBeyHKIoE7ewE0rO66yIm-3UHqHfUJd4BQ5ii73xd8IuhcAgFgTuZ6ffXotxAHuBdoPCEN-qRxcI_dhEXxmiKCNg1QPX1FBpRcbG9uOaw -H \u0026#34;Host: www.kongtest.com\u0026#34; -H \u0026#34;name: siming\u0026#34; -I 5HTTP/1.1 200 OK 6Content-Type: text/html; charset=UTF-8 7Content-Length: 2381 8Connection: keep-alive 9Accept-Ranges: bytes 10Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform 11Date: Fri, 29 Oct 2021 06:51:27 GMT 12Etag: \u0026#34;588604c8-94d\u0026#34; 13Last-Modified: Mon, 23 Jan 2017 13:27:36 GMT 14Pragma: no-cache 15Server: bfe/1.0.8.18 16Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/ 17X-Kong-Upstream-Latency: 39 18X-Kong-Proxy-Latency: 1 19Via: kong/2.6.0 二、apisix 2.1 安装 区别于kong、apisix官方自带web UI\n1$ helm repo add apisix https://charts.apiseven.com 2$ helm repo update 3$ helm fetch apisix/apisix 4$ helm fetch apisix/apisix-dashboard 5 6$ kp 7NAME READY STATUS RESTARTS AGE 8apisix-5d5665c8f-sbjhr 1/1 Running 0 17m 9apisix-dashboard-59fb575657-zt26r 1/1 Running 0 6m 10apisix-etcd-0 0/1 Running 0 13s 11apisix-etcd-1 1/1 Running 0 93s 12apisix-etcd-2 1/1 Running 0 2m55s 部署好了访问dashboard，默认账号密码是admin/admin\n2.2 配置通过apisix访问服务 这里与kong的概念稍有不同：\nupstream：类似kong的sevice，表示由apisix代为调用的上游服务，支持配置负载均衡权重 route：和kong概念相同，可以配置访问apisix的具体路径，方式等 service：route的配置模板，在service中配置了可以直接在route中复用(不用也行) 这里配置一个service，配置要以\u0026quot;www.testapisix.com\u0026quot;域名访问apisix才认为路由匹配，且上游服务为test\n这里配置上游服务为访问www.baidu.com，负载均衡机制为rr\n这里配置route：\n直接复用service配置，所以host部分不需要配置Host； 配置请求apisix的path，支持通配符 URI Override类似kong的strip host，代理的时候变更uri部分 最后还可以附加自定义的Header要求 测试访问\n1$ curl -XGET http://172.16.166.158:9080/api/test/ -H \u0026#34;Host: www.testapisix.com\u0026#34; -H \u0026#34;test: name\u0026#34; -I 2HTTP/1.1 200 OK 3Content-Type: text/html; charset=utf-8 4Content-Length: 2381 5Connection: keep-alive 6Accept-Ranges: bytes 7Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform 8Date: Mon, 01 Nov 2021 08:10:41 GMT 9Etag: \u0026#34;588604c8-94d\u0026#34; 10Last-Modified: Mon, 23 Jan 2017 13:27:36 GMT 11Pragma: no-cache 12Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/ 13Server: APISIX/2.10.0 1.3 consumers and plugin apisix支持自己生成jwt token，这里演示jwt token认证插件\nconsumer中添加plugin配置，编辑json协商\u0026quot;key\u0026quot;: \u0026quot;唯一的值\u0026quot;，然后公钥私钥，加密方式，之后保存\n在service(或者route)开启jwt plugin\n测试访问\n1$ curl -XGET http://172.16.166.158:9080/api/test/more/ -H \u0026#34;Host: www.testapisix.com\u0026#34; -H \u0026#34;test: name\u0026#34; -I 2HTTP/1.1 401 Unauthorized 3Date: Mon, 01 Nov 2021 08:10:17 GMT 4Content-Type: text/plain; charset=utf-8 5Transfer-Encoding: chunked 6Connection: keep-alive 7Server: APISIX/2.10.0 8 9# 获取apisix生成的jwt token 10$ curl -XGET http://172.16.166.158:9080/apisix/plugin/jwt/sign?key=zhangsiming 11eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1YyI6WyItLS0tLUJFR0lOIFBVQkxJQyBLRVktLS0tLVxuTUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUF1MVNVMUxmVkxQSENvek14SDJNb1xuNGxnT0VlUHpObTB0UmdlTGV6VjZmZkF0MGd1blZUTHc3b25MUm5ycTBcL0l6Vzd5V1I3UWtybUJMN2pUS0VuNXVcbitxS2hid0tmQnN0SXMrYk1ZMlprcDE4Z25UeEtMeG9TMnRGY3pHa1BMUGdpenNrdWVtTWdoUm5pV2FvTGN5ZWhcbmtkM3FxR0VsdldcL1ZETDVBYVdUZzBuTFZralJvOXorNDBSUXp1VmFFOEFrQUZteFp6b3czeCtWSllLZGp5a2tKXG4waVQ5d0NTMERSVFh1MjY5VjI2NFZmXC8zanZyZWRaaUtSa2d3bEw5eE5Bd3hYRmcweFwvWEZ3MDA1VVdWUklrZGdcbmNLV1RqcEJQMmRQd1ZaNFdXQys5YUdWZCtHeW4xbzBDTGVsZjRyRWpHb1hiQUFFZ0FxZUdVeHJjSWxialhmYmNcbm13SURBUUFCXG4tLS0tLUVORCBQVUJMSUMgS0VZLS0tLS1cbiJdfQ.eyJleHAiOjE2MzU4NDA2MjksImtleSI6InpoYW5nc2ltaW5nIn0.UErqUg229bwy5ZgYwEibtawT1CpwVP_UKIm-C7-XtMYUjhM6-lE695zFP31s7hp3SsS2PvoZtAEhCgd_fmZXx92EGGL87XYI0xbJ5uWweKettmxkc0cLFWwEL6MNOmqyoaW9gNDtd28K_M1dpzAwZdzz2GNr2e_G1UTrxlQUNorJEg9THIrkLjvrs7NAuvRuSnGd93G4tcXy1G6m0pCAg-Z4oehJS4vMpicmwedQbob0GytBM9Ef_r2gSj8IVVW8MMLWkA-TkPWuMx2nWeK1DdB4-l5I2f4Iu1It17rZqn6VX2ARnN_AFyG5ahT22pIGtdw71Od320hUUDH3I1rmtQ 12 13# 通过jwt token访问 14$ curl -XGET http://172.16.166.158:9080/api/test/?jwt=eyJ0eXAiOiJKV1QiLCJ4NWMiOlsiLS0tLS1CRUdJTiBQVUJMSUMgS0VZLS0tLS1cbk1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBdTFTVTFMZlZMUEhDb3pNeEgyTW9cbjRsZ09FZVB6Tm0wdFJnZUxlelY2ZmZBdDBndW5WVEx3N29uTFJucnEwXC9Jelc3eVdSN1Frcm1CTDdqVEtFbjV1XG4rcUtoYndLZkJzdElzK2JNWTJaa3AxOGduVHhLTHhvUzJ0RmN6R2tQTFBnaXpza3VlbU1naFJuaVdhb0xjeWVoXG5rZDNxcUdFbHZXXC9WREw1QWFXVGcwbkxWa2pSbzl6KzQwUlF6dVZhRThBa0FGbXhaem93M3grVkpZS2RqeWtrSlxuMGlUOXdDUzBEUlRYdTI2OVYyNjRWZlwvM2p2cmVkWmlLUmtnd2xMOXhOQXd4WEZnMHhcL1hGdzAwNVVXVlJJa2RnXG5jS1dUanBCUDJkUHdWWjRXV0MrOWFHVmQrR3luMW8wQ0xlbGY0ckVqR29YYkFBRWdBcWVHVXhyY0lsYmpYZmJjXG5td0lEQVFBQlxuLS0tLS1FTkQgUFVCTElDIEtFWS0tLS0tXG4iXSwiYWxnIjoiUlMyNTYifQ.eyJleHAiOjE2MzU4MzkwNjcsImtleSI6InpoYW5nc2ltaW5nIn0.UR6UnkdAuELK_YP3Kk6V4D4CxoPzTSw5lAx-64As_p68tyUQsp7cuR0MvCLEZ1LtSpF5VlJ1-4fUreAbNAzJQs_FBgDvkUcm4SkdqO_Ss4b0xDiXbF771oJeVybKQA-3fDd_4ieEjCyfsFkg1urgzc_tj96NBiW0YOV98RNJzf9adYZI2MLU_QbEqSEH-f9m0ArTlFLEBnVDOQls3JSc6dWobVbkZZ1kE12YeEq0zCdjEFoUEqy3f6rojobgBFmzvG7xQqn4Jd0o3d5iXBcGbMNn19X_Jo5z47zPI8tCN9ZfHWPtc8ts3HYx_2DmBPZAlEeY3Gs2izPdCHt38evEoA -H \u0026#34;Host: www.testapisix.com\u0026#34; -H \u0026#34;test: name\u0026#34; -I 15HTTP/1.1 200 OK 16Content-Type: text/html; charset=utf-8 17Content-Length: 2381 18Connection: keep-alive 19Accept-Ranges: bytes 20Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform 21Date: Mon, 01 Nov 2021 08:10:41 GMT 22Etag: \u0026#34;588604c8-94d\u0026#34; 23Last-Modified: Mon, 23 Jan 2017 13:27:36 GMT 24Pragma: no-cache 25Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/ 26Server: APISIX/2.10.0 注意，apisix支持的两种jwt访问方式：\nuri param：?jwt=xxxxx Header: -H \u0026quot;Authorization: xxxx\u0026quot; 三、kong 对比 apisix apisix配置更新生效时间0.2毫秒，事件通知；kong需要定期轮询5s左右 单核QPS(开启限流和prometheus插件)，apisix18000，kong1700 并且支持用户自定义负载均衡算法，自带维护dashboard，支持指定时间窗口的限速等 参考链接：\nGitHub - apache/apisix: The Cloud-Native API Gateway 插件 - 插件热加载 - 《Apache APISIX v1.4 使用教程》 - 书栈网 · BookStack\n","link":"https://zhangsiming-blyq.github.io/post/linux/apigateway/","section":"post","tags":["linux","apigateway","中文"],"title":"API网关对比"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/ansible/","section":"tags","tags":null,"title":"ansible"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/ansible/","section":"categories","tags":null,"title":"ansible"},{"body":" Ansible 是一个开源的基于 OpenSSH 的自动化配置管理工具。可以用它来配置系统、部署软件和编排更高级的 IT 任务，比如持续部署或零停机更新。\n一、ansible 简介 Ansible 的主要目标是简单和易用，并且它还高度关注安全性和可靠性。基于这样的目标，Ansible 适用于开发人员、系统管理员、发布工程师、IT 经理，以及介于两者之间的所有人。Ansible 适合管理几乎所有的环境，从拥有少数实例的小型环境到有数千个实例的企业环境。\n1.1 ansible 变量优先级如下 command line values (eg \u0026quot;-u user\u0026quot;) role defaults inventory file or script group vars inventory group_vars/all playbook group_vars/all inventory group_vars/* playbook group_vars/* inventory file or script host vars inventory host_vars/*: inventory 下面的 hosts_vars 目录下的变量优先级大于 group_vars 目录下的 playbook host_vars/* host facts / cached set_facts play vars play vars_prompt play vars_files: vars_files 优先级大于同级别的 vars 字段(play 内定义) role vars (defined in role/vars/main.yml): role 里面的 vars 目录下的定义变量 block vars (only for tasks in block) task vars (only for the task): task 里面的 vars 字段优先级别比较高(本 task) include_vars set_facts / registered vars: 比较常用 set_facts 更改一些后面要使用的变量，全局生效；registered vars 一般连续的 task 用的比较多 role (and include_role) params include params extra vars (always win precedence): 执行 ansible-playbook 命令时候传入的 extra vars 级别最高，高于一切 1.2 变量的作用范围(Scoping variables) Global: this is set by config, environment variables and the command line Play: each play and contained structures, vars entries (vars; vars_files; vars_prompt), role defaults and vars. Host: variables directly associated to a host, like inventory, include_vars, facts or registered task outputs Ansible 1.2 及以上的版本中,group_vars/ 和 host_vars/ 目錄可放在 inventory 目錄下,或是 playbook 目錄下. 如果兩個目錄下都存在,那麼 playbook 目錄下的配置會覆蓋 inventory 目錄的配置(对应解释上分 4-10 条变量优先级)\n1.3 ansible 目录结构最佳实践 结构如下： 1├── playbooks.yml 2├── inventory(inventory下可以任意加子目录进行分组) 3| ├── group_vars ├── all ├── *.yml 4| | ├── *.yml 5| | └── * 6| ├── host_vars 7| | 8│ └── hosts.ini 9| 10├── roles 11│ ├── common 12│ │ ├── files 13│ │ ├── handlers 14│ │ ├── meta 15│ │ ├── templates 16│ │ ├── tasks 17│ │ │ └── main.yml 18│ │ └── vars 19| | 20│ ├── others 21│ ├── files 22│ ├── handlers 23│ ├── meta 24│ ├── templates 25│ ├── tasks 26│ │ └── main.yml 27│ └── vars 28└── *.yml 使用 roles 管理不同的安装模块 使用 inventory/host.ini 管理操作主机，对不同主机进行分组 使用 group_vars 管理基本的默认变量，playbook 中使用 set_facts 设置需要的变量，默认不想被修改的变量写在 roles/vars/main.yml 每个 task 都要写 name tags 写在 role 阶段 1.4 ansible-playbook roles 初始化行为规划 如果角色 /xxx/tasks/main.yaml 存在，则其中列出的任务将添加到任务中，否则将不会添加任务中 如果角色 /xxx/handlers/main.yaml 存在，则其中列出的处理程序将添加到任务中，否则将不会添加任务中 如果角色 /xxx/vars/main.yml 存在，则其中列出的处理程序将添加到任务中，否则将不会添加任务中 如果角色 /xxx/defaults/main.yml 存在，则其中列出的处理程序将添加到任务中，否则将不会添加任务中 如果角色 /xxx/meta/main.yml 存在，则其中列出的任何角色依赖项将添加到角色列表（1.3 及更高版本） 任何副本，脚本，模板或包含任务（在角色中）都可以引用 roles / x / {files，templates，tasks} /（dir 取决于任务）中的文件，而无需相对或绝对地路径化它们 1.5 ansible-playbook 执行顺序 pre_tasks 游戏中定义的任何内容 列出的每个角色将依次执行。将首先运行角色中定义的任何角色依赖项，但需遵循标记过滤和条件。roles meta/main.yml tasks 游戏中定义的任何内容 post_tasks 游戏中定义的任何内容 二、kubespray 部署集群 playbook 解读 github 地址：https://github.com/kubernetes-sigs/kubespray\n1$ gc https://github.com/kubernetes-sigs/kubespray 2$ cd kubespray 3 4# 查看部署集群playbook 5$ cat cluster.yml 6--- 7# 检查ansible版本 8- name: Check ansible version 9 import_playbook: ansible_version.yml 10 11# 检查inventory中组的版本，进行适配旧版本的group名称 12- name: Ensure compatibility with old groups 13 import_playbook: legacy_groups.yml 14 15# 堡垒机配置(没有跳过) 16- hosts: bastion[0] 17 gather_facts: False 18 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 19 roles: 20 - { role: kubespray-defaults } 21 - { role: bastion-ssh-config, tags: [\u0026#34;localhost\u0026#34;, \u0026#34;bastion\u0026#34;] } 22 23# 默认linear,每个主机的单个task执行完成会等待其他都完成后再执行下个任务，设置free可不等待其他主机，继续往下执行(看起来会比较乱) 24# linear策略即线性执行策略，线性执行策略指主机组内所有主机完成一个任务后才继续下一个任务的执行，在执行一个任务时，如果某个主机先执行完则会等待其他主机执行结束。说直白点就是第一个任务在指定的主机都执行完，再进行第二个任务的执行，第二个任务在指定的主机都执行完后，再进行第三个任务的执行…… 以此类推。 25# free策略即自由策略，即在一个play执行完之前，每个主机都各顾各的尽可能快的完成play里的所有任务，而不会因为其他主机没执行完任务而等待，不受线性执行策略那样的约束。所以这种策略的执行结果给人感觉是无序的甚至是杂乱无章的，而且每次执行结果的task显示顺序很可能不一样 26 27# 此阶段对于etcd的每一个节点一个一个的进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact), bootstrap-os(装一些yum源，基础包，改一些主机名等操作) 28- hosts: k8s_cluster:etcd 29 strategy: linear 30 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 31 gather_facts: false 32 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 33 roles: 34 - { role: kubespray-defaults } 35 - { role: bootstrap-os, tags: bootstrap-os} 36 37- name: Gather facts 38 tags: always 39 import_playbook: facts.yml 40 41# 此阶段对于etcd的每一个节点一个一个的进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact)、kubernetes/preinstall(预先配置一些环境，比如禁用SWAP、配置替换resolvconf，设置一些cni的bin路径等、创建/检查一些安装目录、期间对于特定文件触发handler重启相关服务、配置systemd-resolved、修改/etc/hosts文件等)、container-engine(选择容器runtime，进行安装、配置、reload等)、 42# download(模块用于下载所有有需要的镜像，先在ansible执行机缓存，之后再下载所有需要的二进制包，包括kubeadm需要的、kubernetes集群需要的等等) 43- hosts: k8s_cluster:etcd 44 gather_facts: False 45 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 46 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 47 roles: 48 - { role: kubespray-defaults } 49 - { role: kubernetes/preinstall, tags: preinstall } 50 - { role: \u0026#34;container-engine\u0026#34;, tags: \u0026#34;container-engine\u0026#34;, when: deploy_container_engine|default(true) } 51 - { role: download, tags: download, when: \u0026#34;not skip_downloads\u0026#34; } 52 53# 此阶段开始执行etcd role，先进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact)、再进行etcd(检查、创建etcd证书，部署etcd集群，添加etcd节点，检查etcd状态，创建etcd执行用户等...) 54- hosts: etcd 55 gather_facts: False 56 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 57 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 58 roles: 59 - { role: kubespray-defaults } 60 - role: etcd 61 tags: etcd 62 vars: 63 etcd_cluster_setup: true 64 etcd_events_cluster_setup: \u0026#34;{{ etcd_events_cluster_enabled }}\u0026#34; 65 when: not etcd_kubeadm_enabled| default(false) 66 67# 设置etcd_cluster_setup、etcd_events_cluster_setup为 false，主要是将k8s cluster中的机器，分发配置信息etcd的证书, 用于后续与etcd集群交互 68- hosts: k8s_cluster 69 gather_facts: False 70 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 71 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 72 roles: 73 - { role: kubespray-defaults } 74 - role: etcd 75 tags: etcd 76 vars: 77 etcd_cluster_setup: false 78 etcd_events_cluster_setup: false 79 when: not etcd_kubeadm_enabled| default(false) 80 81# 此阶段执行kubernetes/nodes role，先进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact), 再进行kubernetes/node(检查docker cgroup, 配置一些参数等、拷贝一些数据，比如cni目录下的，等等、通过镜像挂载的方式配置安装kubelet、部署apiserver的负载均衡、确保预留nodePort端口范围，通过sysctl修改内核参数、确认kube-proxy ipvs需要的内核模块已开启) 82- hosts: k8s_cluster 83 gather_facts: False 84 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 85 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 86 roles: 87 - { role: kubespray-defaults } 88 - { role: kubernetes/node, tags: node } 89 90# 此阶段部署配置kube_control_plane机器，先进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact)、 91# kubernetes/control-plane(删除control-plane静态文件，删除旧的master容器、下载好kubectl命令行工具、配置kubectl命令行工具自动补全、在kubeadm-setup.yml初始化Initialize first master，创建kubeadm的24h token，然后include_tasks: kubeadm-secondary.yml添加其他的master到集群中、检查etcd证书过期时间等后续检查、更新apiserver客户端的连接地址为fix后的api负载均衡地址、renew集群的证书们, 使用kubeadm renew的方式) 92# kubernetes/client(建立/etc/kubernetes架构目录，拷贝kubeconfig到node并配置好ansible执行机上面的k8s client环境) 93# kubernetes-apps/cluster_roles(通过配置好的kubectl执行，Apply workaround to allow all nodes with cert O=system:nodes to register, 这样之后的所有node加入申请都会自动审批通过...) 94- hosts: kube_control_plane 95 gather_facts: False 96 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 97 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 98 roles: 99 - { role: kubespray-defaults } 100 - { role: kubernetes/control-plane, tags: master } 101 - { role: kubernetes/client, tags: client } 102 - { role: kubernetes-apps/cluster_roles, tags: cluster-roles } 103 104# 此阶段部署配置cluster里面的其他普通node节点，先进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact)、 105# 之后kubernetes/kubeadm(检查kubelet配置文件是否存在、检查kubelet的ca.crt证书是否存在、创建kubeadm join的token、更新kubelet中的地址为负载均衡的apiserver地址、重启kube-proxy的pods)、 106# kubernetes/node-label(根据变量中的{{ role_node_labels + inventory_node_labels }}给node打上对应的标签，通过kubectl label nodes) 107# network_plugin(根据变量决定要部署哪种网络插件，举例cilium的话就是check.yml检查cilium参数，install.yml渲染安装cilium需要的yaml文件到node、apply.yml执行kubectl部署网络插件到集群中) 108- hosts: k8s_cluster 109 gather_facts: False 110 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 111 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 112 roles: 113 - { role: kubespray-defaults } 114 - { role: kubernetes/kubeadm, tags: kubeadm} 115 - { role: kubernetes/node-label, tags: node-label } 116 - { role: network_plugin, tags: network } 117 118# 如果网络插件部署的是calico，还需要到宿主机上面执行一些操作，这里我们选择cilium就跳过了 119- hosts: calico_rr 120 gather_facts: False 121 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 122 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 123 roles: 124 - { role: kubespray-defaults } 125 - { role: network_plugin/calico/rr, tags: [\u0026#39;network\u0026#39;, \u0026#39;calico_rr\u0026#39;] } 126 127# windows master的额外配置，这里跳过, 一般不用 128- hosts: kube_control_plane[0] 129 gather_facts: False 130 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 131 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 132 roles: 133 - { role: kubespray-defaults } 134 - { role: win_nodes/kubernetes_patch, tags: [\u0026#34;master\u0026#34;, \u0026#34;win_nodes\u0026#34;] } 135 136# 这阶段就是在kube_control_plane也就是有权限kubectl的机器上面安装后续应用了，不是每个role都需要走，比如是否部署ingress_controller就由\u0026#34;ingress_nginx_enabled\u0026#34;管理，kubernetes-apps下有个meta/main.yml文件，根据参数选择安装不同的app，流程无非就是下载包，或者传输yaml文件之后kubectl apply 137- hosts: kube_control_plane 138 gather_facts: False 139 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 140 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 141 roles: 142 - { role: kubespray-defaults } 143 - { role: kubernetes-apps/external_cloud_controller, tags: external-cloud-controller } 144 - { role: kubernetes-apps/network_plugin, tags: network } 145 - { role: kubernetes-apps/policy_controller, tags: policy-controller } 146 - { role: kubernetes-apps/ingress_controller, tags: ingress-controller } 147 - { role: kubernetes-apps/external_provisioner, tags: external-provisioner } 148 - { role: kubernetes-apps, tags: apps } 149 150# 这阶段是最后收尾阶段完善集群的dns，host文件，resolv文件里面的格式化对应项等, 至此kubernetes集群通过ansible kubespray安装完毕 151- hosts: k8s_cluster 152 gather_facts: False 153 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 154 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 155 roles: 156 - { role: kubespray-defaults } 157 - { role: kubernetes/preinstall, when: \u0026#34;dns_mode != \u0026#39;none\u0026#39; and resolvconf_mode == \u0026#39;host_resolvconf\u0026#39;\u0026#34;, tags: resolvconf, dns_late: true } 1.2 实际操作使用 ansible-playbook 部署单节点集群 部署参考文档：https://kubespray.io/#/、https://kubespray.io/#/docs/downloads\n1$ sudo apt-get install python3-pip 2$ cd kubespray 3$ sudo pip3 install -r requirements.txt 4# 看下面两个文件有没有要删除的东西 5$ vim inventory/mycluster/group_vars/all/all.yml 6$ vim inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml 7# download缓存相关配置可以查看链接：https://kubespray.io/#/docs/downloads 8# 配置免密sudo，配置免密ssh节点 9$ vim /etc/sudoers 10vagrant ALL=NOPASSWD:ALL 11 12# 开始部署集群 13$ ansible-playbook -i inventory/k8sdemo/inventory.ini --become --become-user=root cluster.yml 14# 部署结果无报错 15... 16PLAY RECAP ******************************************************************************************************************************************************* 17localhost : ok=4 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 18node1 : ok=650 changed=177 unreachable=0 failed=0 skipped=1031 rescued=0 ignored=2 19... 20# 查看集群状态 21$ kubectl get nodes 22NAME STATUS ROLES AGE VERSION 23node1 Ready control-plane,master 3m49s v1.22.2 24$ kubectl get cs 25Warning: v1 ComponentStatus is deprecated in v1.19+ 26NAME STATUS MESSAGE ERROR 27scheduler Healthy ok 28controller-manager Healthy ok 29etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} 30$ kubectl get pods -A 31NAMESPACE NAME READY STATUS RESTARTS AGE 32kube-system cilium-dtgnn 1/1 Running 0 3m16s 33kube-system cilium-operator-66576fd87-bklpc 1/1 Running 0 3m15s 34kube-system coredns-8474476ff8-85qkx 1/1 Running 0 3m9s 35kube-system coredns-8474476ff8-xtmst 0/1 Pending 0 2m45s 36kube-system dns-autoscaler-7df78bfcfb-gsqjb 1/1 Running 0 3m5s 37kube-system kube-apiserver-node1 1/1 Running 0 4m 38kube-system kube-controller-manager-node1 1/1 Running 1 4m 39kube-system kube-proxy-h2lgj 1/1 Running 0 3m16s 40kube-system kube-scheduler-node1 1/1 Running 1 4m 41kube-system nodelocaldns-xnvv7 1/1 Running 0 3m4s 参考链接：\nansible 变量详解\nansible 变量优先级官方原文\n","link":"https://zhangsiming-blyq.github.io/post/linux/ansible-playbook/","section":"post","tags":["ansible","linux","中文"],"title":"ansible-playbook 详解"},{"body":"Requirements Dog and cat have implemented the Pet interface, you can use getPetType() to view the corresponding animal type; NewDog(), NewCat() can create new dog and cat objects respectively.\n1type Pet interface { 2\tgetPetType() string 3} 4 5type fatherPet struct { 6\tType string 7} 8 9func (fp *fatherPet) getPetType() string { 10\treturn fp.Type 11} 12 13type Dog struct { 14\tfatherPet 15} 16 17func NewDog() *Dog { 18\tfmt.Println(\u0026#34;new dog\u0026#34;) 19\treturn \u0026amp;Dog{fatherPet{Type: \u0026#34;dog\u0026#34;}} 20} 21 22func (dog *Dog) getPetType() string { 23\treturn dog.fatherPet.Type 24} 25 26type Cat struct { 27\tfatherPet 28} 29 30func NewCat() *Cat { 31\tfmt.Println(\u0026#34;new cat\u0026#34;) 32\treturn \u0026amp;Cat{fatherPet{Type: \u0026#34;cat\u0026#34;}} 33} 34 35func (cat *Cat) getPetType() string { 36\treturn cat.fatherPet.Type 37} You need to implement a dog-cat queue structure with the following requirements:\nthe user can call the add method to place instances of the cat class or the dog class into the queue; the user can call the pollAll method to pop all the instances in the queue in the order of the queue; the user can call the pollDog method to pop the instances of the dog class in the queue in the order in which they entered the queue; the user may call the pollCat method to populate the queue with instances of the cat class in the order in which they entered the queue; the user can call the isEmpty method to check if there are still instances of dog or cat in the queue the user can call the isDogEmpty method to check if there are instances of the dog class in the queue the user can call the isCatEmpty method to check if there is an instance of the cat class in the queue. Solution According to the question, you need to determine the queue may have dog or cat, with two queues to store dog and cat instances; as for how to determine whether the elements of the dog queue is earlier into the queue or the elements of the cat queue is earlier into the queue need to have a field to store the incoming queue time (globally unique), here you can package a struct to save pet and time;\nThen if you want to look at dog, look at dogqueue, if you want to look at cat, look at catqueue; if you want to see the order of the total queue, take the time of the latest element of both dogqueue and catqueue to compare the order of the total queue.\nGolang Implementation Create a new struct ready to be placed in the queue, including the Pet object and the Time field:\n1type TimePet struct { 2\tPet Pet 3\tTime int 4} 5 6func NewTimePet(pet Pet, time int) *TimePet { 7\treturn \u0026amp;TimePet{ 8\tPet: pet, 9\tTime: time, 10\t} 11} 12 13func (tp *TimePet) getPet() Pet { 14\treturn tp.Pet 15} 16 17func (tp *TimePet) getTime() int { 18\treturn tp.Time 19} Then the dogcatqueue body includes the following logic:\n1type DogCatQueue struct { 2\tDogQueue *customQueue 3\tCatQueue *customQueue 4\tTime int 5} 6 7func NewDogCatQueue() *DogCatQueue { 8\treturn \u0026amp;DogCatQueue{ 9\tDogQueue: newCustomQueue(), 10\tCatQueue: newCustomQueue(), 11\t// Globally unique Time, used to identify the total order 12\tTime: 0, 13\t} 14} 15 16// Adding is relatively simple, dogs into the dog queue, cats into the cat queue 17func (dcqueue *DogCatQueue) add(pet Pet) { 18\tif pet.getPetType() == \u0026#34;dog\u0026#34; { 19\tdcqueue.Time++ 20\tdcqueue.DogQueue.Enqueue(NewTimePet(pet, dcqueue.Time)) 21\t} else if pet.getPetType() == \u0026#34;cat\u0026#34; { 22\tdcqueue.Time++ 23\tdcqueue.CatQueue.Enqueue(NewTimePet(pet, dcqueue.Time)) 24\t} else { 25\tfmt.Errorf(\u0026#34;err, not dog or cat\u0026#34;) 26\t} 27} 28 29// When pollAll, you need to compare the Time of the latest element of the dog queue and the cat queue who is smaller, who is smaller means who is the first queue 30func (dcqueue *DogCatQueue) pollAll() Pet { 31\tif !dcqueue.isDogQueueEmpty() \u0026amp;\u0026amp; !dcqueue.isCatQueueEmpty() { 32\tif dcqueue.DogQueue.Front().(*TimePet).getTime() \u0026lt; dcqueue.CatQueue.Front().(*TimePet).getTime() { 33\treturn dcqueue.DogQueue.Dequeue().(*TimePet).getPet() 34\t} else { 35\treturn dcqueue.CatQueue.Dequeue().(*TimePet).getPet() 36\t} 37\t} else if !dcqueue.isDogQueueEmpty() { 38\treturn dcqueue.DogQueue.Dequeue().(*TimePet).getPet() 39\t} else if !dcqueue.isCatQueueEmpty() { 40\treturn dcqueue.CatQueue.Dequeue().(*TimePet).getPet() 41\t} else { 42\tfmt.Errorf(\u0026#34;err, dogcatqueue is empty\u0026#34;) 43\treturn nil 44\t} 45} 46 47// pollDog can be deleted directly from the dog queue 48func (dcqueue *DogCatQueue) pollDog() Pet { 49\tif !dcqueue.isDogQueueEmpty() { 50\tfmt.Println(\u0026#34;delete latest dog\u0026#34;) 51\treturn dcqueue.DogQueue.Dequeue().(*TimePet).getPet() 52\t} else { 53\tfmt.Errorf(\u0026#34;err, dogcatqueue don\u0026#39;t have dog\u0026#34;) 54\treturn nil 55\t} 56} 57 58// vice versa 59func (dcqueue *DogCatQueue) pollCat() Pet { 60\tif !dcqueue.isCatQueueEmpty() { 61\tfmt.Println(\u0026#34;delete latest cat\u0026#34;) 62\treturn dcqueue.CatQueue.Dequeue().(*TimePet).getPet() 63\t} else { 64\tfmt.Errorf(\u0026#34;err, dogcatqueue don\u0026#39;t have cat\u0026#34;) 65\treturn nil 66\t} 67} 68 69func (dcqueue *DogCatQueue) isEmpty() bool { 70\treturn dcqueue.DogQueue.Empty() \u0026amp;\u0026amp; dcqueue.CatQueue.Empty() 71} 72 73func (dcqueue *DogCatQueue) isDogQueueEmpty() bool { 74\treturn dcqueue.DogQueue.Empty() 75} 76 77func (dcqueue *DogCatQueue) isCatQueueEmpty() bool { 78\treturn dcqueue.CatQueue.Empty() 79} Test Cases 1package main 2 3import ( 4\t\u0026#34;testing\u0026#34; 5 6\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 7) 8 9func TestDogCatQueue(t *testing.T) { 10\t// define a getmin stack 11\ttests := []struct { 12\tname string 13 14\tval func(tq *DogCatQueue) []bool 15 16\twantRes []bool 17\t}{ 18\t{ 19\tname: \u0026#34;operate1\u0026#34;, 20\tval: operate1, 21\twantRes: []bool{true, false, false}, 22\t}, 23\t{ 24\tname: \u0026#34;operate2\u0026#34;, 25\tval: operate2, 26\twantRes: []bool{true, false, false}, 27\t}, 28\t{ 29\tname: \u0026#34;operate3\u0026#34;, 30\tval: operate3, 31\twantRes: []bool{true, true, true}, 32\t}, 33\t{ 34\tname: \u0026#34;operate4\u0026#34;, 35\tval: operate4, 36\twantRes: []bool{true, true, true}, 37\t}, 38\t{ 39\tname: \u0026#34;operate5\u0026#34;, 40\tval: operate5, 41\twantRes: []bool{true, true, true}, 42\t}, 43\t} 44\tfor _, tt := range tests { 45\t// new TwoStackQueue 46\ttq := NewDogCatQueue() 47\t// add test cases 48\tadddogcat(tq) 49\tt.Run(tt.name, func(t *testing.T) { 50\tres := tt.val(tq) 51\tassert.Equal(t, tt.wantRes, res) 52\t}) 53\t} 54} 55 56func adddogcat(tq *DogCatQueue) { 57\ttq.add(NewDog()) 58\ttq.add(NewCat()) 59\ttq.add(NewCat()) 60\ttq.add(NewCat()) 61\ttq.add(NewDog()) 62} 63 64func operate1(tq *DogCatQueue) (queStatus []bool) { 65\ttq.pollAll() 66\ttq.pollAll() 67\ttq.pollCat() 68\ttq.pollCat() 69\tqueStatus = []bool{tq.isCatQueueEmpty(), tq.isDogQueueEmpty(), tq.isEmpty()} 70\treturn queStatus 71} 72 73func operate2(tq *DogCatQueue) (queStatus []bool) { 74\ttq.pollAll() 75\ttq.pollAll() 76\ttq.pollCat() 77\ttq.pollCat() 78\ttq.pollCat() 79\tqueStatus = []bool{tq.isCatQueueEmpty(), tq.isDogQueueEmpty(), tq.isEmpty()} 80\treturn queStatus 81} 82 83func operate3(tq *DogCatQueue) (queStatus []bool) { 84\ttq.pollAll() 85\ttq.pollAll() 86\ttq.pollCat() 87\ttq.pollCat() 88\ttq.pollDog() 89\tqueStatus = []bool{tq.isCatQueueEmpty(), tq.isDogQueueEmpty(), tq.isEmpty()} 90\treturn queStatus 91} 92 93func operate4(tq *DogCatQueue) (queStatus []bool) { 94\ttq.pollAll() 95\ttq.pollAll() 96\ttq.pollCat() 97\ttq.pollCat() 98\ttq.pollAll() 99\tqueStatus = []bool{tq.isCatQueueEmpty(), tq.isDogQueueEmpty(), tq.isEmpty()} 100\treturn queStatus 101} 102 103func operate5(tq *DogCatQueue) (queStatus []bool) { 104\ttq.pollCat() 105\ttq.pollCat() 106\ttq.pollCat() 107\ttq.pollDog() 108\ttq.pollDog() 109\tqueStatus = []bool{tq.isCatQueueEmpty(), tq.isDogQueueEmpty(), tq.isEmpty()} 110\treturn queStatus 111} Result:\n1$ go test -run ^TestDogCatQueue$ . -v 2=== RUN TestDogCatQueue 3=== RUN TestDogCatQueue/operate1 4=== RUN TestDogCatQueue/operate2 5=== RUN TestDogCatQueue/operate3 6=== RUN TestDogCatQueue/operate4 7=== RUN TestDogCatQueue/operate5 8--- PASS: TestDogCatQueue (0.00s) 9 --- PASS: TestDogCatQueue/operate1 (0.00s) 10 --- PASS: TestDogCatQueue/operate2 (0.00s) 11 --- PASS: TestDogCatQueue/operate3 (0.00s) 12 --- PASS: TestDogCatQueue/operate4 (0.00s) 13 --- PASS: TestDogCatQueue/operate5 (0.00s) 14PASS 15ok dogcatqueue 0.008s The length is not short, but the difficulty is very low, note that the topic requires not to modify the original data structure, so define a new structure including Time.\nWhat's More: Use Golang Chain Table To Implement Queue In fact, using slice can also implemente Chain Table, but if the slice is full, the underlying array will be copied once. Using a chain table does not have this problem, the implementation is as follows (accept interface{} as queue elements):\n1package main 2 3import ( 4\t\u0026#34;container/list\u0026#34; 5) 6 7type customQueue struct { 8\tqueue *list.List 9} 10 11func newCustomQueue() *customQueue { 12\treturn \u0026amp;customQueue{queue: list.New()} 13} 14 15func (c *customQueue) Enqueue(value interface{}) { 16\tc.queue.PushBack(value) 17} 18 19func (c *customQueue) Dequeue() interface{} { 20\tif c.queue.Len() \u0026gt; 0 { 21\tele := c.queue.Front() 22\tc.queue.Remove(ele) 23\treturn ele.Value 24\t} 25\treturn nil 26} 27 28func (c *customQueue) Front() interface{} { 29\tif c.queue.Len() \u0026gt; 0 { 30\treturn c.queue.Front().Value 31\t} 32\treturn nil 33} 34 35func (c *customQueue) Size() int { 36\treturn c.queue.Len() 37} 38 39func (c *customQueue) Empty() bool { 40\treturn c.queue.Len() == 0 41} Reference:\nhttps://www.delftstack.com/zh/howto/go/queue-implementation-in-golang/\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/4/","section":"post","tags":["algorithm","English"],"title":"ALGORITHM SERIES | Dog-Cat Queue"},{"body":"Requirements A stack is pressed into 1, 2, 3, 4, 5, then from the top of the stack to the bottom of the stack is 5, 4, 3, 2, 1. After transposing this stack, from the top of the stack to the bottom of the stack is 1, 2, 3, 4, 5, that is, to achieve the reverse order of the stack elements, but only with recursive functions to achieve, can not use other data structures.\nSolution a function to implement the return of the data on the stack, used to return the bottom element of the stack, stop when the stack is empty another function accepts the bottom element of the stack and represses the data of each one onto the stack to achieve reverse order Golang Implementation 1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func getAndRemoveLastElement(stack *Stack) int { 6\tresult := stack.Pop() 7\tif stack.Len() == 0 { 8\treturn result.(int) 9\t} else { 10\tlast := getAndRemoveLastElement(stack) 11\tstack.Push(result) 12\treturn last 13\t} 14} 15 16func reverse(stack *Stack) { 17\tif stack.Len() == 0 { 18\treturn 19\t} else { 20\tgarFuncReturn := getAndRemoveLastElement(stack) 21\treverse(stack) 22\tstack.Push(garFuncReturn) 23\t} 24} Test Cases 1package main 2 3import ( 4\t\u0026#34;testing\u0026#34; 5 6\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 7) 8 9func TestReverseStack(t *testing.T) { 10\t// define a getmin stack 11\ttests := []struct { 12\tname string 13 14\tval func(stk *Stack) 15 16\twantRes []any 17\t}{ 18\t{ 19\tname: \u0026#34;push\u0026#34;, 20\tval: push, 21\t// transpose once, then return to the original order of last-in first-out 22\twantRes: []any{1, 2, 3, 4, 5}, 23\t}, 24\t} 25\tfor _, tt := range tests { 26\t// new TwoStackQueue 27\tstk := NewStack() 28\tt.Run(tt.name, func(t *testing.T) { 29\ttt.val(stk) 30\treverse(stk) 31\tres := popall(stk) 32\tassert.Equal(t, tt.wantRes, res) 33\t}) 34\t} 35} 36 37func push(stk *Stack) { 38\tfor i := 1; i \u0026lt;= 5; i++ { 39\tstk.Push(i) 40\t} 41} 42 43func popall(stk *Stack) (resultList []any) { 44\tfor { 45\tif stk.length == 0 { 46\tbreak 47\t} 48\tresultList = append(resultList, stk.Pop()) 49\t} 50\treturn resultList 51} Result:\n1$ go test -run ^TestReverseStack$ . -v 2=== RUN TestReverseStack 3=== RUN TestReverseStack/push 4--- PASS: TestReverseStack (0.00s) 5 --- PASS: TestReverseStack/push (0.00s) 6PASS 7ok starkreverse 0.008s Implementation is relatively simple, the focus is the need to draw the call-stack diagram, each layer of the variable is what value sorted out, when to pop up, this is much easier to use a pen to draw.\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/3/","section":"post","tags":["algorithm","English"],"title":"ALGORITHM SERIES | How To Inverse Order A Stack Using Only Recursive Functions And Stack"},{"body":"Requirements Write a class that implements a queue with two stacks and supports the basic operations of a queue: add, poll, peek\nSolving Ideas stacks are characterized by last-in-first-out, queues are characterized by first-in-first-out one stack as a press-in stack, the other stack as a pop-up stack, as long as the data pressed into the press-in stack and then pressed into the pop-up stack order will be restored Golang Implementation Note that since the data from stackPush to stackPop is only guaranteed to be coherent each time, stackPush has to press all the data into stackPop at once, and only when stackPop is empty.\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5type TwoStackQueue struct { 6\tstackPush *Stack 7\tstackPop *Stack 8} 9 10func NewTwoStackQueue() *TwoStackQueue { 11\treturn \u0026amp;TwoStackQueue{ 12\tstackPush: NewStack(), 13\tstackPop: NewStack(), 14\t} 15} 16 17func (tsq *TwoStackQueue) add(data interface{}) { 18\ttsq.stackPush.Push(data) 19} 20 21func (tsq *TwoStackQueue) poll() interface{} { 22\tif tsq.stackPop.Len() == 0 \u0026amp;\u0026amp; tsq.stackPush.Len() == 0 { 23\tfmt.Errorf(\u0026#34;TwoStackQueue is empty\u0026#34;) 24\t} else if tsq.stackPop.Len() == 0 { 25\tfor tsq.stackPush.Len() != 0 { 26\ttsq.stackPop.Push(tsq.stackPush.Pop()) 27\t} 28\t} 29\tvalue := tsq.stackPop.Pop() 30\treturn value 31} 32 33func (tsq *TwoStackQueue) peek() interface{} { 34\tif tsq.stackPop.Len() == 0 \u0026amp;\u0026amp; tsq.stackPush.Len() == 0 { 35\tfmt.Errorf(\u0026#34;TwoStackQueue is empty\u0026#34;) 36\t} else if tsq.stackPop.Len() == 0 { 37\tfor tsq.stackPush.Len() != 0 { 38\ttsq.stackPop.Push(tsq.stackPush.Pop()) 39\t} 40\t} 41\tvalue := tsq.stackPop.Peek() 42\treturn value 43} Test Cases 1package main 2 3import ( 4\t\u0026#34;testing\u0026#34; 5 6\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 7) 8 9func TestTwoStackQueue(t *testing.T) { 10\t// define a getmin stack 11\ttests := []struct { 12\tname string 13 14\tval func(tsq *TwoStackQueue) 15 16\twantRes interface{} 17\t}{ 18\t{ 19\tname: \u0026#34;push\u0026#34;, 20\tval: push, 21\twantRes: 1, 22\t}, 23\t{ 24\tname: \u0026#34;push and poll1\u0026#34;, 25\tval: pushpoll1, 26\twantRes: 3, 27\t}, 28\t{ 29\tname: \u0026#34;push and poll2\u0026#34;, 30\tval: pushpoll2, 31\twantRes: 2, 32\t}, 33\t{ 34\tname: \u0026#34;push and poll3\u0026#34;, 35\tval: pushpoll3, 36\twantRes: nil, 37\t}, 38\t} 39\tfor _, tt := range tests { 40\t// new TwoStackQueue 41\ttsq := NewTwoStackQueue() 42\tt.Run(tt.name, func(t *testing.T) { 43\ttt.val(tsq) 44\tres := tsq.peek() 45\tassert.Equal(t, tt.wantRes, res) 46\t}) 47\t} 48} 49 50func push(tsq *TwoStackQueue) { 51\ttsq.add(1) 52\ttsq.add(2) 53\ttsq.add(3) 54} 55 56func pushpoll1(tsq *TwoStackQueue) { 57\ttsq.add(1) 58\ttsq.add(2) 59\ttsq.add(3) 60\ttsq.poll() 61\ttsq.poll() 62} 63 64func pushpoll2(tsq *TwoStackQueue) { 65\ttsq.add(1) 66\ttsq.poll() 67\ttsq.add(2) 68\ttsq.add(3) 69} 70 71func pushpoll3(tsq *TwoStackQueue) { 72\ttsq.add(1) 73\ttsq.add(2) 74\ttsq.poll() 75\ttsq.add(3) 76\ttsq.poll() 77\ttsq.poll() 78} Result:\n1$ go test -run ^TestTwoStackQueue$ . -v 2=== RUN TestTwoStackQueue 3=== RUN TestTwoStackQueue/push 4=== RUN TestTwoStackQueue/push_and_poll1 5=== RUN TestTwoStackQueue/push_and_poll2 6=== RUN TestTwoStackQueue/push_and_poll3 7--- PASS: TestTwoStackQueue (0.00s) 8 --- PASS: TestTwoStackQueue/push (0.00s) 9 --- PASS: TestTwoStackQueue/push_and_poll1 (0.00s) 10 --- PASS: TestTwoStackQueue/push_and_poll2 (0.00s) 11 --- PASS: TestTwoStackQueue/push_and_poll3 (0.00s) 12PASS The test scenario matches the queue's characteristics well. As you can see, a peek operation, add(1), add(2), and add(3) always return 1. In addition, you can only see 2 if you delete 1. so forth.\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/2/","section":"post","tags":["algorithm","English"],"title":"ALGORITHM SERIES | Queue Composed Of Two Stacks"},{"body":"Requirements the time complexity of pop, push, getMin operations are O(1) the design of the stack type can use the ready-made stack structure Solving Ideas use two stacks, starkData and stackMin compare the size of the top data of stackMin with that of starkData each time it is pressed in, and press the new minimum value onto the stack if it is smaller than the top data of stackMin Golang Implementation Golang built-in data structure does not include a stack, define a stack.\nsupport for NewStack() to create support Push(), Pop(), Peek(), Len(), Push supports any type (use assertion) 1package main 2 3import \u0026#34;sync\u0026#34; 4 5type ( 6\tStack struct { 7\ttop *node 8\tlength int 9\tlock *sync.RWMutex 10\t} 11\tnode struct { 12\tvalue interface{} 13\tprev *node 14\t} 15) 16 17// NewStack Create a new stack 18func NewStack() *Stack { 19\treturn \u0026amp;Stack{nil, 0, \u0026amp;sync.RWMutex{}} 20} 21 22// Len Return the number of items in the stack 23func (s *Stack) Len() int { 24\treturn s.length 25} 26 27// Peek View the top item on the stack 28func (s *Stack) Peek() interface{} { 29\tif s.length == 0 { 30\treturn nil 31\t} 32\treturn s.top.value 33} 34 35// Pop the top item of the stack and return it 36func (s *Stack) Pop() interface{} { 37\ts.lock.Lock() 38\tdefer s.lock.Unlock() 39\tif s.length == 0 { 40\treturn nil 41\t} 42\tn := s.top 43\ts.top = n.prev 44\ts.length-- 45\treturn n.value 46} 47 48// Push a value onto the top of the stack 49func (s *Stack) Push(value interface{}) { 50\ts.lock.Lock() 51\tdefer s.lock.Unlock() 52\tn := \u0026amp;node{value, s.top} 53\ts.top = n 54\ts.length++ 55} Stack code implementation with GetMin() method:\n1package main 2 3import ( 4\t\u0026#34;fmt\u0026#34; 5) 6 7type GetMinStack struct { 8\tstackData *Stack 9\tstackMin *Stack 10} 11 12func NewGetMinStack() *GetMinStack { 13\treturn \u0026amp;GetMinStack{ 14\tstackData: NewStack(), 15\tstackMin: NewStack(), 16\t} 17} 18 19func (gms *GetMinStack) Push(newNumber int) { 20\tif gms.stackMin.length == 0 { 21\tgms.stackMin.Push(newNumber) 22\t} else if newNumber \u0026lt;= gms.GetMin().(int) { 23\tgms.stackMin.Push(newNumber) 24\t} 25\tgms.stackData.Push(newNumber) 26} 27 28func (gms *GetMinStack) Pop() int { 29\tif gms.stackMin.length == 0 { 30\t_ = fmt.Errorf(\u0026#34;your stack is empty\u0026#34;) 31\treturn 0 32\t} 33\tvalue := gms.stackData.Pop() 34\tif value == gms.GetMin() { 35\tgms.stackMin.Pop() 36\t} 37\treturn value.(int) 38} 39 40func (gms *GetMinStack) GetMin() interface{} { 41\tif gms.stackMin.length == 0 { 42\t_ = fmt.Errorf(\u0026#34;your stack is empty\u0026#34;) 43\treturn 0 44\t} else { 45\treturn gms.stackMin.Peek() 46\t} 47} Test Cases 1package main 2 3import ( 4\t\u0026#34;testing\u0026#34; 5 6\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 7) 8 9// TDD 10func TestGetMinStack_GetMin(t *testing.T) { 11\t// define a getmin stack 12\ttests := []struct { 13\t// name 14\tname string 15 16\t// input section 17\tval func(gms *GetMinStack) 18 19\t// output section 20\twantRes interface{} 21\t}{ 22\t{ 23\tname: \u0026#34;getmin after push\u0026#34;, 24\tval: push, 25\twantRes: 1, 26\t}, 27\t{ 28\tname: \u0026#34;getmin after push and pop\u0026#34;, 29\tval: pushpop, 30\twantRes: 3, 31\t}, 32\t{ 33\tname: \u0026#34;getmin for empty GetMinStack\u0026#34;, 34\tval: empty, 35\t// default return 0 36\twantRes: 0, 37\t}, 38\t} 39\tfor _, tt := range tests { 40\tgms := NewGetMinStack() 41\tt.Run(tt.name, func(t *testing.T) { 42\ttt.val(gms) 43\tres := gms.GetMin() 44\tassert.Equal(t, tt.wantRes, res) 45\t}) 46\t} 47} 48 49func push(gms *GetMinStack) { 50\tgms.Push(5) 51\tgms.Push(3) 52\tgms.Push(1) 53\tgms.Push(8) 54} 55 56func pushpop(gms *GetMinStack) { 57\tgms.Push(5) 58\tgms.Push(3) 59\tgms.Push(1) 60\tgms.Push(8) 61\tgms.Pop() 62\tgms.Pop() 63} 64 65func empty(gms *GetMinStack) { 66} Result:\n1$ go test -run ^TestGetMinStack_GetMin$ -v . 2=== RUN TestGetMinStack_GetMin 3=== RUN TestGetMinStack_GetMin/getmin_after_push 4=== RUN TestGetMinStack_GetMin/getmin_after_push_and_pop 5=== RUN TestGetMinStack_GetMin/getmin_for_empty_GetMinStack 62022/08/30 15:14:48 your stack is empty 7--- PASS: TestGetMinStack_GetMin (0.00s) 8 --- PASS: TestGetMinStack_GetMin/getmin_after_push (0.00s) 9 --- PASS: TestGetMinStack_GetMin/getmin_after_push_and_pop (0.00s) 10 --- PASS: TestGetMinStack_GetMin/getmin_for_empty_GetMinStack (0.00s) 11PASS 12ok command-line-arguments 0.008s No matter how much data is in the stack, each time it is pressed in and popped out, it is compared with the data at the top of stackMin once, and the minimum value is always at the top of stackMin, so the time complexity is O(1), which satisfies the requirements of this question.\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/1/","section":"post","tags":["algorithm","English"],"title":"ALGORITHM SERIES | Designing A Stack With 'getMin' Function"},{"body":" 在使用client-go的watch接口时候碰到异常退出问题，查了一下google没有多少信息，于是扒了一下代码，把自己踩的坑记录下来方便以后自查自纠。\n使用client-go watch接口 💡 全局的mycluster都等于*kubernetes.Clientset 1. 如何watch 由于kubernetes整合了etcd的watch功能，我们可以通过watch操作去建立一个长连接，不断的接收数据；这种方式要优于普通的反复轮询请求，降低server端的压力;\n使用client-go调用对应对象的Watch()方法之后，会返回一个watch.Event对象，可以对其使用ResultChan()接受watch到的对象。\n1pod, err := mycluster.Clusterclientset.CoreV1().Pods(appNamespace).Watch(context.TODO(), metav1.ListOptions{LabelSelector: label}) 2if err != nil { 3 log.Error(err) 4} 5... 6event, ok := \u0026lt;-pod.ResultChan() 7if !ok { 8 log.Error(err) 9} 异常：watch接口自动断开 1. 现象 在使用过程中，watch操作持续一段时间就会自动断开\n2. 排查 我们进入watch包里面找到streamwatcher.go，其中节选了一些重要片段：\n1type StreamWatcher struct { 2\tsync.Mutex 3\tsource Decoder 4\treporter Reporter 5\tresult chan Event 6\tstopped bool 7} 8... 9func NewStreamWatcher(d Decoder, r Reporter) *StreamWatcher { 10\tsw := \u0026amp;StreamWatcher{ 11\tsource: d, 12\treporter: r, 13\t// It\u0026#39;s easy for a consumer to add buffering via an extra 14\t// goroutine/channel, but impossible for them to remove it, 15\t// so nonbuffered is better. 16\tresult: make(chan Event), 17\t} 18\tgo sw.receive() 19\treturn sw 20} 21... 22func (sw *StreamWatcher) receive() { 23\tdefer close(sw.result) 24\tdefer sw.Stop() 25\tdefer utilruntime.HandleCrash() 26\tfor { 27\taction, obj, err := sw.source.Decode() 28\tif err != nil { 29\t// Ignore expected error. 30\tif sw.stopping() { 31\treturn 32\t} 33\tswitch err { 34\tcase io.EOF: 35\t// watch closed normally 36\tcase io.ErrUnexpectedEOF: 37\tklog.V(1).Infof(\u0026#34;Unexpected EOF during watch stream event decoding: %v\u0026#34;, err) 38\tdefault: 39\tif net.IsProbableEOF(err) || net.IsTimeout(err) { 40\tklog.V(5).Infof(\u0026#34;Unable to decode an event from the watch stream: %v\u0026#34;, err) 41\t} else { 42\tsw.result \u0026lt;- Event{ 43\tType: Error, 44\tObject: sw.reporter.AsObject(fmt.Errorf(\u0026#34;unable to decode an event from the watch stream: %v\u0026#34;, err)), 45\t} 46\t} 47\t} 48\treturn 49\t} 50\tsw.result \u0026lt;- Event{ 51\tType: action, 52\tObject: obj, 53\t} 54\t} 55} 3. 原因 结合代码看一下，StreamWatcher实现了Watch()方法，我们上述调用ResultChan()的时候，实际上返回的是这里的sw.result;\n再往下看新建StreamWatcher的时候，有一个”go sw.receive()”, 也就是几乎在新建对象的同步就开始接受处理数据了，最后看到sw的receive()方法可以看到，在处理数据的时候(sw.source.Decode()), 如果err不为nil, 会switch集中error情况，最后会直接return，然后defer sw.Stop()；\n也就是说如果接受数据解码的时候(sw.source.Decode()), 如果解码失败，那么StreamWatcher就被关闭了，那自然数据通道也就关闭了，造成”watch一段时间之后自动关闭的现象”。\n解决办法(1)：forinfor 那么既然是这种情况会导致watch断开，那么我们首先想到的就是暴力恢复这个StreamWatcher，代码实现如下：\n1for { 2\tpod, err := mycluster.Clusterclientset.CoreV1().Pods(appNamespace).Watch(context.TODO(), metav1.ListOptions{LabelSelector: label}) 3\tif err != nil { 4\tlog.Error(err) 5\t} 6loopier: 7\tfor { 8\tevent, ok := \u0026lt;-pod.ResultChan() 9\tif !ok { 10\ttime.Sleep(time.Second * 5) 11\tlog.Info(\u0026#34;Restarting watcher...\u0026#34;) 12\tbreak loopier 13\t} 14\t// your process logic 15\t} 16} 我们定义一层for嵌套，因为在上述退出的时候会先defer close(sw.result)，所以我们接受数据的通道也就是上面代码里的pod.ResultChan()就会关闭，然后我们加一个错误处理，等待5s之后，break掉这个loopier循环，让外层的for循环继续新建StreamWatcher继续监听数据。以此达到持续监听的效果，好处是实现简单，坏处是缺少错误判断，不能针对错误类型分别处理，对于一直出错的场景也只是无脑重启。\n解决办法(2)：retrywatcher 在官方代码下client-go/tools/watch/retrywatcher.go中其实官方给了一个标准解法，用于解决watch异常退出的问题，下面我们看下这种实现方式：\n1type RetryWatcher struct { 2\tlastResourceVersion string 3\twatcherClient cache.Watcher 4\tresultChan chan watch.Event 5\tstopChan chan struct{} 6\tdoneChan chan struct{} 7\tminRestartDelay time.Duration 8} 9... 10func newRetryWatcher(initialResourceVersion string, watcherClient cache.Watcher, minRestartDelay time.Duration) (*RetryWatcher, error) { 11\tswitch initialResourceVersion { 12\tcase \u0026#34;\u0026#34;, \u0026#34;0\u0026#34;: 13\t// TODO: revisit this if we ever get WATCH v2 where it means start \u0026#34;now\u0026#34; 14\t// without doing the synthetic list of objects at the beginning (see #74022) 15\treturn nil, fmt.Errorf(\u0026#34;initial RV %q is not supported due to issues with underlying WATCH\u0026#34;, initialResourceVersion) 16\tdefault: 17\tbreak 18\t} 19 20\trw := \u0026amp;RetryWatcher{ 21\tlastResourceVersion: initialResourceVersion, 22\twatcherClient: watcherClient, 23\tstopChan: make(chan struct{}), 24\tdoneChan: make(chan struct{}), 25\tresultChan: make(chan watch.Event, 0), 26\tminRestartDelay: minRestartDelay, 27\t} 28 29\tgo rw.receive() 30\treturn rw, nil 31} 和普通的StreamWatcher很类似，这里面RetryWatcher多了一些结构体字段；lastResourceVersion、minRestartDelay用于出错之后重启Watcher的RV保存，以及重试时间；传入initialResourceVersion和watcherClient(cache.Watcher)即可创建一个RetryWatcher;\n同理，RetryWatcher也是在创建对象的同时就开始go rw.receive()接受数据。\n1func (rw *RetryWatcher) receive() { 2\tdefer close(rw.doneChan) 3\tdefer close(rw.resultChan) 4 5\tklog.V(4).Info(\u0026#34;Starting RetryWatcher.\u0026#34;) 6\tdefer klog.V(4).Info(\u0026#34;Stopping RetryWatcher.\u0026#34;) 7 8\tctx, cancel := context.WithCancel(context.Background()) 9\tdefer cancel() 10\tgo func() { 11\tselect { 12\tcase \u0026lt;-rw.stopChan: 13\tcancel() 14\treturn 15\tcase \u0026lt;-ctx.Done(): 16\treturn 17\t} 18\t}() 19 20\t// We use non sliding until so we don\u0026#39;t introduce delays on happy path when WATCH call 21\t// timeouts or gets closed and we need to reestablish it while also avoiding hot loops. 22\twait.NonSlidingUntilWithContext(ctx, func(ctx context.Context) { 23\tdone, retryAfter := rw.doReceive() 24\tif done { 25\tcancel() 26\treturn 27\t} 28 29\ttime.Sleep(retryAfter) 30 31\tklog.V(4).Infof(\u0026#34;Restarting RetryWatcher at RV=%q\u0026#34;, rw.lastResourceVersion) 32\t}, rw.minRestartDelay) 33} 34... 35func (rw *RetryWatcher) Stop() { 36\tclose(rw.stopChan) 37} 上面代码的receive()函数中，wait包起到核心重试逻辑作用，他会循环执行里面的函数，直到收到context Done 的信号才会往下走；而上面代码的ctx只有两种情况才会被关闭：\n有人调用了RetryWatcher的Stop()； 另外就是rw.doReceive()中返回了done, 也会直接调用cancel()结束wait部分。 而如果接受的done为false，则会正常等待time.Sleep(retryAfter)之后，进行重试，实现RetryWatcher！\n接下来就看下这个rw.doReceive()，也就是RetryWatcher的接收处理数据部分, 同时会根据err类型判断是否应该重试：\n1func (rw *RetryWatcher) doReceive() (bool, time.Duration) { 2\twatcher, err := rw.watcherClient.Watch(metav1.ListOptions{ 3\tResourceVersion: rw.lastResourceVersion, 4\tAllowWatchBookmarks: true, 5\t}) 6\t// We are very unlikely to hit EOF here since we are just establishing the call, 7\t// but it may happen that the apiserver is just shutting down (e.g. being restarted) 8\t// This is consistent with how it is handled for informers 9\tswitch err { 10... 11// 省略watch的一些错误处理，都会返回false，也就是继续wait重试 12... 13\t} 14 15\tif watcher == nil { 16\tklog.Error(\u0026#34;Watch returned nil watcher\u0026#34;) 17\t// Retry 18\treturn false, 0 19\t} 20 21\tch := watcher.ResultChan() 22\tdefer watcher.Stop() 23 24\tfor { 25\tselect { 26\tcase \u0026lt;-rw.stopChan: 27\tklog.V(4).Info(\u0026#34;Stopping RetryWatcher.\u0026#34;) 28\treturn true, 0 29\tcase event, ok := \u0026lt;-ch: 30\tif !ok { 31\tklog.V(4).Infof(\u0026#34;Failed to get event! Re-creating the watcher. Last RV: %s\u0026#34;, rw.lastResourceVersion) 32\treturn false, 0 33\t} 34 35\t// We need to inspect the event and get ResourceVersion out of it 36\tswitch event.Type { 37\tcase watch.Added, watch.Modified, watch.Deleted, watch.Bookmark: 38\tmetaObject, ok := event.Object.(resourceVersionGetter) 39\t... 40\tresourceVersion := metaObject.GetResourceVersion() 41\t... 42 // All is fine; send the non-bookmark events and update resource version. 43\tif event.Type != watch.Bookmark { 44\tok = rw.send(event) 45\tif !ok { 46\treturn true, 0 47\t} 48\t} 49\trw.lastResourceVersion = resourceVersion 50 51\tcontinue 52 53\tcase watch.Error: 54\t... 55\t} 56\t} 57\t} 58} 59... 60func (rw *RetryWatcher) send(event watch.Event) bool { 61\t// Writing to an unbuffered channel is blocking operation 62\t// and we need to check if stop wasn\u0026#39;t requested while doing so. 63\tselect { 64\tcase rw.resultChan \u0026lt;- event: 65\treturn true 66\tcase \u0026lt;-rw.stopChan: 67\treturn false 68\t} 69} 上面代码我已经非常清晰的展示出了doReceive()的处理逻辑，第一步会按照rw中定义的watcher开始真实的监听对应资源对象，这里返回错误的话也会进行rw的重试逻辑；然后会获取真实的watcher.ResultChan()也就是可以获取到真实对象的通道，套用for select模式，循环接受数据，如果数据一直是正常的，那么会通过rw.send(event)发送给rw.ResultChan，然后记录保存rw.lastResourceVersion然后继续接收，实现watch的功能；\n这里多说一句，由于rw.lastResourceVersion是保存在rw的，也就是及时重启(对应上面的任意一个case返回的是true)，会从rw.lastResourceVersion也就是最新的RV开始监听，这样实现根据特定原因重启故障的watcher，比较合理，也很巧妙，是官方的标准答案。\n个人版RetryWatcher代码实现： 说了那么多，那么具体要怎么使用这个RetryWatcher呢？我个人做了一个妥协方案可以参考：\n还记得RetryWatcher中定义的watcherClient类型吗，需要是cache.Watcher，然后我们看client-go/tools/cache里面定义的cache.Watcher接口，定义如下：\n1type Watcher interface { 2\t// Watch should begin a watch at the specified version. 3\tWatch(options metav1.ListOptions) (watch.Interface, error) 4} 也就是实现了Watch(xxxx)这个方法的就是符合的cache.Watcher; 而最开始我们代码正好是通过MyCluster.Clusterclientset.CoreV1().Pods()返回的接口v1.PodInterface中调用的Watch(xxxxx), 那么直接用“MyCluster.Clusterclientset.CoreV1().Pods()”来生成RetryWatcher是不是就行了！\n我们来看一下这个接口类型：\n1type PodInterface interface { 2\t... 3\tWatch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error) 4\t... 5} 答案是不行… 下面这个多了一个ctx参数 🥶，但是我们就是想从Clientset这里用怎么办，自己实现一个符合的结构吧：\n1type PodListWatch struct { 2\tMyCluster *initk8s.MyCluster 3\tAppNamespace string 4} 5// watch指定命名空间下的Pod例子 6func (p PodListWatch) Watch(options metav1.ListOptions) (watch.Interface, error) { 7\treturn p.MyCluster.Clusterclientset.CoreV1().Pods(p.AppNamespace).Watch(context.TODO(), options) 8} 这个PodListWatch的Watch(xxxx)方法正好满足cache.Watcher, 单后watch的方式还是按照我们原先Clientset的，然后生成RetryWatcher的方式如下：\n1func ResourceWatcher(mycluster *initk8s.MyCluster, appNamespace string) { 2\tpodWatcher := PodListWatch{MyCluster: mycluster, AppNamespace: appNamespace} 3\t// generate new retrywatcher, use podRetryWatcher.ResultChan() to keep a chronic watch operation 4\tpodRetryWatcher, err := retrywatcher.NewRetryWatcher(label, podWatcher) 5\tif err != nil { 6\tlog.Error(err) 7\t} 8FORWATCHER: 9\tfor { 10\tselect { 11\tcase \u0026lt;-podRetryWatcher.Done(): 12\tlog.Info(\u0026#34;Retrywatcher is exiting, please check...\u0026#34;) 13\tbreak FORWATCHER 14\tcase event, ok := \u0026lt;-podRetryWatcher.ResultChan(): 15\tif !ok { 16\tlog.Warn(\u0026#34;Retrywatcher is not open, please check...\u0026#34;) 17\tcontinue 18\t} 19\t... 上述只是一种思路提供，希望能够帮到使用client-go进行watch的同学；官方的实现RetryWatcher确实更加合理，还可以进行改造加减不同情况是否重试的策略。另外watch方法毕竟是直接和kubernetes中的apiserver沟通，如果想要减轻apiserver的压力kubernetes提供了更加常用的informer机制(sharedinformer也是众多controller使用的，同时也是我认为kubernetes最核心的功能)，至于使用informer就又有很多要说的了，以后有时间可能会更新~\n","link":"https://zhangsiming-blyq.github.io/post/golang/retrywatcher/","section":"post","tags":["golang","kubernetes","中文"],"title":"client-go watch接口隔一段时间自动退出怎么办？"},{"body":" Explain the common usage scenarios of iterators, generators, and yield fields in Python.\nIterators Python object implements iter() and next() methods, we become iterable objects (iterables), through iter() can return an iterator object (iterators).\n__iter__() method: return the iterator object itself __next__() method: returns the next element of the container, and raises a StopIteration Exception at the end to terminate the iterator 1lst = [1, 2, 3] 2print(type(lst)) 3new_iter = lst.__iter__() 4print(type(new_iter)) 5 6# Output 7\u0026lt;class \u0026#39;list\u0026#39;\u0026gt; 8\u0026lt;class \u0026#39;list_iterator\u0026#39;\u0026gt; The for loop actually gets iterators by iter() and then does next() to fetch until StopIteration.\nGenerators\u0026amp;yield Generators are a special kind of iterators. If a field exists anywhere in the function, when you call the function, the function will not execute directly, but will return a generator. In addition generators also support generator expressions (similar to lists, except that [] is replaced with ()).\n1def test(): 2 print(\u0026#34;for test\u0026#34;) 3 yield 0 4gen1 = test() 5print(type(gen1)) 6gen2 = (x*x for x in range(0, 3)) 7print(type(gen2)) 8 9# Output 10\u0026lt;class \u0026#39;generator\u0026#39;\u0026gt; 11\u0026lt;class \u0026#39;generator\u0026#39;\u0026gt; yield Unlike iterators, which store all of their content in memory, generators allocate memory as next() is called over and over again.\nEach time generators will run to the yield field, then save the generator state and return; the next call to next() will continue from the current position to the next yield field. This continues until StopIteration stops. Take a look at the following example:\n1def test(): 2 print(\u0026#34;start\u0026#34;) 3 yield 0 4 print(\u0026#34;end\u0026#34;) 5 yield 1 6 7gen1 = test() 8print(gen1.__next__()) 9print(gen1.__next__()) 10 11print(\u0026#34;\u0026#34;) 12gen2 = (x*x for x in range(0, 3)) 13print(gen2.__next__()) 14print(gen2.__next__()) 15print(gen2.__next__()) 16print(gen2.__next__()) 17 18# Output 19start 200 21end 221 23 240 251 264 27Traceback (most recent call last): 28 File \u0026#34;/home/vagrant/aa.py\u0026#34;, line 16, in \u0026lt;module\u0026gt; 29 print(gen2.__next__()) 30StopIteration Yield In Action We have a batch of 100 pieces of data in a list and want to divide it into 10 groups of 10 and process them separately:\n1def chunks(lst, n): 2 \u0026#34;\u0026#34;\u0026#34;Yield successive n-sized chunks from lst.\u0026#34;\u0026#34;\u0026#34; 3 for i in range(0, len(lst), n): 4 yield lst[i:i + n] 5 6origin_lst = list(range(0, 100)) 7for i in chunks(origin_lst, 10): 8 print(i) 9 10# Output 11[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 12[10, 11, 12, 13, 14, 15, 16, 17, 18, 19] 13[20, 21, 22, 23, 24, 25, 26, 27, 28, 29] 14[30, 31, 32, 33, 34, 35, 36, 37, 38, 39] 15[40, 41, 42, 43, 44, 45, 46, 47, 48, 49] 16[50, 51, 52, 53, 54, 55, 56, 57, 58, 59] 17[60, 61, 62, 63, 64, 65, 66, 67, 68, 69] 18[70, 71, 72, 73, 74, 75, 76, 77, 78, 79] 19[80, 81, 82, 83, 84, 85, 86, 87, 88, 89] 20[90, 91, 92, 93, 94, 95, 96, 97, 98, 99] Reference Links:\nhttps://blog.csdn.net/Yuyh131/article/details/83310486 https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do ","link":"https://zhangsiming-blyq.github.io/post/python/python-yield/","section":"post","tags":["python","English"],"title":"Python Yield"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/apiserver/","section":"tags","tags":null,"title":"apiserver"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/controller-manager/","section":"tags","tags":null,"title":"controller-manager"},{"body":" 对于 kubernetes 集群的控制平面组件，监控是必要的, 他可以帮助我们获取到集群的整体负载压力，并在核心组件出问题的时候配合告警让管理员及时发现问题，及时处理，更稳定的保证集群的生命周期。\n一、Prometheus 如何自动发现 Kubernetes Metrics 接口? prometheus 收集 kubernetes 集群中的指标有两种方式，一种是使用 crd(servicemonitors.monitoring.coreos.com)的方式，主要通过标签匹配；另一种是通过 scrape_config，支持根据配置好的\u0026quot;relabel_configs\u0026quot;中的具体目标, 进行不断拉取(拉取间隔为\u0026quot;scrape_interval\u0026quot;)\n配置权限： k8s 中 RBAC 支持授权资源对象的权限，比如可以 get、list、watch 集群中的 pod，还支持直接赋予对象访问 api 路径的权限，比如获取/healthz, /api 等, 官方对于 non_resource_urls 的解释如下：\nnon_resource_urls - (Optional) NonResourceURLs is a set of partial urls that a user should have access to. *s are allowed, but only as the full, final step in the path Since non-resource URLs are not namespaced, this field is only applicable for ClusterRoles referenced from a ClusterRoleBinding. Rules can either apply to API resources (such as \u0026quot;pods\u0026quot; or \u0026quot;secrets\u0026quot;) or non-resource URL paths (such as \u0026quot;/api\u0026quot;), but not both.\n既然 prometheus 要主动抓取指标，就必须对他使用的 serviceaccount 提前进行 RBAC 授权：\n1# clusterrole.yaml 2apiVersion: rbac.authorization.k8s.io/v1 3kind: ClusterRole 4metadata: 5 name: monitor 6rules: 7- apiGroups: [\u0026#34;\u0026#34;] 8 resources: 9 - nodes 10 - pods 11 - endpoints 12 - services 13 verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] 14- nonResourceURLs: [\u0026#34;/metrics\u0026#34;] 15 verbs: [\u0026#34;get\u0026#34;] 16 17# clusterrolebinding 18apiVersion: rbac.authorization.k8s.io/v1 19kind: ClusterRoleBinding 20metadata: 21 name: prometheus-api-monitor 22roleRef: 23 apiGroup: rbac.authorization.k8s.io 24 kind: ClusterRole 25 name: monitor 26subjects: 27- kind: ServiceAccount 28 name: prometheus-operator-nx-prometheus 29 namespace: monitor 获取 apiserver 自身的 metric 信息： prometheus 中配置\u0026quot;scrape_config\u0026quot;, 或者 prometheus-operator 中配置\u0026quot;additionalScrapeConfigs\u0026quot;, 配置获取 default 命名空间下的 kubernetes endpoints\n1- job_name: \u0026#39;kubernetes-apiservers\u0026#39; 2 kubernetes_sd_configs: 3 - role: endpoints 4 scheme: https 5 tls_config: 6 ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt 7 bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token 8 relabel_configs: 9 - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] 10 action: keep 11 regex: default;kubernetes;https 获取 controller-manager、scheduler 的 metric 信息： controller-manager 和 scheduler 因为自身暴露 metric 接口，需要修改对应 manifests 下的静态 pod 文件，添加匹配的 annotations 即可完成抓取：\n1# prometheus端配置 2kubernetes_sd_configs: 3 - role: pod 4 relabel_configs: 5 - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] 6 action: \u0026#34;keep\u0026#34; 7 regex: \u0026#34;true\u0026#34; 8 - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] 9 action: replace 10 regex: ([^:]+)(?::\\d+)?;(\\d+) 11 replacement: $1:$2 12 target_label: __address__ 13 - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] 14 action: replace 15 target_label: __metrics_path__ 16 regex: \u0026#34;(.+)\u0026#34; 17 - action: labelmap 18 regex: __meta_kubernetes_pod_label_(.+) 19 - source_labels: [__meta_kubernetes_namespace] 20 action: replace 21 target_label: kubernetes_namespace 22 - source_labels: [__meta_kubernetes_pod_name] 23 action: replace 24 target_label: kubernetes_pod_name 25 26# controller-manager配置: 27metadata: 28 annotations: 29 prometheus_io_scrape: \u0026#34;true\u0026#34; 30 prometheus.io/port: \u0026#34;10252\u0026#34; 31 32# scheduler配置： 33metadata: 34 annotations: 35 prometheus_io_scrape: \u0026#34;true\u0026#34; 36 prometheus.io/port: \u0026#34;10251\u0026#34; 获取 etcd 的 metric 信息： etcd 是跑在物理机上的，所以我们先创建对应的 endpoints 绑定好 service，然后采用 servicemonitor 的方式去匹配获取 etcd 的监控指标：\n1# service.yaml 2apiVersion: v1 3kind: Service 4metadata: 5 name: etcd-k8s 6 namespace: kube-system 7 labels: 8 k8s-app: etcd 9spec: 10 type: ClusterIP 11 clusterIP: None 12 ports: 13 - name: port 14 port: 2379 15 protocol: TCP 16 17# endpoint.yaml 18apiVersion: v1 19kind: Endpoints 20metadata: 21 name: etcd-k8s 22 namespace: kube-system 23 labels: 24 k8s-app: etcd 25subsets: 26- addresses: 27 - ip: xx.xx.xx.xx 28 - ip: xx.xx.xx.xx 29 - ip: xx.xx.xx.xx 30 ports: 31 - name: port 32 port: 2379 33 protocol: TCP 34 35# servicemonitor.yaml(需要配置好相关的证书) 36apiVersion: monitoring.coreos.com/v1 37kind: ServiceMonitor 38metadata: 39 name: etcd-k8s 40 namespace: monitor 41 labels: 42 k8s-app: etcd-k8s 43 release: prometheus-operator-nx 44spec: 45 jobLabel: k8s-app 46 endpoints: 47 - port: port 48 interval: 30s 49 scheme: https 50 tlsConfig: 51 caFile: /ca.pem 52 certFile: /server.pem 53 keyFile: /server-key.pem 54 insecureSkipVerify: true 55 selector: 56 matchLabels: 57 k8s-app: etcd 58 namespaceSelector: 59 matchNames: 60 - kube-system 61 62# 最后附上etcd secret的创建方法，将etcd证书挂载进入提供连接使用 63apiVersion: v1 64data: 65 ca.pem: xx 66 server.pem: xx 67 server-key.pem: xx 68kind: Secret 69metadata: 70 name: etcd-certs 71 namespace: monitor 72type: Opaque 二、我该重点关注哪些 control plane 指标？ apiserver: 其中计算延迟可以采用\u0026quot;percentiles\u0026quot;而不是平均数去更好的展示延迟出现情况 apiserver_request_duration_seconds: 计算读(Non-LIST)请求，读(LIST)请求，写请求的平均处理时间\napiserver_request_total: 计算 apiserver 的 QPS、计算读请求、写请求的成功率; 还可以计算请求错误数量以及错误码\napiserver_current_inflight_requests: 计算正在处理的读、写请求\napiserver_dropped_requests_total: 计算失败的请求\ncontroller-manager: leader_election_master_status: 关注是否有 leader\nxxx_depth: 关注正在调和的控制队列深度\nscheduler: leader_election_master_status: 关注是否有 leader\nscheduler_schedule_attempts_total: 帮助查看是否调度器不能正常工作; Number of attempts to schedule pods, by the result. 'unschedulable' means a pod could not be scheduled, while 'error' means an internal scheduler problem\nscheduler_e2e_scheduling_duration_seconds_sum: scheduler 调度延迟(参数弃用)\nrest_client_requests_total: client 请求次数(次重要); Number of HTTP requests, partitioned by status code, method, and host\netcd: etcd_server_has_leader: etcd 是否有 leader\netcd_server_leader_changes_seen_total: etcd leader 切换的次数，如果太频繁可能是一些连接不稳定现象或者 etcd 集群负载过大\netcd_server_proposals_failed_total: 一个提议请求是需要完整走过 raft protocol 的，这个指标帮助我们提供请求出错次数，大多数情况是 etcd 选举 leader 失败或者集群缺乏选举的候选人\netcd_disk_wal_fsync_duration_seconds_sum/etcd_disk_backend_commit_duration_seconds_sum: etcd 磁盘存储了 kubernetes 的所有重要信息，如果磁盘同步有很大延迟会影响 kubernetes 集群的操作, 此指标提供了 etcd 磁盘同步的平均延迟\netcd_debugging_mvcc_db_total_size_in_bytes: etcd 各节点容量\netcd_network_peer_sent_bytes_total/etcd_network_peer_received_bytes_total: 可以计算 etcd 节点的发送/接收数据速率\ngrpc_server_started_total: 可以用于计算 etcd 各个方法的调用速率\n最后采用的 kubernetes 维护界面由：apiserver 专用仪表盘、etcd 仪表盘、综合控制平面仪表盘和证书监控组成；并且在维护使用过程中根据不同参数，不断调整, 够用就行。\n参考链接: https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/cluster_role\nhttps://prometheus.io/docs/prometheus/latest/configuration/configuration/\nhttps://www.datadoghq.com/blog/kubernetes-control-plane-monitoring/\nhttps://sysdig.com/blog/monitor-kubernetes-api-server/\nhttps://sysdig.com/blog/monitor-etcd/\n","link":"https://zhangsiming-blyq.github.io/post/kubernetes/monitor-control-plane/","section":"post","tags":["kubernetes","apiserver","etcd","controller-manager","scheduler","中文"],"title":"prometheus 监测 kubernetes 控制平面"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/scheduler/","section":"tags","tags":null,"title":"scheduler"},{"body":"+++ title = \u0026quot;Search\u0026quot; searchPage = true type = \u0026quot;search\u0026quot; +++\n","link":"https://zhangsiming-blyq.github.io/search/","section":"","tags":null,"title":""},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/about/","section":"tags","tags":null,"title":"about"},{"body":"Embrace the IT World: A Guide to Kubernetes, Go Language, Python, Shell, and Linux\nIntroduction NOTE: 1-2 articles are updated per week on average.\nDear readers,\nI'm excited to announce that I'll be writing blog posts in both English and Chinese moving forward! I understand that some of you may prefer reading in English while others prefer reading in Chinese, so I want to cater to both audiences.\nTo make it easy for you to find the posts you're interested in, I'll be using tags to separate the English posts from the Chinese ones. English posts will be tagged as \u0026quot;English\u0026quot; and Chinese posts will be tagged as \u0026quot;中文\u0026quot;. That way, you can easily find the posts that are written in your preferred language.\nThank you for your continued support, and I hope you enjoy reading my bilingual blog!\nFocus On The world of information technology is rapidly evolving, and it can be challenging to keep up with the latest developments. However, with the right mindset, tools, and resources, you can not only keep up but also excel in this field. In this blog post, we will explore some of the most important topics in the IT world, including Kubernetes, Golang, Python, Shell, and Linux. Whether you are a beginner or an experienced IT professional, this guide will provide you with valuable insights and practical tips to help you succeed in your career.\nLife Advices Don't stop learning; continually acquire new knowledge and skills to help you improve your work and boost your confidence.\nBuild strong relationships: Focus on building strong relationships with your family, friends and colleagues. Surround yourself with positive and supportive people who will inspire and motivate you to be your best self.\nTake risks: Don't be afraid to take risks and try new things. Failure is often the best teacher and it can help you learn and grow as a person. Taking calculated risks will help you step out of your comfort zone and gain valuable experience.\nLearn to manage your finances wisely, including budgeting, saving, and investing. This will provide financial stability and security for your future.\nWork-life balance is crucial. You should also schedule time for your favorite hobbies and allocated some personal leisure.\nStay healthy: Take care of your physical and mental health. Get enough sleep, eat a balanced diet, exercise regularly and take time to de-stress. This will help you maintain a positive outlook on life and enjoy it to the fullest.\nLearn from your mistakes: Nobody is perfect, and we all make mistakes. Instead of dwelling on your mistakes, focus on learning from them and growing as a person.\nPursue your passions: Pursue what makes you happy, whether it's a hobby(money, healthy, creative, knowledge, mindset) or a career. Finding meaning and purpose in your life will give you a sense of fulfillment and happiness.\nTime is your most valuable resource. Use it wisely and prioritize the things that matter most to you.\nSet goals and create a plan to achieve them. This will give you direction and purpose in life.\nConclusion By mastering Kubernetes, Golang, Python, Shell, and Linux, you can become a well-rounded IT professional with a diverse set of skills and knowledge. These technologies are not only useful for building cutting-edge software and systems but also for improving your productivity, efficiency, and problem-solving abilities. Remember, it's never too late to start studying and making progress. Stay focused, stay motivated, and do your best.\nAs a reminder, all content on this website is based on the principles of spreading positive energy and promoting mutual learning. We may reference some publicly available content on the internet. If there is any copyright infringement, please contact us at zhangsiming10307@gmail.com to request removal. Thank you for your understanding and support.\n","link":"https://zhangsiming-blyq.github.io/about/","section":"","tags":["about","bailiyingqi","English"],"title":"About me"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/bailiyingqi/","section":"tags","tags":null,"title":"bailiyingqi"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/containerd/","section":"tags","tags":null,"title":"containerd"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/containerd/","section":"categories","tags":null,"title":"containerd"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/docker/","section":"categories","tags":null,"title":"docker"},{"body":" 伴随着kubernetes对docker的弃用，containerd开始进入大众视野；相比于kubelet中集成docker-shim连接docker，docker再次条用containerd去管理容器，直接使用containerd可以通过原生CRI接口的调用实现容器runtime，简化了调用链路，更加的灵活可靠。\n一、安装使用 ctr 管理 containerd 1# Install Dependent Libraries 2$ sudo apt-get update 3$ sudo apt-get install libseccomp2 4 5# 下载 6# 目前是下载的1.5.2 7$ wget https://github.com/containerd/containerd/releases/download/v${VERSION}/cri-containerd-cni-${VERSION}-linux-amd64.tar.gz 8 9# 安装 10$ sudo tar --no-overwrite-dir -C / -xzf cri-containerd-cni-${VERSION}-linux-amd64.tar.gz 11# 初始化containerd配置 12$ containerd config default \u0026gt; /etc/containerd/config.toml 13# 修改默认的sandbox_image 14$ vim /etc/containerd/config.toml 15... 16sandbox_image = \u0026#34;registry.cn-beijing.aliyuncs.com/shannonai-k8s/pause:3.1\u0026#34; 17... 18 19# 启动服务 20sudo systemctl daemon-reload 21sudo systemctl start containerd 22 23# 查看版本 24$ ctr version 25Client: 26 Version: 1.4.3 27 Revision: 269548fa27e0089a8b8278fc4fc781d7f65a939b 28 Go version: go1.13.15 29 30Server: 31 Version: 1.4.3 32 Revision: 269548fa27e0089a8b8278fc4fc781d7f65a939b 33 UUID: b7e3b0e7-8a36-4105-a198-470da2be02f2 二、containerd 使用 2.1 运行一个 busybox 镜像： demo: 1# 拉取镜像 2$ ctr -n k8s.io i pull docker.io/library/busybox:latest 3# 创建一个container(此时还未运行) 4$ ctr -n k8s.io container create docker.io/library/busybox:latest busybox 5# 创建一个task 6$ ctr -n k8s.io task start -d busybox 7 8# 上述步骤也可以简写成如下 9$ ctr -n k8s.io run -d docker.io/library/busybox:latest busybox 查看容器在宿主机的 pid，及状态:\n1$ ctr -n k8s.io task ls 2TASK PID STATUS 3busybox 2356 RUNNING 进入容器：\n1$ ctr -n k8s.io t exec --exec-id $RANDOM -t busybox sh 杀死移除容器：\n1$ ctr -n k8s.io t kill -s SIGKILL busybox 2$ ctr -n k8s.io t rm busybox 3WARN[0000] task busybox exit with non-zero exit code 137 2.2 其他 ctr 命令 镜像标记:\n1$ ctr -n k8s.io i tag A B 2# 若新镜像reference 已存在, 需要先删除新reference, 或者如下方式强制替换 3$ ctr -n k8s.io i tag --force A B 删除镜像:\n1$ ctr -n k8s.io i rm A 拉取镜像:\n1$ ctr -n k8s.io i pull -k A 查看镜像:\n1$ ctr -n k8s.io i ls 推送镜像:\n1$ ctr -n k8s.io i push -k A 导出镜像:\n1$ ctr -n k8s.io i export A.tar A 导入镜像:\n1$ ctr -n k8s.io i import A.tar 读取日志: 导出到文件中进行读取:\n1$ ctr -n k8s.io run --log-uri file:///var/log/xx.log 三、containerd + docker 安装新版本的 docker-ce 默认，采用 containerd(--containerd=/run/containerd/containerd.sock)\n1$ systemctl status docker 2● docker.service - Docker Application Container Engine 3 Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) 4 Active: active (running) since 四 2021-06-10 15:46:44 CST; 1h 46min ago 5 Docs: https://docs.docker.com 6 Main PID: 4965 (dockerd) 7 CGroup: /system.slice/docker.service 8 ├─1830 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/e0a50564d9a23a85240f54f3bf 9 ├─4965 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock 10 ├─5402 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 8082 -container-ip 172.17.0.2 -container-port 80 11 └─5417 /usr/bin/docker-proxy -proto tcp -host-ip :: -host-port 8082 -container-ip 172.17.0.2 -container-port 80 注意： ctr 直接跑起来一个容器只有 lo 网卡，也就是无法与外网通信；如果想连接外网，请参考 其他参考链接:\nhttps://mp.weixin.qq.com/s/A9zU7gZH0liLjc-e-dhk8A\nhttps://blog.csdn.net/tongzidane/article/details/114587138\nhttps://github.com/containerd/containerd/blob/master/docs/cri/installation.md\n","link":"https://zhangsiming-blyq.github.io/post/kubernetes/containerd/","section":"post","tags":["kubernetes","containerd","中文"],"title":"浅谈 containerd"},{"body":"","link":"https://zhangsiming-blyq.github.io/series/","section":"series","tags":null,"title":"Series"}]
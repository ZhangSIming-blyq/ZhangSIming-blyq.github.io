[{"body":"","link":"https://zhangsiming-blyq.github.io/","section":"","tags":null,"title":"bailiyingqi's blog"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/golang/","section":"tags","tags":null,"title":"golang"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/golang/","section":"categories","tags":null,"title":"golang"},{"body":"","link":"https://zhangsiming-blyq.github.io/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/python/","section":"tags","tags":null,"title":"python"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/python/","section":"categories","tags":null,"title":"python"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":" 本文为golang、python新手通俗地解释什么是闭包，以及在这两种语言中使用闭包时候需要注意的地方。本文运行的时候保留了package、import等重复的代码部分，为了方便大家复制下来自己跑一下，劣势是导致有部分冗杂，还请理解。\n什么是闭包? a closure is a record storing a function together with an environment.\n闭包的两大要素：函数和环境\n函数：指的是在闭包实际实现的时候，往往通过调用一个外部函数返回其内部函数来实现的。内部函数可能是内部实名函数、匿名函数或者一段lambda表达式; 环境：在实际中引用环境是指外部函数的环境，闭包保存/记录了它产生时的外部函数的所有环境; 综上，闭包是一种延伸了作用域的函数，他会保留定义函数时存在的自由变量(未在本地作用域中绑定的变量)的绑定，这样调用函数时，所以定义作用域不可用了，但是仍能使用那些绑定。下面看一下闭包的常见用法：\ngolang:\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func outer() func(v int) { 6 x := 1 7 return func(v int) { 8 y := x + v 9 fmt.Println(x) 10 fmt.Println(y) 11 } 12} 13 14func main() { 15 a := outer() 16 a(1) 17 a(2) 18} 19 20// 输出结果 211 222 231 243 golang例子里面的逻辑中，a是一个闭包，闭包函数为内部的func(v int){}, 闭包环境为外部的x, 由于'捕获'了外部环境，所以每次执行闭包x都是1，最后输出结果1,2,1,3。\npython:\n1def outer(): 2 x = 1 3 4 def inner(v): 5 y = x + v 6 print(x) 7 print(y) 8 return inner 9 10a = outer() 11a(1) 12a(2) 13 14# 输出结果 151 162 171 183 python例子中，同样闭包函数为inner(v), 闭包环境为x，所以每次执行闭包x都为1(因为闭包环境被捕获了), 然后闭包里面的逻辑是把x加上闭包传入的参数，所以最后的输出结果是1,2,1,3。\ngolang闭包 golang匿名函数 由于golang中匿名函数的使用非常频繁，所以先提出来在这里说一下：匿名函数及其\u0026quot;捕获\u0026quot;的自由变量被称为闭包, 且无论正常使用、在for循环使用，defer中使用都一样为闭包，看一下下面这个例子：\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func main() { 6 x := 1 7 a := func(v int) { 8 y := x + v 9 fmt.Println(x) 10 fmt.Println(y) 11 } 12 a(2) 13 a(3) 14} 15 16// 输出结果 171 183 191 204 上面的例子中a是闭包，闭包函数为匿名函数func(v int){}, 闭包环境为x, 所以结果为1,3,1,4。\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func main() { 6 y := 2 7 8 defer func() { 9 fmt.Println(y) 10 }() 11} 12 13// 输出结果 142 在defer中使用匿名函数也一样，匿名函数为defer中的func(){}, 闭包环境为y, 所以输出结果为2。\ngolang修改闭包环境 首先说明一点，golang中所有传参都是值传递，如果有类似引用传递的说法(本文后面为了方便表达也会提及), 其实就是底层传递的指针的'值',以此实现的所谓的引用传递;\n如果想要在闭包内(闭包函数中)修改闭包环境，golang中比较省事，因为golang为声明式语言，赋值和声明是不同的写法(:= 为声明，=为赋值); 而且golang闭包\u0026quot;捕获\u0026quot;闭包环境的本质就是引用传递而非值传递, 所以直接修改即可，看下面例子： 1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func make_avg() func(v int) { 6 count := 0 7 total := 0 8 return func(v int) { 9 count += 1 10 total += v 11 fmt.Println(float32(total)/float32(count)) 12 } 13} 14 15func main() { 16 a := make_avg() 17 a(1) 18 a(2) 19 a(3) 20} 21 22// 输出结果 231 241.5 252 例子为计算平均值，闭包为a，闭包函数为内部的func(v int){}匿名函数，闭包环境为count和total；count += 1, total += v 都是直接修改闭包环境的行为，并且得到了预期的效果;\n特别地，可以利用golang闭包\u0026quot;捕获\u0026quot;闭包环境的本质就是引用传递这一特性在匿名函数内部(闭包函数内部)修改全局变量(闭包环境)，看下面这个例子：\n1package main 2 3import ( 4 \u0026#34;fmt\u0026#34; 5) 6 7var x int = 1 8 9func main() { 10 a := func() { 11 x += 1 12 } 13 fmt.Println(x) 14 a() 15 fmt.Println(x) 16} 17 18// 输出结果 191 202 如果要在闭包外修改闭包环境 你一定会有所疑问，闭包外能修改闭包环境吗？修改了这还算\u0026quot;闭\u0026quot;包么，没闭住啊；其实golang中是可以的，记住下面两句话：\n如果外部函数的所有变量可见性都是local的，即生命周期在外部函数结束时也结束的，那么闭包的环境也是封闭的; 反之，那么闭包其实不再封闭，全局可见的变量的修改，也会对闭包内的这个变量造成影响; 通俗来讲就是如果通过指针可以修改闭包环境，那么就可以从闭包外部修改闭包环境, 看下面这个例子：\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func foo1(x *int) func() { 6 return func() { 7 *x = *x + 1 8 fmt.Println(*x) 9 } 10} 11func foo2(x int) func() { 12 return func() { 13 x = x + 1 14 fmt.Println(x) 15 } 16} 17 18func main() { 19 x := 133 20 f1 := foo1(\u0026amp;x) 21 f2 := foo2(x) 22 f1() 23 f1() 24 f2() 25 f2() 26 27 x = 233 28 f1() 29 f1() 30 f2() 31 f2() 32 33 foo1(\u0026amp;x)() 34 foo1(\u0026amp;x)() 35 foo2(x)() 36 foo2(x)() 37} 猛地一看有点乱，这里需要分析两个闭包内部的逻辑，一个为指针变量相加，按照我们前面说的\u0026quot;通过指针可以修改闭包环境\u0026quot;，每次执行闭包或者在外面直接赋值都会真正的改变变量的值，而不使用指针的foo2，也就是正常的闭包，即闭包环境只在闭包内；所以前四组输出为：\n1134 2135 3134 4135 中间四组由于外部强制修改x的值，所以f1闭包在修改后的233上累加，f2闭包正在自己的环境里累加，所以输出为：\n1234 2235 3136 4137 最后四组由于生成了四个新的闭包，所以foo1部分仍为基于当前x的值累加，且累加后的值实际作用到全局变量x中；foo2中的累加仍然是在自己的闭包内, 所以输出为：\n1236 2237 3238 4238 通过这个例子我们区分开了从闭包内、从闭包外修改闭包环境的方法。\n闭包的延迟绑定 这个问题是每一个golang新手都会碰到，非常疑惑的问题；请记住下面这句话：执行闭包的时候，闭包环境的声明周期得到了保证，并且会去外部环境寻找最新的闭包环境(数值), 下面例子中执行闭包的时候i是闭包环境，执行闭包的时候最新数值已经是10了，所以全部会输出10。\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func main() { 6 var handlers []func() 7 for i := 0; i \u0026lt; 10; i++ { 8 handlers = append(handlers, func() { 9 fmt.Println(i) 10 }) 11 } 12 for _, handler := range handlers { 13 handler() 14 } 15} 16 17// 输出结果 1810 1910 2010 2110 2210 2310 2410 2510 2610 2710 解决办法是，在for循环中复制一个非闭包引用的环境变量，然后使用这个值代替闭包环境，改动版本如下：\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func main() { 6 var handlers []func() 7 for i := 0; i \u0026lt; 10; i++ { 8 // a因为每次都重新声明，所以不是闭包环境 9 a := i 10 handlers = append(handlers, func() { 11 fmt.Println(a) 12 }) 13 } 14 for _, handler := range handlers { 15 handler() 16 } 17} 18 19// 输出结果 200 211 222 233 244 255 266 277 288 299 其实原理搞清楚了，也就无关乎for循环的事情了，正常使用，defer使用都会受到这个原则约束，执行闭包的时候，闭包环境的声明周期得到了保证，并且会去外部环境寻找最新的闭包环境(数值)\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func main() { 6 x, y := 1, 2 7 8 defer func(a int) { 9 fmt.Println(a, y) 10 }(x) 11 12 x += 100 13 y += 100 14} 15 16// 输出结果, y是闭包环境，所以执行闭包的时候会去找最新的数值，而a不是闭包环境，复制了x的值所以不受牵连 171 102 go routine中使用匿名函数是个常见场景，也存在这个问题，看下面的例子：\n1package main 2 3import ( 4 \u0026#34;fmt\u0026#34; 5 \u0026#34;time\u0026#34; 6) 7 8func show(val int) { 9 fmt.Println(val) 10} 11 12func main() { 13 values := []int{1, 2, 3, 5} 14 for _, val := range values { 15 go show(val) 16 } 17 time.Sleep(time.Second) 18} 19 20// 每次都会输出1,2,3,5四个，尽管顺序不同，因为没有使用匿名函数，也就不是闭包 215 221 233 242 如果使用闭包，结果肯定都是输出切片的最后一个值，也就是5。\n1package main 2 3import ( 4 \u0026#34;fmt\u0026#34; 5 \u0026#34;time\u0026#34; 6) 7 8func main() { 9 values := []int{1, 2, 3, 5} 10 for _, val := range values { 11 go func(){ 12 fmt.Println(val) 13 }() 14 } 15 time.Sleep(time.Second) 16} 17 18// 输出结果 195 205 215 225 修改方法和for循环例子里面一样，采用传参避开闭包环境。\n1package main 2 3import ( 4 \u0026#34;fmt\u0026#34; 5 \u0026#34;time\u0026#34; 6) 7 8func main() { 9 values := []int{1, 2, 3, 5} 10 for _, val := range values { 11 go func(val int){ 12 fmt.Println(val) 13 }(val) 14 } 15 time.Sleep(time.Second) 16} 17 18// 输出结果 191 205 213 222 python闭包 python修改闭包环境 从内部修改闭包环境 由于python不是声明式语言，一个\u0026quot;=\u0026quot;吃遍天，我们需要用nonlocal参数来显式声明这个变量为闭包环境，而不是局部变量，看一下下面的例子, 如果和golang一样的方式直接改，有时候会出问题：\n闭包环境为列表\n1def make_avg(count, total): 2 count = [] 3 4 def avg(v): 5 count.append(v) 6 total = sum(count) 7 print(sum(count)/len(count)) 8 return avg 9 10 11a = make_avg(0, 0) 12a(1) 13a(2) 14a(3) 15 16 17# 可以正常输出 181.0 191.5 202.0 闭包环境为字符串\n1def make_avg(count, total): 2 count = 0 3 total = 0 4 5 def avg(v): 6 count += 1 7 total += v 8 print(total/count) 9 return avg 10 11 12a = make_avg(0, 0) 13a(1) 14a(2) 15a(3) 16 17 18# 输出报错 19Traceback (most recent call last): 20 File \u0026#34;/root/code/linux/blog/aa.py\u0026#34;, line 14, in \u0026lt;module\u0026gt; 21 a(1) 22 File \u0026#34;/root/code/linux/blog/aa.py\u0026#34;, line 7, in avg 23 count += 1 24UnboundLocalError: local variable \u0026#39;count\u0026#39; referenced before assignment 因为python有两种类型的数据，一种为可变数据类型，一种为不可变数据类型，在传参的时候可变数据类型采用引用传参，这个和golang闭包直接修改的特性相匹配，但是不可变数据类型比如上面的字符串就会出现问题，因为python会把他作为新生成的局部变量，所以解决办法就是使用nonlocal告诉python解释器，我这个变量为闭包环境，这样才能正常运行，正确写法如下：\n1def make_avg(count, total): 2 count = 0 3 total = 0 4 5 def avg(v): 6 nonlocal count, total 7 count += 1 8 total += v 9 print(total/count) 10 return avg 11 12 13a = make_avg(0, 0) 14a(1) 15a(2) 16a(3) 17 18# 输出结果 191.0 201.5 212.0 闭包与装饰器 本来想说一说python装饰器的，奈何篇幅到此已经不少，故决定留着下次再谈。\n总结 上面有很多例子，每个例子都看了之后，想明白为什么，那么闭包问题就迎刃而解了；另外由于闭包并不常用，个人感觉也并不很实用，如果工作中没必要用闭包的时候一定不要强行实用闭包装逼，否则装逼不成反而搞出内存泄露可不是小问题\n闭包的两大要素：函数和环境; 闭包是一种延伸了作用域的函数，他会保留定义函数时存在的自由变量(未在本地作用域中绑定的变量)的绑定，这样调用函数时，所以定义作用域不可用了，但是仍能使用那些绑定; golang中匿名函数的使用实际上就是闭包; 可以通过指针从闭包以外修改闭包环境变量; golang闭包延迟绑定问题：执行闭包的时候，闭包环境的声明周期得到了保证，并且会去外部环境寻找最新的闭包环境(数值); python闭包中使用nonlocal声明变量为闭包环境, 而不是局部变量; 以上就是我对于闭包的理解，希望golang、python新手看了之后可以避开一个必踩之坑，如有疑问环境随时评论或者邮箱联系，谢谢~\n参考链接：\nhttps://juejin.cn/post/6844904133208604679\nhttps://www.jianshu.com/p/fa21e6fada70\nhttps://zhuanlan.zhihu.com/p/92634505\n","link":"https://zhangsiming-blyq.github.io/post/closure/","section":"post","tags":["golang","python"],"title":"聊一聊golang、python中的闭包"},{"body":" 近期配置了uber家的zap日志库，觉得性能比较强，展示比较美观，在这里做一个分享，代码在第三部分可以自取。\n为什么不选择原生log? 说起golang如何优雅的打印日志，任何一个golang的初学者大概都是用的原生log库，或者直接fmt.Println()...但是这种方式并不优雅，并且有以下缺点：\n对于基础日志：不能细粒度区分info和debug级别的日志; 对于错误日志: 不支持除了fatal或者panic的普通error级别告知。 log示例 1package main 2 3import \u0026#34;log\u0026#34; 4 5func main() { 6 log.Print(\u0026#34;info or debug\u0026#34;) 7 log.Fatal(\u0026#34;fatal\u0026#34;) 8 log.Panic(\u0026#34;panic\u0026#34;) 9} 10 11// 输出如下 122022/11/23 22:23:32 info or debug 132022/11/23 22:23:32 fatal 14exit status 1 为什么不选择logrus? logrus也是比较常用的自定义日志库，不过因为Go语言是一门强类型的静态语言，而logrus需要知道数据的类型来打印日志，怎么办呢？实现方案是使用反射，这导致大量分配计数。虽然通常不是一个大问题（取决于代码），但是在大规模、高并发的项目中频繁的反射开销影响很大，所以这里不进行采用。\n仓库链接: logrus\nlogrus示例 1package main 2 3import log \u0026#34;github.com/sirupsen/logrus\u0026#34; 4 5var logger = log.New() 6 7func main() { 8 // 这里可以通过WithFields来附加字段 9 logger.WithFields(log.Fields{\u0026#34;testfield\u0026#34;: \u0026#34;test\u0026#34;}).Info(\u0026#34;test info\u0026#34;) 10 logger.Info(\u0026#34;info\u0026#34;) 11 logger.Error(\u0026#34;Error\u0026#34;) 12 logger.Fatal(\u0026#34;Error\u0026#34;) 13 logger.Panic(\u0026#34;Panic\u0026#34;) 14} 15 16// 输出如下 17INFO[0000] test info testfield=test 18INFO[0000] info 19ERRO[0000] Error 20FATA[0000] Error 21exit status 1 聊一聊zap日志库 仓库链接：zap\n1. zap支持的六种日志级别 名称 作用 Debug 打印调试时候的debug日志 Info 正常输出普通信息 Warn 警告，可能有部分出现问题但是不影响程序运行 Error 错误，不会中断程序运行但是程序可能已经不正常 Fatal 输出信息，然后调用os.Exit Panic 调用panic 个人建议如果有错就在初始化检查中可以panic掉，之后程序运行期间就不要碰到小错误就panic掉了，能容忍就抛出Error，这样方便程序员主动停掉服务排查而不是已经工作起来的程序异常挂掉。\n2. zap的性能问题及benchmark 官方github在Performance模块中明确说道:\nFor applications that log in the hot path, reflection-based serialization and string formatting are prohibitively expensive — they're CPU-intensive and make many small allocations. Put differently, using encoding/json and fmt.Fprintf to log tons of interface{}s makes your application slow.\nZap takes a different approach. It includes a reflection-free, zero-allocation JSON encoder, and the base Logger strives to avoid serialization overhead and allocations wherever possible. By building the high-level SugaredLogger on that foundation, zap lets users choose when they need to count every allocation and when they'd prefer a more familiar, loosely typed API.\n也就是说，基于反射的序列化，或者字符串格式化这种是很吃cpu资源的，严重了会导致程序变慢(logrus存在这个问题); zap这里定义了一个无反射的，无分配的json encoder来优化这一部分，并且在此基础上提供了Sugar可以舍弃部分性能换取更简单的配置这一特点，我们下面的部分会演示。\n通过benckmark可以看出, zap和zap-sugar在性能上还是非常有优势的!\n3. 代码展示 下面分段展示完整代码，第一部分是各种包的导入；值得注意的是定义了默认的DefaultLog，用于把Info级别以上的日志人性化可读地输出到控制台; 还支持通过给InitLogger函数传递参数自定义想要的日志形式，支持(假设库名叫logger)：\n初始化方式 日志形式 logger.DefaultLog 人性化输出到控制台, 输出高于Info级别的日志 logger.InitLogger(\u0026quot;\u0026quot;, \u0026quot;console\u0026quot;, \u0026quot;debug\u0026quot;).Sugar() 人性化输出到控制台, 输出高于Debug级别的日志 logger.InitLogger(\u0026quot;\u0026quot;, \u0026quot;file\u0026quot;, \u0026quot;debug\u0026quot;).Sugar() 人性化输出到文件(默认当前目录下的log目录，会自动创建), 输出高于Debug级别的日志 logger.InitLogger(\u0026quot;json\u0026quot;, \u0026quot;console\u0026quot;, \u0026quot;debug\u0026quot;).Sugar() json格式输出到控制台, 输出高于Debug级别的日志 1package logger 2 3import ( 4\t\u0026#34;os\u0026#34; 5 6\t\u0026#34;github.com/natefinch/lumberjack\u0026#34; 7\t\u0026#34;go.uber.org/zap\u0026#34; 8\t\u0026#34;go.uber.org/zap/zapcore\u0026#34; 9) 10 11var ( 12\tMyLogger = InitLogger() 13\tDefaultLog = MyLogger.Sugar() 14) 15 16func InitLogger(logArgs ...string) *zap.Logger { 17 ... 18} InitLogger函数中大概分为四个部分:\n接收参数初始化变量 生成encoder 定义日志级别和输出形式 根据指定配置生成并返回日志实例 代码 1func InitLogger(logArgs ...string) *zap.Logger { 2\tvar logger *zap.Logger 3\tvar coreArr []zapcore.Core 4\tvar format, logType, priority string 5 6\t// get the parameters 7\tswitch { 8\tcase len(logArgs) \u0026gt;= 3: 9\tformat = logArgs[0] 10\tlogType = logArgs[1] 11\tpriority = logArgs[2] 12\tcase len(logArgs) == 2: 13\tformat = logArgs[0] 14\tlogType = logArgs[1] 15\tcase len(logArgs) == 1: 16\tformat = logArgs[0] 17\t} 18 19\t// get encoder 20\tencoderConfig := zap.NewProductionEncoderConfig() 21\tencoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder // time format 22\tencoderConfig.EncodeLevel = zapcore.CapitalColorLevelEncoder // use different color for various log levels 23\t// uncomment next line to show full path of the code 24\t// encoderConfig.EncodeCaller = zapcore.FullCallerEncoder 25\t// NewJSONEncoder() for json，NewConsoleEncoder() for normal 26\tif format == \u0026#34;\u0026#34; { 27\tformat = \u0026#34;normal\u0026#34; 28\t} 29\tencoder := zapcore.NewConsoleEncoder(encoderConfig) 30\tif format == \u0026#34;json\u0026#34; { 31\tencoder = zapcore.NewJSONEncoder(encoderConfig) 32\t} 33 34\t// log levels 35\terrorPriority := zap.LevelEnablerFunc(func(lev zapcore.Level) bool { 36\treturn lev \u0026gt;= zap.ErrorLevel 37\t}) 38\tinfoPriority := zap.LevelEnablerFunc(func(lev zapcore.Level) bool { 39\treturn lev \u0026lt; zap.ErrorLevel \u0026amp;\u0026amp; lev \u0026gt;= zap.InfoLevel 40\t}) 41\tdebugPriority := zap.LevelEnablerFunc(func(lev zapcore.Level) bool { 42\treturn lev \u0026lt; zap.InfoLevel \u0026amp;\u0026amp; lev \u0026gt;= zap.DebugLevel 43\t}) 44\tif logType == \u0026#34;\u0026#34; { 45\tlogType = \u0026#34;console\u0026#34; 46\t} 47\t// writeSyncer for debug file 48\tdebugFileWriteSyncer := zapcore.AddSync(\u0026amp;lumberjack.Logger{ 49\tFilename: \u0026#34;./log/debug.log\u0026#34;, // will create if not exist 50\tMaxSize: 128, // max size for log file, unit:MB 51\tMaxBackups: 3, // max backup\u0026#39;s count 52\tMaxAge: 10, // max reserved days for log file 53\tCompress: false, // whether to compress or not 54\t}) 55\tdebugFileCore := zapcore.NewCore(encoder, os.Stdout, debugPriority) 56\tif logType == \u0026#34;file\u0026#34; { 57\tdebugFileCore = zapcore.NewCore(encoder, zapcore.NewMultiWriteSyncer(debugFileWriteSyncer, zapcore.AddSync(os.Stdout)), debugPriority) 58\t} 59\t// writeSyncer for info file 60\tinfoFileWriteSyncer := zapcore.AddSync(\u0026amp;lumberjack.Logger{ 61\tFilename: \u0026#34;./log/info.log\u0026#34;, 62\tMaxSize: 128, 63\tMaxBackups: 3, 64\tMaxAge: 10, 65\tCompress: false, 66\t}) 67\tinfoFileCore := zapcore.NewCore(encoder, os.Stdout, infoPriority) 68\tif logType == \u0026#34;file\u0026#34; { 69\tinfoFileCore = zapcore.NewCore(encoder, zapcore.NewMultiWriteSyncer(infoFileWriteSyncer, zapcore.AddSync(os.Stdout)), infoPriority) 70\t} 71\t// writeSyncer for error file 72\terrorFileWriteSyncer := zapcore.AddSync(\u0026amp;lumberjack.Logger{ 73\tFilename: \u0026#34;./log/error.log\u0026#34;, 74\tMaxSize: 128, 75\tMaxBackups: 5, 76\tMaxAge: 10, 77\tCompress: false, 78\t}) 79\terrorFileCore := zapcore.NewCore(encoder, os.Stdout, errorPriority) 80\tif logType == \u0026#34;file\u0026#34; { 81\terrorFileCore = zapcore.NewCore(encoder, zapcore.NewMultiWriteSyncer(errorFileWriteSyncer, zapcore.AddSync(os.Stdout)), errorPriority) 82\t} 83 84\tswitch priority { 85\tcase \u0026#34;\u0026#34;: 86\tcoreArr = append(coreArr, infoFileCore) 87\tcoreArr = append(coreArr, errorFileCore) 88\tcase \u0026#34;info\u0026#34;: 89\tcoreArr = append(coreArr, infoFileCore) 90\tcoreArr = append(coreArr, errorFileCore) 91\tcase \u0026#34;error\u0026#34;: 92\tcoreArr = append(coreArr, errorFileCore) 93\tcase \u0026#34;debug\u0026#34;: 94\tcoreArr = append(coreArr, debugFileCore) 95\tcoreArr = append(coreArr, infoFileCore) 96\tcoreArr = append(coreArr, errorFileCore) 97\t} 98\tlogger = zap.New(zapcore.NewTee(coreArr...), zap.AddCaller()) //zap.AddCaller() is to show the line number 99\treturn logger 100} 4. 效果演示 1package main 2 3import ( 4\t\u0026#34;xxx/pkg/logger\u0026#34; 5) 6 7// var log = logger.DefaultLog 8var log = logger.InitLogger(\u0026#34;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;debug\u0026#34;).Sugar() 9 10func main() { 11\tlog.Debug(\u0026#34;debug\u0026#34;) 12\tlog.Info(\u0026#34;info\u0026#34;) 13\tlog.Warn(\u0026#34;warn\u0026#34;) 14\tlog.Error(\u0026#34;error\u0026#34;) 15\tlog.Fatal(\u0026#34;fatal\u0026#34;) 16\tlog.Panic(\u0026#34;panic\u0026#34;) 17} 输出 在性能开销很小的情况下，还可以清晰的展示日志级别，并且使用颜色区分, 十分强大美观，欢迎读者拷贝并使用我个人这份配置，有疑问我们下面评论区随时沟通探讨。\n","link":"https://zhangsiming-blyq.github.io/post/gozap/","section":"post","tags":["golang"],"title":"使用zap打造你的golang日志库"},{"body":"题目要求 有一个整形数组arr和一个大小为w的窗口从数组最左边滑到最右边，窗口每次向右边滑一个位置。例如，数组为[4,3,5,4,3,3,6,7], 窗口大小为3时:\n如果数组长度为n，窗口大小为w，则一共产生 n-w+1 个窗口最大值。请实现一个函数：\n输入：整形数组arr，窗口大小为w 输出：一个长度为 n-w+1 的数组res，res[i]表示每一种窗口状态下的最大值，要求时间复杂度O(N*w) 解题思路 整体逻辑：\n创建一个双端数组，我们需要根据不同情况从两端分别push和pop 核心就是循环这个arr，把最大的值在arr中的下标放到双端数组的最左端(对应下文程序中\u0026quot;for !bq.Empty() \u0026amp;\u0026amp; arr[i] \u0026gt; arr[bq.Back().(int)] {}\u0026quot;的处理) 当窗口走过对应数值区域的时候，把相应的数据进行过期(对应下文程序中\u0026quot;if bq.Front().(int) == i-w {}\u0026quot;) 注意，因为要输出长度为 n-w+1, 所以循环当\u0026quot;i \u0026gt;= w -1\u0026quot;的时候，才开始输出结果到res中 golang实现 1package main 2 3func Getmaxwindow(arr []int, w int) (res []int) { 4\tres = []int{} 5\t// 1. create a bidirectional queue to store the result 6\tbq := newCustomQueue() 7\t// 2. for i \u0026lt; length(arr) 8\tfor i := 0; i \u0026lt; len(arr); i++ { 9\t// 3. for !empty or value(in queue) \u0026gt; value(in bqueue), popback bqueue 10\tfor !bq.Empty() \u0026amp;\u0026amp; arr[i] \u0026gt; arr[bq.Back().(int)] { 11\tbq.PopBack() 12\t} 13\t// 4. pushback id(in queue) 14\tbq.Pushback(i) 15\t// 5. if bqueue\u0026#39;s id \u0026gt;= i - w, expire the front value(in bqueue) 16\tif bq.Front().(int) == i-w { 17\tbq.PopFront() 18\t} 19\t// 6. record results to res slice 20\tif i \u0026gt;= w-1 { 21\tres = append(res, arr[bq.Front().(int)]) 22\t} 23\t} 24\treturn res 25} 测试用例 1package main 2 3import ( 4\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 5\t\u0026#34;testing\u0026#34; 6) 7 8func TestGetmaxwindow(t *testing.T) { 9\ttests := []struct { 10\t// 名字 11\tname string 12 13\t// 输入部分 14\tarr []int 15\tw int 16 17\t// 输出部分 18\twantRes []int 19\t}{ 20\t{ 21\tname: \u0026#34;getmaxwindow\u0026#34;, 22\tarr: []int{4, 3, 5, 4, 3, 3, 6, 7}, 23\tw: 3, 24\twantRes: []int{5, 5, 5, 4, 6, 7}, 25\t}, 26\t{ 27\tname: \u0026#34;getmaxwindow1\u0026#34;, 28\tarr: []int{4, 3, 5, 4, 3, 3, 6, 7}, 29\tw: 4, 30\twantRes: []int{5, 5, 5, 6, 7}, 31\t}, 32\t{ 33\tname: \u0026#34;getmaxwindow2\u0026#34;, 34\tarr: []int{4, 3, 9, 8, 3, 1, 6, 7}, 35\tw: 3, 36\twantRes: []int{9, 9, 9, 8, 6, 7}, 37\t}, 38\t} 39\tfor _, tt := range tests { 40\tt.Run(tt.name, func(t *testing.T) { 41\t// test case 42\tres := Getmaxwindow(tt.arr, tt.w) 43\tassert.Equal(t, tt.wantRes, res) 44\t}) 45\t} 46} 测试结果\n1$ go test -run ^TestSortStark$ . -v 2$ go test -run ^TestGetmaxwindow$ . -v 3=== RUN TestGetmaxwindow 4=== RUN TestGetmaxwindow/getmaxwindow 5=== RUN TestGetmaxwindow/getmaxwindow1 6=== RUN TestGetmaxwindow/getmaxwindow2 7--- PASS: TestGetmaxwindow (0.00s) 8 --- PASS: TestGetmaxwindow/getmaxwindow (0.00s) 9 --- PASS: TestGetmaxwindow/getmaxwindow1 (0.00s) 10 --- PASS: TestGetmaxwindow/getmaxwindow2 (0.00s) 11PASS 测试没有问题，输出结果符合预期。\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/6/","section":"post","tags":["algorithm"],"title":"【算法系列】生成窗口最大值数组"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/algorithm/","section":"tags","tags":null,"title":"algorithm"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/algorithm/","section":"categories","tags":null,"title":"algorithm"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/kubernetes/","section":"tags","tags":null,"title":"kubernetes"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/kubernetes/","section":"categories","tags":null,"title":"kubernetes"},{"body":" kubernetes各个组件都是加密通信的, 那么都有哪些证书、各个证书怎么交互、这些证书什么时候过期，这个就变得至关重要; 本文引用了一些其他网络内容(均已附上原文链接)，并适当补充完善，用于让新手完善熟悉kubernetes证书体系(如有侵权联系邮箱可以删除)。\n一、数字证书原理 1.1 传统非对称加密 1message --\u0026gt; (公钥加密) --\u0026gt; || 传输 || --\u0026gt; (私钥解密) --\u0026gt; message 注意:\n1.这里与数字证书认证相反，是公钥加密私钥解密\n2.公钥私钥需要是一个秘钥对\n1.2 哈希函数 1message --\u0026gt; H(message) --\u0026gt; Hash message 处理加入一个随机数，然后得出结果(加盐); 可以有效缓解在输入值是一个有效的集合，哈希值也是固定长度被别人‘试’出来的几率\n1message --\u0026gt; H(R|message) --\u0026gt; Hash message 1.3 数字证书 1.3.1 数字签名 数字签名；把数据根据私钥/哈希进行加密，然后必须要对应的公钥来进行解密认证才能确保数据安全。前半句的加密过程就叫做 '数字签名'\n1.3.2 数字证书认证过程 Alice 想要通过证书加密让 Bob 安全读到自己的信息流程如下：\nAlice 在本地生成 Private Key 和 CSR（Certificate Signing Request）。CSR 中包含了 Alice 的公钥和姓名，机构、地址等身份信息。 Alice 使用该 CSR 向证书机构发起数字证书申请。 证书机构验证 Alice 的身份后，使用 CSR 中的信息生成数字证书，并使用自己的 CA 根证书对应的私钥对该证书签名。 Alice 使用自己的 Private Key 对合同进行签名，然后将签名后的合同和自己的证书一起并发送给 Bob。 Bob 使用操作系统中自带的证书机构根证书中的公钥来验证 Alice 证书中的签名，以确认 Alice 的身份和公钥。(使用内置根证书确认身份并获取Alice证书) Alice 的证书验证成功后，Bob 使用 Alice 证书中的公钥来验证合同中数字签名。(使用刚刚获取的Alice证书(公钥)解析Alice发送的内容) 合同数字签名通过验证，可以证明该合同为 Alice 本人发送，并且中间未被第三方篡改过。 注意：\n1.自签发证书：证书签发商和证书持有者是同一个人，缺点是没有人能告诉你\u0026quot;这个人就是这个人(我就是我)\u0026quot;; 所以需要上面的信任证书机构的介入，多一环进行认证证明\n2.证书链：但是信任的证书机构的根证书是很机密的，一旦被盗取可能会导致很多人都无法证明\u0026quot;我就是我\u0026quot;, 所以一般会引入一些中间证书商，多一层上面的解析循环；这样也保证了万一中间商证书泄露，不至于全部使用证书机构的人都陷于危险之中\n3.证书签发一般都是有一定时期的, 过期了就如同废纸\n参考链接：\nhttps://zhaohuabing.com/post/2020-03-19-pki/\n二、双向 TLS 认证原理 双向 TLS 认证需要的场景是，服务端和客户端都需要确认，这样就是正反都走一遍上面的流程，来完成双向的数据加密，保证双向的数据安全\n参考链接：\nhttps://medium.com/sitewards/the-magic-of-tls-x509-and-mutual-authentication-explained-b2162dec4401/\n三、Kubernetes 中的证书工作机制 3.1 Kubernetes 中使用到的主要证书 etcd 集群中各个节点之间相互通信使用的证书。由于一个 etctd 节点既为其他节点提供服务，又需要作为客户端访问其他节点，因此该证书同时用作服务器证书和客户端证书 etcd 集群向外提供服务使用的证书。该证书是服务器证书 kube-apiserver 作为客户端访问 etcd 使用的证书。该证书是客户端证书 kube-apiserver 对外提供服务使用的证书。该证书是服务器证书 kube-controller-manager 作为客户端访问 kube-apiserver 使用的证书,该证书是客户端证书 kube-scheduler 作为客户端访问 kube-apiserver 使用的证书,该证书是客户端证书 kube-proxy 作为客户端访问 kube-apiserver 使用的证书,该证书是客户端证书 kubelet 作为客户端访问 kube-apiserver 使用的证书,该证书是客户端证书 管理员用户通过 kubectl 访问 kube-apiserver 使用的证书,该证书是客户端证书 kubelet 对外提供服务使用的证书。该证书是服务器证书 kube-apiserver 作为客户端访问 kubelet 采用的证书。该证书是客户端证书 kube-controller-manager 用于生成和验证 service-account token 的证书。该证书并不会像其他证书一样用于身份认证，而是将证书中的公钥/私钥对用于 service account token 的生成和验证。kube-controller-manager 会用该证书的私钥来生成 service account token，然后以 secret 的方式加载到 pod 中。pod 中的应用可以使用该 token 来访问 kube-apiserver， kube-apiserver 会使用该证书中的公钥来验证请求中的 token 注意:\n只有当你运行 kube-proxy 并要支持 扩展 API 服务器 时，才需要 front-proxy 证书 3.2 etcd 证书 1$ ssh 10.20.11.120 2$ sudo systemctl status etcd 3● etcd.service - etcd docker wrapper 4 Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled) 5 Active: active (running) since Mon 2020-08-03 15:42:36 CST; 1 months 18 days ago 6$ cat /etc/etcd.env | tail -13 7 8# TLS settings 9ETCD_TRUSTED_CA_FILE=/etc/ssl/etcd/ssl/ca.pem # etcd验证访问 etcd 服务器的客户端证书的 CA 根证书, 服务端签名证书和服务端私钥 10ETCD_CERT_FILE=/etc/ssl/etcd/ssl/member-ip-10-20-11-120.pem 11ETCD_KEY_FILE=/etc/ssl/etcd/ssl/member-ip-10-20-11-120-key.pem 12ETCD_CLIENT_CERT_AUTH=true 13 14ETCD_PEER_TRUSTED_CA_FILE=/etc/ssl/etcd/ssl/ca.pem # etcd peer之间验证访问 etcd 服务器的客户端证书的 CA 根证书, 服务端签名证书和服务端私钥 15ETCD_PEER_CERT_FILE=/etc/ssl/etcd/ssl/member-ip-10-20-11-120.pem 16ETCD_PEER_KEY_FILE=/etc/ssl/etcd/ssl/member-ip-10-20-11-120-key.pem 17ETCD_PEER_CLIENT_CERT_AUTH=True 18 19# 验证etcd证书过期时间 20$ sudo openssl x509 -in /etc/ssl/etcd/ssl/ca.pem -noout -dates 21notBefore=Aug 6 05:07:43 2019 GMT 22notAfter=Jul 13 05:07:43 2119 GMT 3.3 kube-apiserver 证书 1$ ssh 10.20.11.120 2$ cd /etc/kubernetes 3$ cat manifests/kube-apiserver.yaml 4apiVersion: v1 5kind: Pod 6metadata: 7 creationTimestamp: null 8 labels: 9 component: kube-apiserver 10 tier: control-plane 11 name: kube-apiserver 12 namespace: kube-system 13spec: 14 containers: 15 - command: 16 - kube-apiserver 17 - --advertise-address=10.20.11.120 18 - --etcd-cafile=/etc/ssl/etcd/ssl/ca.pem # 用于验证 etcd 客户端证书的 CA 根证书, 用于访问 etcd 的客户端证书和私钥 19 - --etcd-certfile=/etc/ssl/etcd/ssl/node-ip-10-20-11-120.pem 20 - --etcd-keyfile=/etc/ssl/etcd/ssl/node-ip-10-20-11-120-key.pem 21 - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver-kubelet-client.crt # 用于访问 kubelet 的客户端证书和私钥 22 - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-kubelet-client.key 23 - --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.crt # 只有当你运行 kube-proxy 并要支持 扩展 API 服务器 时，才需要 front-proxy 证书 24 - --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client.key 25 - --client-ca-file=/etc/kubernetes/ssl/ca.crt # 用于验证访问 kube-apiserver 的客户端的证书的 CA 根证书 26 - --service-account-key-file=/etc/kubernetes/ssl/sa.pub # 用于验证 service account token 的公钥 27 - --tls-cert-file=/etc/kubernetes/ssl/apiserver.crt # 用于对外提供服务的服务器证书和私钥 28 - --tls-private-key-file=/etc/kubernetes/ssl/apiserver.key 29 ... 3.4 controller-manager, kubelet, scheduler 证书 上面三个 apiserver 的客户端组件的证书都写在了对应的 KUBECONFIG 中,名为 controller-manager.conf, kubelet.conf, scheduler.conf, 就不一一展示, 随便列举一个如下:\n1# 包含验证apiserver证书的ca证书, 还有请求时候的看客户端证书和私钥 2$ cat scheduler.conf 3apiVersion: v1 4clusters: 5- cluster: 6 certificate-authority-data: ca证书 7 server: https://10.20.11.120:6443 8 name: kubernetes 9contexts: 10- context: 11 cluster: kubernetes 12 user: system:kube-scheduler 13 name: system:kube-scheduler@kubernetes 14current-context: system:kube-scheduler@kubernetes 15kind: Config 16preferences: {} 17users: 18- name: system:kube-scheduler 19 user: 20 client-certificate-data: server证书 21 client-key-data: 秘钥 3.5 Service Account 秘钥对 Service account 主要被 pod 用于访问 kube-apiserver。 在为一个 pod 指定了 service account 后，kubernetes 会为该 service account 生成一个 JWT token，并使用 secret 将该 service account token 挂载到 pod 上。pod 中的应用可以使用 service account token 来访问 api server。service account 证书被用于生成和验证 service account token。\n注意:\nService account加密过程是文章开头说的\u0026quot;非对称加密\u0026quot;, 也就是说只是controller-manager私钥加密数据, 然后kube-apiserver的公钥进行解密而已 由于是非对称加密, 也就是说没有\u0026quot;双向 tls 认证\u0026quot; istio的做法就是为每个 service account 生成一个证书, 之后就可以\u0026quot;双向 tls 认证\u0026quot; 1$ cat manifests/kube-controller-manager.yaml | grep service-account 2 - --service-account-private-key-file=/etc/kubernetes/ssl/sa.key 3 - --use-service-account-credentials=true 4$ cat manifests/kube-apiserver.yaml | grep service-account 5 - --service-account-key-file=/etc/kubernetes/ssl/sa.pub 四、监控kubernetes证书 比较好用的是开源的x509 exporter\n开源链接：\nhttps://github.com/enix/x509-certificate-exporter/\n1$ gc https://github.com/enix/x509-certificate-exporter.git 2$ cd deploy/charts/x509-certificate-exporter 3$ vim values.yaml 4... 5 daemonSets: 6 master: 7 nodeSelector: 8 \u0026#34;node-role.kubernetes.io/controlplane\u0026#34;: \u0026#34;true\u0026#34; 9 tolerations: 10 - effect: NoSchedule 11 operator: Exists 12 - effect: NoExecute 13 operator: Exists 14 watchDirectories: 15 - /etc/kubernetes/ssl/ 16 nodes: 17 tolerations: 18 - effect: NoSchedule 19 operator: Exists 20 watchKubeconfFiles: 21 - /etc/kubernetes/ssl/kubecfg-kube-node.yaml 22 - /etc/kubernetes/ssl/kubecfg-kube-proxy.yaml 23 24$ helm install x509-certificate-exporter . 25$ k get daemonset 26NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE 27x509-certificate-exporter-master 3 3 3 3 3 node-role.kubernetes.io/controlplane=true 18d 28x509-certificate-exporter-nodes 202 202 197 202 197 \u0026lt;none\u0026gt; 18d 29... 在daemonSet配置里面支持将核心的控制节点证书路径，或者kubeconfig文件写进去，然后部署即可；开源社区提供配套dashboard:\nFAQ 1. 在安装 Kubernetes 时，我们需要为每一个工作节点上的 Kubelet 分别生成一个证书。由于工作节点可能很多，手动生成 Kubelet 证书的过程会比较繁琐怎么办? Kubernetes 提供了一个 certificates.k8s.io API，可以使用配置的 CA 根证书来签发用户证书 Kubernetes 提供了 TLS bootstrapping 的方式来简化 Kubelet 证书的生成过程 过程如下:(需要 apiserver 启用--enable-bootstrap-token-auth)\n调用 kube-apiserver 生成一个 bootstrap token 将该 bootstrap token 写入到一个 kubeconfig 文件中，作为 kubelet 调用 kube-apiserver 的客户端验证方式 通过 --bootstrap-kubeconfig 启动参数将 bootstrap token 传递给 kubelet 进程 Kubelet 采用 bootstrap token 调用 kube-apiserver API，生成自己所需的服务器和客户端证书 证书生成后，Kubelet 采用生成的证书和 kube-apiserver 进行通信，并删除本地的 kubeconfig 文件，以避免 bootstrap token 泄漏风险 参考链接:\nhttps://zhaohuabing.com/post/2020-05-19-k8s-certificate/\nhttps://kubernetes.io/zh/docs/tasks/tls/managing-tls-in-a-cluster/\n2. 如何升级 kubernetes 证书但是不让 serviceaccout 轮换？ 我们都知道 serviceaccount 的 token 是依赖于 sa.key 和 sa.pub 的，也就是说如果这俩秘钥更换了，所有的 serviceaccout 就都失效了，会重新创建(kube-system 命名空间的会自动轮换，其他 ns 的需要手动); 但是如果只是更换证书的话，其实是不需要更换 sa 秘钥对的，因为 sa 秘钥对采用的是\u0026quot;非对称加密\u0026quot;, 也就是说永不过期，除非删除秘钥对；\n事实上我们只要手动更换其他证书即可：\n1$ cp -R /etc/kubernetes/ssl /etc/kubernetes/ssl.backup 2$ cp /etc/kubernetes/admin.conf /etc/kubernetes/admin.conf.backup 3$ cp /etc/kubernetes/controller-manager.conf /etc/kubernetes/controller-manager.conf.backup 4$ cp /etc/kubernetes/kubelet.conf /etc/kubernetes/kubelet.conf.backup 5$ cp /etc/kubernetes/scheduler.conf /etc/kubernetes/scheduler.conf.backup 6 7$ kubeadm alpha certs renew apiserver-kubelet-client 8$ kubeadm alpha certs renew apiserver 9$ kubeadm alpha certs renew front-proxy-client 10$ kubeadm alpha kubeconfig user --client-name system:kube-controller-manager \u0026gt; /etc/kubernetes/controller-manager.conf 11$ kubeadm alpha kubeconfig user --client-name system:kube-scheduler \u0026gt; /etc/kubernetes/scheduler.conf 12$ kubeadm alpha kubeconfig user --client-name system:node:{nodename} --org system:nodes \u0026gt; /etc/kubernetes/kubelet.conf 13 14$ kubeadm alpha kubeconfig user --client-name kubernetes-admin --org system:masters \u0026gt; /etc/kubernetes/admin.conf 15$ cp /etc/kubernetes/admin.conf ~/.kube/config 上述过程每个 master 都需要做(如果有多个 master), 最后重启基础组件即可(嫌麻烦可以直接 master 节点关机重启)\n参考链接：\nhttps://github.com/kubernetes-sigs/kubespray/issues/5464/\n3. kubernetes kubelet 证书自动 renew 1# kubelet配置 2--feature-gates=RotateKubeletServerCertificate=true 3--feature-gates=RotateKubeletClientCertificate=true 4# 1.8版本以上包含1.8都支持证书更换自动重载，以下版本只能手动重启服务 5--rotate-certificates 6 7 8# kube-controller-manager配置 9# 证书有效期为10年 10--experimental-cluster-signing-duration=87600h0m0s 11--feature-gates=RotateKubeletServerCertificate=true 创建自动批准相关 CSR 请求的 ClusterRole:\n1kind: ClusterRole 2apiVersion: rbac.authorization.k8s.io/v1 3metadata: 4 name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver 5rules: 6- apiGroups: [\u0026#34;certificates.k8s.io\u0026#34;] 7 resources: [\u0026#34;certificatesigningrequests/selfnodeserver\u0026#34;] 8 verbs: [\u0026#34;create\u0026#34;] 自动批准 kubelet-bootstrap 用户 TLS bootstrapping 首次申请证书的 CSR 请求\n1kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --user=kubelet-bootstrap 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求\n1kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求\n1kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes 参考链接：\nhttps://www.leiyawu.com/2020/10/11/Untitled/\n","link":"https://zhangsiming-blyq.github.io/post/kubernetes-certificate/","section":"post","tags":["kubernetes"],"title":"谈谈kubernetes 证书认证那些事儿"},{"body":" ingress在将流量发往后端的时候是不经过kube-proxy的，ingress controller会直接和kube-apiserver进行交互，然后获取pod endpoints和service的对应关系，进行轮询，负载均衡到后端节点。\n一、验证试验 从集群中删除kube-proxy，查看通过ingress的方式是否可以访问成功?\n1# 实验中选择的是\u0026#34;iptables模式的kube-proxy\u0026#34; 2# 首先关闭kube-proxy 3$ sudo systemctl stop kube-proxy 4 5# 查看服务，ClusterIP的端口是8080，NodePort的端口是30948 6$ ks 7NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 8example-test NodePort 10.0.0.226 \u0026lt;none\u0026gt; 8080:30948/TCP 18h 9traefik NodePort 10.0.0.85 \u0026lt;none\u0026gt; 9000:30001/TCP,80:30002/TCP,443:31990/TCP 19h 10 11# 查看与\u0026#34;10.0.0.226\u0026#34;有关的iptables条目，得知访问\u0026#34;10.0.0.226\u0026#34;且目的端口是\u0026#34;8080\u0026#34;的流量会发送给KUBE-SVC-KNYNFDNL67C7KAZZ链 12$ sudo iptables -S -t nat | grep 10.0.0.226 13-A KUBE-SERVICES ! -s 10.0.0.0/24 -d 10.0.0.226/32 -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080 cluster IP\u0026#34; -m tcp --dport 8080 -j KUBE-MARK-MASQ 14-A KUBE-SERVICES -d 10.0.0.226/32 -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080 cluster IP\u0026#34; -m tcp --dport 8080 -j KUBE-SVC-KNYNFDNL67C7KAZZ 15 16# 查看KUBE-SVC-KNYNFDNL67C7KAZZ链发现，采用的是\u0026#34;--probability\u0026#34;策略，进行后端两个pod的负载均衡 17$ sudo iptables -S -t nat | grep KUBE-SVC-KNYNFDNL67C7KAZZ 18-N KUBE-SVC-KNYNFDNL67C7KAZZ 19-A KUBE-NODEPORTS -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080\u0026#34; -m tcp --dport 30948 -j KUBE-SVC-KNYNFDNL67C7KAZZ 20-A KUBE-SERVICES -d 10.0.0.226/32 -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080 cluster IP\u0026#34; -m tcp --dport 8080 -j KUBE-SVC-KNYNFDNL67C7KAZZ 21-A KUBE-SVC-KNYNFDNL67C7KAZZ -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080\u0026#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-O6MCO5C4JB4A4VEJ 22-A KUBE-SVC-KNYNFDNL67C7KAZZ -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080\u0026#34; -j KUBE-SEP-KREF3VGT6NHFYTVH 23 24# 我们再看后面转发的\u0026#34;KUBE-SEP-KREF3VGT6NHFYTVH\u0026#34;, 得知这一步就发送给了真实的pod 25$ sudo iptables -t nat -L KUBE-SEP-KREF3VGT6NHFYTVH --line-numbers 26Chain KUBE-SEP-KREF3VGT6NHFYTVH (1 references) 27num target prot opt source destination 281 KUBE-MARK-MASQ all -- 10.244.2.4 anywhere /* traffic-dispatcher/example-test:port-8080 */ 292 DNAT tcp -- anywhere anywhere /* traffic-dispatcher/example-test:port-8080 */ tcp to:10.244.2.4:8080 30 31# 现在我们要模拟不能走kube-proxy添加的规则访问到服务，于是我们删除刚刚\u0026#34;50%/50%\u0026#34;规则进行负载均衡转发的那两条规则(kube-proxy本质就是负载均衡) 32$ sudo iptables -t nat -L KUBE-SVC-KNYNFDNL67C7KAZZ --line-numbers 33Chain KUBE-SVC-KNYNFDNL67C7KAZZ (2 references) 34num target prot opt source destination 351 KUBE-SEP-O6MCO5C4JB4A4VEJ all -- anywhere anywhere /* traffic-dispatcher/example-test:port-8080 */ statistic mode random probability 0.50000000000 362 KUBE-SEP-KREF3VGT6NHFYTVH all -- anywhere anywhere /* traffic-dispatcher/example-test:port-8080 */ 37 38# 删除规则 39$ sudo iptables -t nat -D KUBE-SVC-KNYNFDNL67C7KAZZ 1 2 现在以ClusterIP+Port的方式访问服务应该走不通了，测试访问:\n1$ ks 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3example-test NodePort 10.0.0.226 \u0026lt;none\u0026gt; 8080:30948/TCP 18h 4traefik NodePort 10.0.0.85 \u0026lt;none\u0026gt; 9000:30001/TCP,80:30002/TCP,443:31990/TCP 19h 5$ curl 10.0.0.226:8080 6访问失败 7$ curl 10.20.13.90:30948 8访问失败 9$ curl 10.20.13.90:30002 -H \u0026#39;Host:testfile.shannonai.com\u0026#39; 10访问成功, 由此证明ingress是不走kube-proxy的! 二、结论 kube-proxy的工作是附加一些转发规则，所以说单单的\u0026quot;systemctl stop kube-proxy\u0026quot;是不会影响已经存在的规则的，也就是原来可以走通的服务只要没有变化，就依旧可以走通；只是我们再添加新的服务，就不能走通了(因为没有kube-proxy为我们添加这些规则了); 另外多节点的集群，只要其他节点的kube-proxy是ok的，就已经可以从其他节点访问服务; 我们做了上述那么多破坏如果想要恢复的话，也只是需要重新\u0026quot;systemctl restart kube-proxy\u0026quot;就会自动把我们破坏掉的规则加回来\n如何理解我们集群的ClusterIP和他对应的端口？我们了解过kube-proxy的规则了之后就很明显的可以得出结论：\n1）pod的ip是pod所处的networkns的真实ip，通过veth和网桥和我们宿主机的真实ip通信\n2）clusterip仅仅为kube-proxy的iptables或者ipvs规则匹配所用，也就是并不会出现在本机进行路由\n1# 本机的路由表只能看到真实的节点ip段和网络插件附加的pod ip段, 没有clusterip的地址段 2$ route -n 3内核 IP 路由表 4目标 网关 子网掩码 标志 跃点 引用 使用 接口 50.0.0.0 10.20.13.1 0.0.0.0 UG 100 0 0 ens5 610.20.13.0 0.0.0.0 255.255.255.0 U 0 0 0 ens5 710.20.13.1 0.0.0.0 255.255.255.255 UH 100 0 0 ens5 810.244.1.0 10.244.1.0 255.255.255.0 UG 0 0 0 flannel.1 910.244.2.0 10.244.2.0 255.255.255.0 UG 0 0 0 flannel.1 10172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 11 12# iptables和ipvs的规则中可以看到clusterip的地址段 13$ sudo iptables -S -t nat | grep 10.0.0.226 14-A KUBE-SERVICES ! -s 10.0.0.0/24 -d 10.0.0.226/32 -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080 cluster IP\u0026#34; -m tcp --dport 8080 -j KUBE-MARK-MASQ 15-A KUBE-SERVICES -d 10.0.0.226/32 -p tcp -m comment --comment \u0026#34;traffic-dispatcher/example-test:port-8080 cluster IP\u0026#34; -m tcp --dport 8080 -j KUBE-SVC-KNYNFDNL67C7KAZZ 由此得出，pod的targetport在对于不同的pod的情况下是可以重复的；nodeport如果是处于同一个节点是不能重复的；clusterip在clusterip的地址不同(也就是iptables和ipvs的规则中不冲突/或者简单理解为不同服务)的情况下是可以重复的\n参考链接: https://xuxinkun.github.io/2016/07/22/kubernetes-proxy/\n","link":"https://zhangsiming-blyq.github.io/post/ingress-mechanism/","section":"post","tags":["kubernetes"],"title":"浅谈kubernetes ingress机制"},{"body":"","link":"https://zhangsiming-blyq.github.io/archives/","section":"post","tags":null,"title":"文章归档"},{"body":"题目要求 一个栈中元素的类型为整型，现在想将该栈从顶到底按从小到大的顺序排序，只允许申请一个栈。除此之外，可以申请新的变量，但不能申请额外的数据结构。如何完成排序？\n解题思路 申请一个新的help栈，不断从原有栈中获取数据, 去跟新的help栈的栈顶数据相比; 如果符合排序要求，就push到help栈中 如果不符合排序要求，就从help栈中pop出栈顶数据，push到原栈中，直至符合排序要求 最终原栈被清空，help栈是全部排好序的栈，重新一股脑写回来即完成栈排序 golang实现 1package main 2 3// SortStark stark top --\u0026gt; stark bottom(from small to big) 4func SortStark(stk *Stack) { 5\thelpStack := NewStack() 6\t// 1. while go through target stack until stack is empty 7\tfor stk.Len() \u0026gt; 0 { 8\tcur := stk.Pop() 9\t// 2. if cur \u0026lt; peek, push helpStack.pop() to target stack until cur \u0026gt; peek or helpStack is empty 10\tfor (helpStack.Len() \u0026gt; 0) \u0026amp;\u0026amp; (cur.(int) \u0026lt; helpStack.Peek().(int)) { 11\tstk.Push(helpStack.Pop()) 12\t} 13\t// 3. if helkStack is not empty, compare cur with helpStack\u0026#39;s peek value 14\t// 4. while cur \u0026gt; peek, push cur to helpStack 15\thelpStack.Push(cur) 16\t} 17\t// 5. push back helpStack to target stack 18\tfor helpStack.Len() \u0026gt; 0 { 19\tstk.Push(helpStack.Pop()) 20\t} 21} 测试用例 1package main 2 3import ( 4\t\u0026#34;testing\u0026#34; 5 6\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 7) 8 9func TestSortStark(t *testing.T) { 10\ttests := []struct { 11\t// 名字 12\tname string 13 14\t// 输入部分 15\tval *Stack 16 17\t// 输出部分 18\twantRes []int 19\t}{ 20\t{ 21\tname: \u0026#34;sortstark\u0026#34;, 22\tval: NewStack(), 23\t// transpose once, then return to the original order of last-in first-out 24\twantRes: []int{0, 1, 2, 3, 5, 8}, 25\t}, 26\t} 27\tfor _, tt := range tests { 28\tt.Run(tt.name, func(t *testing.T) { 29\t// test case 30\tstk := tt.val 31\tstk.Push(2) 32\tstk.Push(8) 33\tstk.Push(1) 34\tstk.Push(5) 35\tstk.Push(0) 36\tstk.Push(3) 37\tSortStark(stk) 38\tres := []int{} 39\tfor stk.length \u0026gt; 0 { 40\tres = append(res, tt.val.Pop().(int)) 41\t} 42\tassert.Equal(t, tt.wantRes, res) 43\t}) 44\t} 45} 测试结果\n1$ go test -run ^TestSortStark$ . -v 2=== RUN TestSortStark 3=== RUN TestSortStark/sortstark 4--- PASS: TestSortStark (0.00s) 5 --- PASS: TestSortStark/sortstark (0.00s) 6PASS 7 8Process finished with the exit code 0 测试没有问题，成功完成排序\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/5/","section":"post","tags":["algorithm"],"title":"【算法系列】用一个栈实现另一个栈的排序"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/apigateway/","section":"tags","tags":null,"title":"apigateway"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/apigateway/","section":"categories","tags":null,"title":"apigateway"},{"body":" kong、apisix是当前比较火的两款开源api网关，本文对比了二者的部署、使用方式；提供一个简单的参考; 对于kong，大家都比较熟悉，但是对于apisix可能熟悉的并不多，那么kong、apisix在使用方式，功能命名上是否有相似，还是理念不同，请看下文。\n一、kong 1.1 安装 1# 安装kong 2$ helm repo add kong https://charts.konghq.com 3$ helm repo update 4$ helm fetch kong/kong 5$ tar xf kong-2.5.0.tgz 6$ cd kong 7$ ls 8CHANGELOG.md Chart.yaml FAQs.md README.md UPGRADE.md charts ci crds example-values requirements.lock requirements.yaml templates values.yaml 9...需要配置 101. postgresql作为存储 112. 允许plain text调用admin API 12 13# 安装konga 14$ gc https://github.com/pantsel/konga.git 15$ ls konga 16Chart.yaml templates values.yaml 17...需要配置 181. 获取postgresql的secret写入连接信息 19 20# 部署 21$ helm install kong . 22$ helm install konga . 23$ kp 24NAME READY STATUS RESTARTS AGE 25kong-kong-85d4dfd88b-hjkwt 2/2 Running 2 111s 26kong-postgresql-0 1/1 Running 0 15m 27konga-5b8c899c9-9zbd6 1/1 Running 0 14m 28vagrant@node1:~/kong/kong$ ks 29NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 30kong-kong-admin NodePort 172.30.46.68 \u0026lt;none\u0026gt; 8001:31055/TCP 15m 31kong-postgresql ClusterIP 172.30.194.189 \u0026lt;none\u0026gt; 5432/TCP 15m 32kong-postgresql-headless ClusterIP None \u0026lt;none\u0026gt; 5432/TCP 15m 33konga NodePort 172.30.111.78 \u0026lt;none\u0026gt; 80:32001/TCP 14m 访问konga UI：localhost:32001\n配置kong admin连接地址：http://kong-kong-admin.kong.svc.cluster.local:8001/\n1.2 配置通过kong访问服务 配置service；service在kong中表示实际要访问的服务，这里配置协议、域名、端口、路径、重试等\nroute必须在service创建，这里创建一个多path路由，并指定规定代理的Host\n1# 测试访问 2$ curl -XGET http://172.16.166.149:8000/api/test/ -H \u0026#34;Host: www.kongtest.com\u0026#34; -H \u0026#34;name: siming\u0026#34; -I 3HTTP/1.1 200 OK 4Content-Type: text/html; charset=UTF-8 5Content-Length: 2381 6Connection: keep-alive 7Accept-Ranges: bytes 8Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform 9Date: Fri, 29 Oct 2021 03:05:59 GMT 10Etag: \u0026#34;588604c8-94d\u0026#34; 11Last-Modified: Mon, 23 Jan 2017 13:27:36 GMT 12Pragma: no-cache 13Server: bfe/1.0.8.18 14Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/ 15X-Kong-Upstream-Latency: 31 16X-Kong-Proxy-Latency: 109 17Via: kong/2.6.0 1.3 consumers and plugin consumers抽象表示一组相同的请求，plugin可以实现认证、限流等多种控制功能，这里展示一个jwt认证插件\n在route中的plugin里面enable jwt认证，配置获取jwt的token方式为从uri param \u0026quot;jwt\u0026quot;中获取，并且jwt的校验key为client_id jwt可以再jwt官网(https://jwt.io/)生成，填写对应的加密方式和PAYLOAD(之前route里面约定的key) 然后把上面的信息填入consumers的jwt plugin中(key的名字，加密算法，公钥)并保存 测试访问 1$ curl -XGET http://172.16.166.149:8000/api/test/?username=zhangsiming -H \u0026#34;Host: www.kongtest.com\u0026#34; -H \u0026#34;name: siming\u0026#34; 2{\u0026#34;message\u0026#34;:\u0026#34;Unauthorized\u0026#34;} 3 4$ curl -XGET http://172.16.166.149:8000/api/test/?jwt=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbGllbnRfaWQiOiJzaW1pbmcifQ.OciIJI2HRNa6CteoCn3D87q7pjJZ3u7vrXp0TaKWTuCgwyUJfCoC2c1RSKTz0Eg2GjOrP-u74hVMhBPZzCK1T9ChEOlFqmmS-CnKmh_jlC8RPeGJ2AhJDk7yOos176xgu11jt14nFVFAKzaTaKI4YkmXJ7eTx7TB3WYG0HpNDAgIl6q3UluHERMO-5DT4n3-ev5xHCe-H6InHmGzKkR2t02_lUbbR7EDz2M_YDWJu8enXgBeyHKIoE7ewE0rO66yIm-3UHqHfUJd4BQ5ii73xd8IuhcAgFgTuZ6ffXotxAHuBdoPCEN-qRxcI_dhEXxmiKCNg1QPX1FBpRcbG9uOaw -H \u0026#34;Host: www.kongtest.com\u0026#34; -H \u0026#34;name: siming\u0026#34; -I 5HTTP/1.1 200 OK 6Content-Type: text/html; charset=UTF-8 7Content-Length: 2381 8Connection: keep-alive 9Accept-Ranges: bytes 10Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform 11Date: Fri, 29 Oct 2021 06:51:27 GMT 12Etag: \u0026#34;588604c8-94d\u0026#34; 13Last-Modified: Mon, 23 Jan 2017 13:27:36 GMT 14Pragma: no-cache 15Server: bfe/1.0.8.18 16Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/ 17X-Kong-Upstream-Latency: 39 18X-Kong-Proxy-Latency: 1 19Via: kong/2.6.0 二、apisix 2.1 安装 区别于kong、apisix官方自带web UI\n1$ helm repo add apisix https://charts.apiseven.com 2$ helm repo update 3$ helm fetch apisix/apisix 4$ helm fetch apisix/apisix-dashboard 5 6$ kp 7NAME READY STATUS RESTARTS AGE 8apisix-5d5665c8f-sbjhr 1/1 Running 0 17m 9apisix-dashboard-59fb575657-zt26r 1/1 Running 0 6m 10apisix-etcd-0 0/1 Running 0 13s 11apisix-etcd-1 1/1 Running 0 93s 12apisix-etcd-2 1/1 Running 0 2m55s 部署好了访问dashboard，默认账号密码是admin/admin\n2.2 配置通过apisix访问服务 这里与kong的概念稍有不同：\nupstream：类似kong的sevice，表示由apisix代为调用的上游服务，支持配置负载均衡权重 route：和kong概念相同，可以配置访问apisix的具体路径，方式等 service：route的配置模板，在service中配置了可以直接在route中复用(不用也行) 这里配置一个service，配置要以\u0026quot;www.testapisix.com\u0026quot;域名访问apisix才认为路由匹配，且上游服务为test\n这里配置上游服务为访问www.baidu.com，负载均衡机制为rr\n这里配置route：\n直接复用service配置，所以host部分不需要配置Host； 配置请求apisix的path，支持通配符 URI Override类似kong的strip host，代理的时候变更uri部分 最后还可以附加自定义的Header要求 测试访问\n1$ curl -XGET http://172.16.166.158:9080/api/test/ -H \u0026#34;Host: www.testapisix.com\u0026#34; -H \u0026#34;test: name\u0026#34; -I 2HTTP/1.1 200 OK 3Content-Type: text/html; charset=utf-8 4Content-Length: 2381 5Connection: keep-alive 6Accept-Ranges: bytes 7Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform 8Date: Mon, 01 Nov 2021 08:10:41 GMT 9Etag: \u0026#34;588604c8-94d\u0026#34; 10Last-Modified: Mon, 23 Jan 2017 13:27:36 GMT 11Pragma: no-cache 12Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/ 13Server: APISIX/2.10.0 1.3 consumers and plugin apisix支持自己生成jwt token，这里演示jwt token认证插件\nconsumer中添加plugin配置，编辑json协商\u0026quot;key\u0026quot;: \u0026quot;唯一的值\u0026quot;，然后公钥私钥，加密方式，之后保存\n在service(或者route)开启jwt plugin\n测试访问\n1$ curl -XGET http://172.16.166.158:9080/api/test/more/ -H \u0026#34;Host: www.testapisix.com\u0026#34; -H \u0026#34;test: name\u0026#34; -I 2HTTP/1.1 401 Unauthorized 3Date: Mon, 01 Nov 2021 08:10:17 GMT 4Content-Type: text/plain; charset=utf-8 5Transfer-Encoding: chunked 6Connection: keep-alive 7Server: APISIX/2.10.0 8 9# 获取apisix生成的jwt token 10$ curl -XGET http://172.16.166.158:9080/apisix/plugin/jwt/sign?key=zhangsiming 11eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1YyI6WyItLS0tLUJFR0lOIFBVQkxJQyBLRVktLS0tLVxuTUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUF1MVNVMUxmVkxQSENvek14SDJNb1xuNGxnT0VlUHpObTB0UmdlTGV6VjZmZkF0MGd1blZUTHc3b25MUm5ycTBcL0l6Vzd5V1I3UWtybUJMN2pUS0VuNXVcbitxS2hid0tmQnN0SXMrYk1ZMlprcDE4Z25UeEtMeG9TMnRGY3pHa1BMUGdpenNrdWVtTWdoUm5pV2FvTGN5ZWhcbmtkM3FxR0VsdldcL1ZETDVBYVdUZzBuTFZralJvOXorNDBSUXp1VmFFOEFrQUZteFp6b3czeCtWSllLZGp5a2tKXG4waVQ5d0NTMERSVFh1MjY5VjI2NFZmXC8zanZyZWRaaUtSa2d3bEw5eE5Bd3hYRmcweFwvWEZ3MDA1VVdWUklrZGdcbmNLV1RqcEJQMmRQd1ZaNFdXQys5YUdWZCtHeW4xbzBDTGVsZjRyRWpHb1hiQUFFZ0FxZUdVeHJjSWxialhmYmNcbm13SURBUUFCXG4tLS0tLUVORCBQVUJMSUMgS0VZLS0tLS1cbiJdfQ.eyJleHAiOjE2MzU4NDA2MjksImtleSI6InpoYW5nc2ltaW5nIn0.UErqUg229bwy5ZgYwEibtawT1CpwVP_UKIm-C7-XtMYUjhM6-lE695zFP31s7hp3SsS2PvoZtAEhCgd_fmZXx92EGGL87XYI0xbJ5uWweKettmxkc0cLFWwEL6MNOmqyoaW9gNDtd28K_M1dpzAwZdzz2GNr2e_G1UTrxlQUNorJEg9THIrkLjvrs7NAuvRuSnGd93G4tcXy1G6m0pCAg-Z4oehJS4vMpicmwedQbob0GytBM9Ef_r2gSj8IVVW8MMLWkA-TkPWuMx2nWeK1DdB4-l5I2f4Iu1It17rZqn6VX2ARnN_AFyG5ahT22pIGtdw71Od320hUUDH3I1rmtQ 12 13# 通过jwt token访问 14$ curl -XGET http://172.16.166.158:9080/api/test/?jwt=eyJ0eXAiOiJKV1QiLCJ4NWMiOlsiLS0tLS1CRUdJTiBQVUJMSUMgS0VZLS0tLS1cbk1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBdTFTVTFMZlZMUEhDb3pNeEgyTW9cbjRsZ09FZVB6Tm0wdFJnZUxlelY2ZmZBdDBndW5WVEx3N29uTFJucnEwXC9Jelc3eVdSN1Frcm1CTDdqVEtFbjV1XG4rcUtoYndLZkJzdElzK2JNWTJaa3AxOGduVHhLTHhvUzJ0RmN6R2tQTFBnaXpza3VlbU1naFJuaVdhb0xjeWVoXG5rZDNxcUdFbHZXXC9WREw1QWFXVGcwbkxWa2pSbzl6KzQwUlF6dVZhRThBa0FGbXhaem93M3grVkpZS2RqeWtrSlxuMGlUOXdDUzBEUlRYdTI2OVYyNjRWZlwvM2p2cmVkWmlLUmtnd2xMOXhOQXd4WEZnMHhcL1hGdzAwNVVXVlJJa2RnXG5jS1dUanBCUDJkUHdWWjRXV0MrOWFHVmQrR3luMW8wQ0xlbGY0ckVqR29YYkFBRWdBcWVHVXhyY0lsYmpYZmJjXG5td0lEQVFBQlxuLS0tLS1FTkQgUFVCTElDIEtFWS0tLS0tXG4iXSwiYWxnIjoiUlMyNTYifQ.eyJleHAiOjE2MzU4MzkwNjcsImtleSI6InpoYW5nc2ltaW5nIn0.UR6UnkdAuELK_YP3Kk6V4D4CxoPzTSw5lAx-64As_p68tyUQsp7cuR0MvCLEZ1LtSpF5VlJ1-4fUreAbNAzJQs_FBgDvkUcm4SkdqO_Ss4b0xDiXbF771oJeVybKQA-3fDd_4ieEjCyfsFkg1urgzc_tj96NBiW0YOV98RNJzf9adYZI2MLU_QbEqSEH-f9m0ArTlFLEBnVDOQls3JSc6dWobVbkZZ1kE12YeEq0zCdjEFoUEqy3f6rojobgBFmzvG7xQqn4Jd0o3d5iXBcGbMNn19X_Jo5z47zPI8tCN9ZfHWPtc8ts3HYx_2DmBPZAlEeY3Gs2izPdCHt38evEoA -H \u0026#34;Host: www.testapisix.com\u0026#34; -H \u0026#34;test: name\u0026#34; -I 15HTTP/1.1 200 OK 16Content-Type: text/html; charset=utf-8 17Content-Length: 2381 18Connection: keep-alive 19Accept-Ranges: bytes 20Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform 21Date: Mon, 01 Nov 2021 08:10:41 GMT 22Etag: \u0026#34;588604c8-94d\u0026#34; 23Last-Modified: Mon, 23 Jan 2017 13:27:36 GMT 24Pragma: no-cache 25Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/ 26Server: APISIX/2.10.0 注意，apisix支持的两种jwt访问方式：\nuri param：?jwt=xxxxx Header: -H \u0026quot;Authorization: xxxx\u0026quot; 三、kong 对比 apisix apisix配置更新生效时间0.2毫秒，事件通知；kong需要定期轮询5s左右 单核QPS(开启限流和prometheus插件)，apisix18000，kong1700 并且支持用户自定义负载均衡算法，自带维护dashboard，支持指定时间窗口的限速等 参考链接：\nGitHub - apache/apisix: The Cloud-Native API Gateway 插件 - 插件热加载 - 《Apache APISIX v1.4 使用教程》 - 书栈网 · BookStack\n","link":"https://zhangsiming-blyq.github.io/post/apigateway/","section":"post","tags":["linux","apigateway"],"title":"API网关对比"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/linux/","section":"tags","tags":null,"title":"linux"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/linux/","section":"categories","tags":null,"title":"linux"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/ansible/","section":"tags","tags":null,"title":"ansible"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/ansible/","section":"categories","tags":null,"title":"ansible"},{"body":" Ansible 是一个开源的基于 OpenSSH 的自动化配置管理工具。可以用它来配置系统、部署软件和编排更高级的 IT 任务，比如持续部署或零停机更新。\n一、ansible 简介 Ansible 的主要目标是简单和易用，并且它还高度关注安全性和可靠性。基于这样的目标，Ansible 适用于开发人员、系统管理员、发布工程师、IT 经理，以及介于两者之间的所有人。Ansible 适合管理几乎所有的环境，从拥有少数实例的小型环境到有数千个实例的企业环境。\n1.1 ansible 变量优先级如下 command line values (eg \u0026quot;-u user\u0026quot;) role defaults inventory file or script group vars inventory group_vars/all playbook group_vars/all inventory group_vars/* playbook group_vars/* inventory file or script host vars inventory host_vars/*: inventory 下面的 hosts_vars 目录下的变量优先级大于 group_vars 目录下的 playbook host_vars/* host facts / cached set_facts play vars play vars_prompt play vars_files: vars_files 优先级大于同级别的 vars 字段(play 内定义) role vars (defined in role/vars/main.yml): role 里面的 vars 目录下的定义变量 block vars (only for tasks in block) task vars (only for the task): task 里面的 vars 字段优先级别比较高(本 task) include_vars set_facts / registered vars: 比较常用 set_facts 更改一些后面要使用的变量，全局生效；registered vars 一般连续的 task 用的比较多 role (and include_role) params include params extra vars (always win precedence): 执行 ansible-playbook 命令时候传入的 extra vars 级别最高，高于一切 1.2 变量的作用范围(Scoping variables) Global: this is set by config, environment variables and the command line Play: each play and contained structures, vars entries (vars; vars_files; vars_prompt), role defaults and vars. Host: variables directly associated to a host, like inventory, include_vars, facts or registered task outputs Ansible 1.2 及以上的版本中,group_vars/ 和 host_vars/ 目錄可放在 inventory 目錄下,或是 playbook 目錄下. 如果兩個目錄下都存在,那麼 playbook 目錄下的配置會覆蓋 inventory 目錄的配置(对应解释上分 4-10 条变量优先级)\n1.3 ansible 目录结构最佳实践 结构如下： 1├── playbooks.yml 2├── inventory(inventory下可以任意加子目录进行分组) 3| ├── group_vars ├── all ├── *.yml 4| | ├── *.yml 5| | └── * 6| ├── host_vars 7| | 8│ └── hosts.ini 9| 10├── roles 11│ ├── common 12│ │ ├── files 13│ │ ├── handlers 14│ │ ├── meta 15│ │ ├── templates 16│ │ ├── tasks 17│ │ │ └── main.yml 18│ │ └── vars 19| | 20│ ├── others 21│ ├── files 22│ ├── handlers 23│ ├── meta 24│ ├── templates 25│ ├── tasks 26│ │ └── main.yml 27│ └── vars 28└── *.yml 使用 roles 管理不同的安装模块 使用 inventory/host.ini 管理操作主机，对不同主机进行分组 使用 group_vars 管理基本的默认变量，playbook 中使用 set_facts 设置需要的变量，默认不想被修改的变量写在 roles/vars/main.yml 每个 task 都要写 name tags 写在 role 阶段 1.4 ansible-playbook roles 初始化行为规划 如果角色 /xxx/tasks/main.yaml 存在，则其中列出的任务将添加到任务中，否则将不会添加任务中 如果角色 /xxx/handlers/main.yaml 存在，则其中列出的处理程序将添加到任务中，否则将不会添加任务中 如果角色 /xxx/vars/main.yml 存在，则其中列出的处理程序将添加到任务中，否则将不会添加任务中 如果角色 /xxx/defaults/main.yml 存在，则其中列出的处理程序将添加到任务中，否则将不会添加任务中 如果角色 /xxx/meta/main.yml 存在，则其中列出的任何角色依赖项将添加到角色列表（1.3 及更高版本） 任何副本，脚本，模板或包含任务（在角色中）都可以引用 roles / x / {files，templates，tasks} /（dir 取决于任务）中的文件，而无需相对或绝对地路径化它们 1.5 ansible-playbook 执行顺序 pre_tasks 游戏中定义的任何内容 列出的每个角色将依次执行。将首先运行角色中定义的任何角色依赖项，但需遵循标记过滤和条件。roles meta/main.yml tasks 游戏中定义的任何内容 post_tasks 游戏中定义的任何内容 二、kubespray 部署集群 playbook 解读 github 地址：https://github.com/kubernetes-sigs/kubespray\n1$ gc https://github.com/kubernetes-sigs/kubespray 2$ cd kubespray 3 4# 查看部署集群playbook 5$ cat cluster.yml 6--- 7# 检查ansible版本 8- name: Check ansible version 9 import_playbook: ansible_version.yml 10 11# 检查inventory中组的版本，进行适配旧版本的group名称 12- name: Ensure compatibility with old groups 13 import_playbook: legacy_groups.yml 14 15# 堡垒机配置(没有跳过) 16- hosts: bastion[0] 17 gather_facts: False 18 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 19 roles: 20 - { role: kubespray-defaults } 21 - { role: bastion-ssh-config, tags: [\u0026#34;localhost\u0026#34;, \u0026#34;bastion\u0026#34;] } 22 23# 默认linear,每个主机的单个task执行完成会等待其他都完成后再执行下个任务，设置free可不等待其他主机，继续往下执行(看起来会比较乱) 24# linear策略即线性执行策略，线性执行策略指主机组内所有主机完成一个任务后才继续下一个任务的执行，在执行一个任务时，如果某个主机先执行完则会等待其他主机执行结束。说直白点就是第一个任务在指定的主机都执行完，再进行第二个任务的执行，第二个任务在指定的主机都执行完后，再进行第三个任务的执行…… 以此类推。 25# free策略即自由策略，即在一个play执行完之前，每个主机都各顾各的尽可能快的完成play里的所有任务，而不会因为其他主机没执行完任务而等待，不受线性执行策略那样的约束。所以这种策略的执行结果给人感觉是无序的甚至是杂乱无章的，而且每次执行结果的task显示顺序很可能不一样 26 27# 此阶段对于etcd的每一个节点一个一个的进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact), bootstrap-os(装一些yum源，基础包，改一些主机名等操作) 28- hosts: k8s_cluster:etcd 29 strategy: linear 30 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 31 gather_facts: false 32 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 33 roles: 34 - { role: kubespray-defaults } 35 - { role: bootstrap-os, tags: bootstrap-os} 36 37- name: Gather facts 38 tags: always 39 import_playbook: facts.yml 40 41# 此阶段对于etcd的每一个节点一个一个的进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact)、kubernetes/preinstall(预先配置一些环境，比如禁用SWAP、配置替换resolvconf，设置一些cni的bin路径等、创建/检查一些安装目录、期间对于特定文件触发handler重启相关服务、配置systemd-resolved、修改/etc/hosts文件等)、container-engine(选择容器runtime，进行安装、配置、reload等)、 42# download(模块用于下载所有有需要的镜像，先在ansible执行机缓存，之后再下载所有需要的二进制包，包括kubeadm需要的、kubernetes集群需要的等等) 43- hosts: k8s_cluster:etcd 44 gather_facts: False 45 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 46 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 47 roles: 48 - { role: kubespray-defaults } 49 - { role: kubernetes/preinstall, tags: preinstall } 50 - { role: \u0026#34;container-engine\u0026#34;, tags: \u0026#34;container-engine\u0026#34;, when: deploy_container_engine|default(true) } 51 - { role: download, tags: download, when: \u0026#34;not skip_downloads\u0026#34; } 52 53# 此阶段开始执行etcd role，先进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact)、再进行etcd(检查、创建etcd证书，部署etcd集群，添加etcd节点，检查etcd状态，创建etcd执行用户等...) 54- hosts: etcd 55 gather_facts: False 56 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 57 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 58 roles: 59 - { role: kubespray-defaults } 60 - role: etcd 61 tags: etcd 62 vars: 63 etcd_cluster_setup: true 64 etcd_events_cluster_setup: \u0026#34;{{ etcd_events_cluster_enabled }}\u0026#34; 65 when: not etcd_kubeadm_enabled| default(false) 66 67# 设置etcd_cluster_setup、etcd_events_cluster_setup为 false，主要是将k8s cluster中的机器，分发配置信息etcd的证书, 用于后续与etcd集群交互 68- hosts: k8s_cluster 69 gather_facts: False 70 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 71 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 72 roles: 73 - { role: kubespray-defaults } 74 - role: etcd 75 tags: etcd 76 vars: 77 etcd_cluster_setup: false 78 etcd_events_cluster_setup: false 79 when: not etcd_kubeadm_enabled| default(false) 80 81# 此阶段执行kubernetes/nodes role，先进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact), 再进行kubernetes/node(检查docker cgroup, 配置一些参数等、拷贝一些数据，比如cni目录下的，等等、通过镜像挂载的方式配置安装kubelet、部署apiserver的负载均衡、确保预留nodePort端口范围，通过sysctl修改内核参数、确认kube-proxy ipvs需要的内核模块已开启) 82- hosts: k8s_cluster 83 gather_facts: False 84 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 85 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 86 roles: 87 - { role: kubespray-defaults } 88 - { role: kubernetes/node, tags: node } 89 90# 此阶段部署配置kube_control_plane机器，先进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact)、 91# kubernetes/control-plane(删除control-plane静态文件，删除旧的master容器、下载好kubectl命令行工具、配置kubectl命令行工具自动补全、在kubeadm-setup.yml初始化Initialize first master，创建kubeadm的24h token，然后include_tasks: kubeadm-secondary.yml添加其他的master到集群中、检查etcd证书过期时间等后续检查、更新apiserver客户端的连接地址为fix后的api负载均衡地址、renew集群的证书们, 使用kubeadm renew的方式) 92# kubernetes/client(建立/etc/kubernetes架构目录，拷贝kubeconfig到node并配置好ansible执行机上面的k8s client环境) 93# kubernetes-apps/cluster_roles(通过配置好的kubectl执行，Apply workaround to allow all nodes with cert O=system:nodes to register, 这样之后的所有node加入申请都会自动审批通过...) 94- hosts: kube_control_plane 95 gather_facts: False 96 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 97 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 98 roles: 99 - { role: kubespray-defaults } 100 - { role: kubernetes/control-plane, tags: master } 101 - { role: kubernetes/client, tags: client } 102 - { role: kubernetes-apps/cluster_roles, tags: cluster-roles } 103 104# 此阶段部署配置cluster里面的其他普通node节点，先进行kubespray-defaults(设置一些fallback_ip、noproxy的set_fact)、 105# 之后kubernetes/kubeadm(检查kubelet配置文件是否存在、检查kubelet的ca.crt证书是否存在、创建kubeadm join的token、更新kubelet中的地址为负载均衡的apiserver地址、重启kube-proxy的pods)、 106# kubernetes/node-label(根据变量中的{{ role_node_labels + inventory_node_labels }}给node打上对应的标签，通过kubectl label nodes) 107# network_plugin(根据变量决定要部署哪种网络插件，举例cilium的话就是check.yml检查cilium参数，install.yml渲染安装cilium需要的yaml文件到node、apply.yml执行kubectl部署网络插件到集群中) 108- hosts: k8s_cluster 109 gather_facts: False 110 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 111 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 112 roles: 113 - { role: kubespray-defaults } 114 - { role: kubernetes/kubeadm, tags: kubeadm} 115 - { role: kubernetes/node-label, tags: node-label } 116 - { role: network_plugin, tags: network } 117 118# 如果网络插件部署的是calico，还需要到宿主机上面执行一些操作，这里我们选择cilium就跳过了 119- hosts: calico_rr 120 gather_facts: False 121 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 122 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 123 roles: 124 - { role: kubespray-defaults } 125 - { role: network_plugin/calico/rr, tags: [\u0026#39;network\u0026#39;, \u0026#39;calico_rr\u0026#39;] } 126 127# windows master的额外配置，这里跳过, 一般不用 128- hosts: kube_control_plane[0] 129 gather_facts: False 130 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 131 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 132 roles: 133 - { role: kubespray-defaults } 134 - { role: win_nodes/kubernetes_patch, tags: [\u0026#34;master\u0026#34;, \u0026#34;win_nodes\u0026#34;] } 135 136# 这阶段就是在kube_control_plane也就是有权限kubectl的机器上面安装后续应用了，不是每个role都需要走，比如是否部署ingress_controller就由\u0026#34;ingress_nginx_enabled\u0026#34;管理，kubernetes-apps下有个meta/main.yml文件，根据参数选择安装不同的app，流程无非就是下载包，或者传输yaml文件之后kubectl apply 137- hosts: kube_control_plane 138 gather_facts: False 139 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 140 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 141 roles: 142 - { role: kubespray-defaults } 143 - { role: kubernetes-apps/external_cloud_controller, tags: external-cloud-controller } 144 - { role: kubernetes-apps/network_plugin, tags: network } 145 - { role: kubernetes-apps/policy_controller, tags: policy-controller } 146 - { role: kubernetes-apps/ingress_controller, tags: ingress-controller } 147 - { role: kubernetes-apps/external_provisioner, tags: external-provisioner } 148 - { role: kubernetes-apps, tags: apps } 149 150# 这阶段是最后收尾阶段完善集群的dns，host文件，resolv文件里面的格式化对应项等, 至此kubernetes集群通过ansible kubespray安装完毕 151- hosts: k8s_cluster 152 gather_facts: False 153 any_errors_fatal: \u0026#34;{{ any_errors_fatal | default(true) }}\u0026#34; 154 environment: \u0026#34;{{ proxy_disable_env }}\u0026#34; 155 roles: 156 - { role: kubespray-defaults } 157 - { role: kubernetes/preinstall, when: \u0026#34;dns_mode != \u0026#39;none\u0026#39; and resolvconf_mode == \u0026#39;host_resolvconf\u0026#39;\u0026#34;, tags: resolvconf, dns_late: true } 1.2 实际操作使用 ansible-playbook 部署单节点集群 部署参考文档：https://kubespray.io/#/、https://kubespray.io/#/docs/downloads\n1$ sudo apt-get install python3-pip 2$ cd kubespray 3$ sudo pip3 install -r requirements.txt 4# 看下面两个文件有没有要删除的东西 5$ vim inventory/mycluster/group_vars/all/all.yml 6$ vim inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml 7# download缓存相关配置可以查看链接：https://kubespray.io/#/docs/downloads 8# 配置免密sudo，配置免密ssh节点 9$ vim /etc/sudoers 10vagrant ALL=NOPASSWD:ALL 11 12# 开始部署集群 13$ ansible-playbook -i inventory/k8sdemo/inventory.ini --become --become-user=root cluster.yml 14# 部署结果无报错 15... 16PLAY RECAP ******************************************************************************************************************************************************* 17localhost : ok=4 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 18node1 : ok=650 changed=177 unreachable=0 failed=0 skipped=1031 rescued=0 ignored=2 19... 20# 查看集群状态 21$ kubectl get nodes 22NAME STATUS ROLES AGE VERSION 23node1 Ready control-plane,master 3m49s v1.22.2 24$ kubectl get cs 25Warning: v1 ComponentStatus is deprecated in v1.19+ 26NAME STATUS MESSAGE ERROR 27scheduler Healthy ok 28controller-manager Healthy ok 29etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} 30$ kubectl get pods -A 31NAMESPACE NAME READY STATUS RESTARTS AGE 32kube-system cilium-dtgnn 1/1 Running 0 3m16s 33kube-system cilium-operator-66576fd87-bklpc 1/1 Running 0 3m15s 34kube-system coredns-8474476ff8-85qkx 1/1 Running 0 3m9s 35kube-system coredns-8474476ff8-xtmst 0/1 Pending 0 2m45s 36kube-system dns-autoscaler-7df78bfcfb-gsqjb 1/1 Running 0 3m5s 37kube-system kube-apiserver-node1 1/1 Running 0 4m 38kube-system kube-controller-manager-node1 1/1 Running 1 4m 39kube-system kube-proxy-h2lgj 1/1 Running 0 3m16s 40kube-system kube-scheduler-node1 1/1 Running 1 4m 41kube-system nodelocaldns-xnvv7 1/1 Running 0 3m4s 参考链接：\nansible 变量详解\nansible 变量优先级官方原文\n","link":"https://zhangsiming-blyq.github.io/post/ansible-playbook/","section":"post","tags":["ansible","linux"],"title":"ansible-playbook 详解"},{"body":"题目要求 狗和猫都实现了Pet接口，可以用getPetType()查看对应的动物类型; NewDog()、NewCat()可以分别新建狗猫对象。\n1type Pet interface { 2\tgetPetType() string 3} 4 5type fatherPet struct { 6\tType string 7} 8 9func (fp *fatherPet) getPetType() string { 10\treturn fp.Type 11} 12 13type Dog struct { 14\tfatherPet 15} 16 17func NewDog() *Dog { 18\tfmt.Println(\u0026#34;new dog\u0026#34;) 19\treturn \u0026amp;Dog{fatherPet{Type: \u0026#34;dog\u0026#34;}} 20} 21 22func (dog *Dog) getPetType() string { 23\treturn dog.fatherPet.Type 24} 25 26type Cat struct { 27\tfatherPet 28} 29 30func NewCat() *Cat { 31\tfmt.Println(\u0026#34;new cat\u0026#34;) 32\treturn \u0026amp;Cat{fatherPet{Type: \u0026#34;cat\u0026#34;}} 33} 34 35func (cat *Cat) getPetType() string { 36\treturn cat.fatherPet.Type 37} 实现一种狗猫队列的结构，要求如下：\n用户可以调用add方法将cat类或dog类的实例放入队列中； 用户可以调用pollAll方法，将队列中所有的实例按照队列的先后顺序依次弹出； 用户可以调用pollDog方法，将队列中dog类的实例按照进队列的先后顺序依次弹出； 用户可以调用pollCat方法，将队列中cat类的实例按照进队列的先后顺序依次弹出； 用户可以调用isEmpty方法，检查队列中是否还有dog或cat的实例； 用户可以调用isDogEmpty方法，检查队列中是否有dog类的实例； 用户可以调用isCatEmpty方法，检查队列中是否有cat类的实例。 解题思路 依照题意，需要确定队列中可能有dog也可能有cat，用两个队列分别保存dog和cat实例；至于如何判断dog队列的元素是更早入队列还是cat队列的元素是更早入队列需要有一个字段用来存储入队列time(全局唯一)，这里可以封装一个struct来保存pet和time；\n那么接下来如果想要看dog，就看dogqueue，如果想看cat就看catqueue；如果想看总队列的顺序，取dogqueue和catqueue二者最新元素的time进行比较得出在总队列的顺序。\ngolang实现 新创建一个准备放入队列的struct，包括Pet对象和Time字段：\n1type TimePet struct { 2\tPet Pet 3\tTime int 4} 5 6func NewTimePet(pet Pet, time int) *TimePet { 7\treturn \u0026amp;TimePet{ 8\tPet: pet, 9\tTime: time, 10\t} 11} 12 13func (tp *TimePet) getPet() Pet { 14\treturn tp.Pet 15} 16 17func (tp *TimePet) getTime() int { 18\treturn tp.Time 19} 那么dogcatqueue主体包括逻辑如下：\n1type DogCatQueue struct { 2\tDogQueue *customQueue 3\tCatQueue *customQueue 4\tTime int 5} 6 7func NewDogCatQueue() *DogCatQueue { 8\treturn \u0026amp;DogCatQueue{ 9\tDogQueue: newCustomQueue(), 10\tCatQueue: newCustomQueue(), 11\t// 全局唯一的Time, 用于标识总顺序 12\tTime: 0, 13\t} 14} 15 16// 添加的时候比较简单，狗进狗队列，猫进猫队列即可 17func (dcqueue *DogCatQueue) add(pet Pet) { 18\tif pet.getPetType() == \u0026#34;dog\u0026#34; { 19\tdcqueue.Time++ 20\tdcqueue.DogQueue.Enqueue(NewTimePet(pet, dcqueue.Time)) 21\t} else if pet.getPetType() == \u0026#34;cat\u0026#34; { 22\tdcqueue.Time++ 23\tdcqueue.CatQueue.Enqueue(NewTimePet(pet, dcqueue.Time)) 24\t} else { 25\tfmt.Errorf(\u0026#34;err, not dog or cat\u0026#34;) 26\t} 27} 28 29// pollAll的时候需要对比狗队列和猫队列最新元素的Time谁小，谁小表示谁先进的队列 30func (dcqueue *DogCatQueue) pollAll() Pet { 31\tif !dcqueue.isDogQueueEmpty() \u0026amp;\u0026amp; !dcqueue.isCatQueueEmpty() { 32\tif dcqueue.DogQueue.Front().(*TimePet).getTime() \u0026lt; dcqueue.CatQueue.Front().(*TimePet).getTime() { 33\treturn dcqueue.DogQueue.Dequeue().(*TimePet).getPet() 34\t} else { 35\treturn dcqueue.CatQueue.Dequeue().(*TimePet).getPet() 36\t} 37\t} else if !dcqueue.isDogQueueEmpty() { 38\treturn dcqueue.DogQueue.Dequeue().(*TimePet).getPet() 39\t} else if !dcqueue.isCatQueueEmpty() { 40\treturn dcqueue.CatQueue.Dequeue().(*TimePet).getPet() 41\t} else { 42\tfmt.Errorf(\u0026#34;err, dogcatqueue is empty\u0026#34;) 43\treturn nil 44\t} 45} 46 47// pollDog直接从狗队列删除即可 48func (dcqueue *DogCatQueue) pollDog() Pet { 49\tif !dcqueue.isDogQueueEmpty() { 50\tfmt.Println(\u0026#34;delete latest dog\u0026#34;) 51\treturn dcqueue.DogQueue.Dequeue().(*TimePet).getPet() 52\t} else { 53\tfmt.Errorf(\u0026#34;err, dogcatqueue don\u0026#39;t have dog\u0026#34;) 54\treturn nil 55\t} 56} 57 58// 同理 59func (dcqueue *DogCatQueue) pollCat() Pet { 60\tif !dcqueue.isCatQueueEmpty() { 61\tfmt.Println(\u0026#34;delete latest cat\u0026#34;) 62\treturn dcqueue.CatQueue.Dequeue().(*TimePet).getPet() 63\t} else { 64\tfmt.Errorf(\u0026#34;err, dogcatqueue don\u0026#39;t have cat\u0026#34;) 65\treturn nil 66\t} 67} 68 69func (dcqueue *DogCatQueue) isEmpty() bool { 70\treturn dcqueue.DogQueue.Empty() \u0026amp;\u0026amp; dcqueue.CatQueue.Empty() 71} 72 73func (dcqueue *DogCatQueue) isDogQueueEmpty() bool { 74\treturn dcqueue.DogQueue.Empty() 75} 76 77func (dcqueue *DogCatQueue) isCatQueueEmpty() bool { 78\treturn dcqueue.CatQueue.Empty() 79} 测试用例 1package main 2 3import ( 4\t\u0026#34;testing\u0026#34; 5 6\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 7) 8 9func TestDogCatQueue(t *testing.T) { 10\t// define a getmin stack 11\ttests := []struct { 12\t// 名字 13\tname string 14 15\t// 输入部分 16\tval func(tq *DogCatQueue) []bool 17 18\t// 输出部分 19\twantRes []bool 20\t}{ 21\t{ 22\tname: \u0026#34;operate1\u0026#34;, 23\tval: operate1, 24\twantRes: []bool{true, false, false}, 25\t}, 26\t{ 27\tname: \u0026#34;operate2\u0026#34;, 28\tval: operate2, 29\twantRes: []bool{true, false, false}, 30\t}, 31\t{ 32\tname: \u0026#34;operate3\u0026#34;, 33\tval: operate3, 34\twantRes: []bool{true, true, true}, 35\t}, 36\t{ 37\tname: \u0026#34;operate4\u0026#34;, 38\tval: operate4, 39\twantRes: []bool{true, true, true}, 40\t}, 41\t{ 42\tname: \u0026#34;operate5\u0026#34;, 43\tval: operate5, 44\twantRes: []bool{true, true, true}, 45\t}, 46\t} 47\tfor _, tt := range tests { 48\t// new TwoStackQueue 49\ttq := NewDogCatQueue() 50\t// add test cases 51\tadddogcat(tq) 52\tt.Run(tt.name, func(t *testing.T) { 53\tres := tt.val(tq) 54\tassert.Equal(t, tt.wantRes, res) 55\t}) 56\t} 57} 58 59func adddogcat(tq *DogCatQueue) { 60\ttq.add(NewDog()) 61\ttq.add(NewCat()) 62\ttq.add(NewCat()) 63\ttq.add(NewCat()) 64\ttq.add(NewDog()) 65} 66 67func operate1(tq *DogCatQueue) (queStatus []bool) { 68\ttq.pollAll() 69\ttq.pollAll() 70\ttq.pollCat() 71\ttq.pollCat() 72\tqueStatus = []bool{tq.isCatQueueEmpty(), tq.isDogQueueEmpty(), tq.isEmpty()} 73\treturn queStatus 74} 75 76func operate2(tq *DogCatQueue) (queStatus []bool) { 77\ttq.pollAll() 78\ttq.pollAll() 79\ttq.pollCat() 80\ttq.pollCat() 81\ttq.pollCat() 82\tqueStatus = []bool{tq.isCatQueueEmpty(), tq.isDogQueueEmpty(), tq.isEmpty()} 83\treturn queStatus 84} 85 86func operate3(tq *DogCatQueue) (queStatus []bool) { 87\ttq.pollAll() 88\ttq.pollAll() 89\ttq.pollCat() 90\ttq.pollCat() 91\ttq.pollDog() 92\tqueStatus = []bool{tq.isCatQueueEmpty(), tq.isDogQueueEmpty(), tq.isEmpty()} 93\treturn queStatus 94} 95 96func operate4(tq *DogCatQueue) (queStatus []bool) { 97\ttq.pollAll() 98\ttq.pollAll() 99\ttq.pollCat() 100\ttq.pollCat() 101\ttq.pollAll() 102\tqueStatus = []bool{tq.isCatQueueEmpty(), tq.isDogQueueEmpty(), tq.isEmpty()} 103\treturn queStatus 104} 105 106func operate5(tq *DogCatQueue) (queStatus []bool) { 107\ttq.pollCat() 108\ttq.pollCat() 109\ttq.pollCat() 110\ttq.pollDog() 111\ttq.pollDog() 112\tqueStatus = []bool{tq.isCatQueueEmpty(), tq.isDogQueueEmpty(), tq.isEmpty()} 113\treturn queStatus 114} 测试结果\n1$ go test -run ^TestDogCatQueue$ . -v 2=== RUN TestDogCatQueue 3=== RUN TestDogCatQueue/operate1 4=== RUN TestDogCatQueue/operate2 5=== RUN TestDogCatQueue/operate3 6=== RUN TestDogCatQueue/operate4 7=== RUN TestDogCatQueue/operate5 8--- PASS: TestDogCatQueue (0.00s) 9 --- PASS: TestDogCatQueue/operate1 (0.00s) 10 --- PASS: TestDogCatQueue/operate2 (0.00s) 11 --- PASS: TestDogCatQueue/operate3 (0.00s) 12 --- PASS: TestDogCatQueue/operate4 (0.00s) 13 --- PASS: TestDogCatQueue/operate5 (0.00s) 14PASS 15ok dogcatqueue 0.008s 篇幅不短，但是难度极低，注意题目要求不要修改原有数据结构，定义一个包括Time的新的结构体即可~\n补充: 使用golang链表实现queue 其实使用slice也可以实现，但是如果slice满了以后就会对底层数组进行一次拷贝, 使用链表则没有这个问题, 实现如下(接受interface{}作为队列元素)：\n1package main 2 3import ( 4\t\u0026#34;container/list\u0026#34; 5) 6 7type customQueue struct { 8\tqueue *list.List 9} 10 11func newCustomQueue() *customQueue { 12\treturn \u0026amp;customQueue{queue: list.New()} 13} 14 15func (c *customQueue) Enqueue(value interface{}) { 16\tc.queue.PushBack(value) 17} 18 19func (c *customQueue) Dequeue() interface{} { 20\tif c.queue.Len() \u0026gt; 0 { 21\tele := c.queue.Front() 22\tc.queue.Remove(ele) 23\treturn ele.Value 24\t} 25\treturn nil 26} 27 28func (c *customQueue) Front() interface{} { 29\tif c.queue.Len() \u0026gt; 0 { 30\treturn c.queue.Front().Value 31\t} 32\treturn nil 33} 34 35func (c *customQueue) Size() int { 36\treturn c.queue.Len() 37} 38 39func (c *customQueue) Empty() bool { 40\treturn c.queue.Len() == 0 41} 参考链接：\nhttps://www.delftstack.com/zh/howto/go/queue-implementation-in-golang/\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/4/","section":"post","tags":["algorithm"],"title":"【算法系列】猫狗队列"},{"body":"题目要求 一个栈依次压入1、2、3、4、5，那么从栈顶到栈底分别为5、4、3、2、1。将这个栈转置后，从栈顶到栈底为1、2、3、4、5，也就是实现栈元素的逆序，但是只能用递归函数来实现，不能用其他数据结构。\n解题思路 一个函数实现返回栈内数据，用于返回栈底元素，当栈空的时候停止 另一个函数接受栈底元素并将每一个的数据重新压入栈中，即可实现逆序 golang实现 1package main 2 3import \u0026#34;fmt\u0026#34; 4 5func getAndRemoveLastElement(stack *Stack) int { 6\tresult := stack.Pop() 7\tif stack.Len() == 0 { 8\treturn result.(int) 9\t} else { 10\tlast := getAndRemoveLastElement(stack) 11\tstack.Push(result) 12\treturn last 13\t} 14} 15 16func reverse(stack *Stack) { 17\tif stack.Len() == 0 { 18\treturn 19\t} else { 20\tgarFuncReturn := getAndRemoveLastElement(stack) 21\treverse(stack) 22\tstack.Push(garFuncReturn) 23\t} 24} 测试用例 1package main 2 3import ( 4\t\u0026#34;testing\u0026#34; 5 6\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 7) 8 9func TestReverseStack(t *testing.T) { 10\t// define a getmin stack 11\ttests := []struct { 12\t// 名字 13\tname string 14 15\t// 输入部分 16\tval func(stk *Stack) 17 18\t// 输出部分 19\twantRes []any 20\t}{ 21\t{ 22\tname: \u0026#34;push\u0026#34;, 23\tval: push, 24\t// transpose once, then return to the original order of last-in first-out 25\twantRes: []any{1, 2, 3, 4, 5}, 26\t}, 27\t} 28\tfor _, tt := range tests { 29\t// new TwoStackQueue 30\tstk := NewStack() 31\tt.Run(tt.name, func(t *testing.T) { 32\ttt.val(stk) 33\treverse(stk) 34\tres := popall(stk) 35\tassert.Equal(t, tt.wantRes, res) 36\t}) 37\t} 38} 39 40func push(stk *Stack) { 41\tfor i := 1; i \u0026lt;= 5; i++ { 42\tstk.Push(i) 43\t} 44} 45 46func popall(stk *Stack) (resultList []any) { 47\tfor { 48\tif stk.length == 0 { 49\tbreak 50\t} 51\tresultList = append(resultList, stk.Pop()) 52\t} 53\treturn resultList 54} 测试结果\n1$ go test -run ^TestReverseStack$ . -v 2=== RUN TestReverseStack 3=== RUN TestReverseStack/push 4--- PASS: TestReverseStack (0.00s) 5 --- PASS: TestReverseStack/push (0.00s) 6PASS 7ok starkreverse 0.008s 实现起来比较简单，重点是需要画调用栈图，将每一层变量是什么值梳理清楚，什么时候向上弹出，这个用笔画一下就容易多了。\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/3/","section":"post","tags":["algorithm"],"title":"【算法系列】如何仅用递归函数和栈操作逆序一个栈"},{"body":"题目要求 编写一个类，用两个栈实现队列，支持队列的基本操作：add、poll、peek\n解题思路 栈的特点是后进先出、队列的特点是先进先出 一个栈作为压入栈，另一个栈作为弹出栈，只要把压入栈的数据再压入弹出栈顺序就恢复回来了 golang实现 需要注意的是，因为只能保证每次从stackPush到stackPop的数据是连贯的, 所以stackPush在向stackPop压数据的时候必须一次性将全部数据压入, 并且只有stackPop为空时，才进行一次性压入操作。\n1package main 2 3import \u0026#34;fmt\u0026#34; 4 5type TwoStackQueue struct { 6\tstackPush *Stack 7\tstackPop *Stack 8} 9 10func NewTwoStackQueue() *TwoStackQueue { 11\treturn \u0026amp;TwoStackQueue{ 12\tstackPush: NewStack(), 13\tstackPop: NewStack(), 14\t} 15} 16 17func (tsq *TwoStackQueue) add(data interface{}) { 18\ttsq.stackPush.Push(data) 19} 20 21func (tsq *TwoStackQueue) poll() interface{} { 22\tif tsq.stackPop.Len() == 0 \u0026amp;\u0026amp; tsq.stackPush.Len() == 0 { 23\tfmt.Errorf(\u0026#34;TwoStackQueue is empty\u0026#34;) 24\t} else if tsq.stackPop.Len() == 0 { 25\tfor tsq.stackPush.Len() != 0 { 26\ttsq.stackPop.Push(tsq.stackPush.Pop()) 27\t} 28\t} 29\tvalue := tsq.stackPop.Pop() 30\treturn value 31} 32 33func (tsq *TwoStackQueue) peek() interface{} { 34\tif tsq.stackPop.Len() == 0 \u0026amp;\u0026amp; tsq.stackPush.Len() == 0 { 35\tfmt.Errorf(\u0026#34;TwoStackQueue is empty\u0026#34;) 36\t} else if tsq.stackPop.Len() == 0 { 37\tfor tsq.stackPush.Len() != 0 { 38\ttsq.stackPop.Push(tsq.stackPush.Pop()) 39\t} 40\t} 41\tvalue := tsq.stackPop.Peek() 42\treturn value 43} 测试用例 1package main 2 3import ( 4\t\u0026#34;testing\u0026#34; 5 6\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 7) 8 9func TestTwoStackQueue(t *testing.T) { 10\t// define a getmin stack 11\ttests := []struct { 12\t// 名字 13\tname string 14 15\t// 输入部分 16\tval func(tsq *TwoStackQueue) 17 18\t// 输出部分 19\twantRes interface{} 20\t}{ 21\t{ 22\tname: \u0026#34;push\u0026#34;, 23\tval: push, 24\twantRes: 1, 25\t}, 26\t{ 27\tname: \u0026#34;push and poll1\u0026#34;, 28\tval: pushpoll1, 29\twantRes: 3, 30\t}, 31\t{ 32\tname: \u0026#34;push and poll2\u0026#34;, 33\tval: pushpoll2, 34\twantRes: 2, 35\t}, 36\t{ 37\tname: \u0026#34;push and poll3\u0026#34;, 38\tval: pushpoll3, 39\twantRes: nil, 40\t}, 41\t} 42\tfor _, tt := range tests { 43\t// new TwoStackQueue 44\ttsq := NewTwoStackQueue() 45\tt.Run(tt.name, func(t *testing.T) { 46\ttt.val(tsq) 47\tres := tsq.peek() 48\tassert.Equal(t, tt.wantRes, res) 49\t}) 50\t} 51} 52 53func push(tsq *TwoStackQueue) { 54\ttsq.add(1) 55\ttsq.add(2) 56\ttsq.add(3) 57} 58 59func pushpoll1(tsq *TwoStackQueue) { 60\ttsq.add(1) 61\ttsq.add(2) 62\ttsq.add(3) 63\ttsq.poll() 64\ttsq.poll() 65} 66 67func pushpoll2(tsq *TwoStackQueue) { 68\ttsq.add(1) 69\ttsq.poll() 70\ttsq.add(2) 71\ttsq.add(3) 72} 73 74func pushpoll3(tsq *TwoStackQueue) { 75\ttsq.add(1) 76\ttsq.add(2) 77\ttsq.poll() 78\ttsq.add(3) 79\ttsq.poll() 80\ttsq.poll() 81} 测试结果：\n1$ go test -run ^TestTwoStackQueue$ . -v 2=== RUN TestTwoStackQueue 3=== RUN TestTwoStackQueue/push 4=== RUN TestTwoStackQueue/push_and_poll1 5=== RUN TestTwoStackQueue/push_and_poll2 6=== RUN TestTwoStackQueue/push_and_poll3 7--- PASS: TestTwoStackQueue (0.00s) 8 --- PASS: TestTwoStackQueue/push (0.00s) 9 --- PASS: TestTwoStackQueue/push_and_poll1 (0.00s) 10 --- PASS: TestTwoStackQueue/push_and_poll2 (0.00s) 11 --- PASS: TestTwoStackQueue/push_and_poll3 (0.00s) 12PASS 符合队列的特性，在add(1)、add(2)、add(3)的时候peek总是看到1，后面只有移除了1才能看到2，按照顺序直至队列取空。\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/2/","section":"post","tags":["algorithm"],"title":"【算法系列】由两个栈组成的队列"},{"body":"题目要求 pop、push、getMin操作的时间复杂度都是O(1) 设计的栈类型可以使用现成的栈结构 解题思路 使用两个栈，starkData和stackMin 每次压入starkData时候同时比较和stackMin顶部数据大小，小于stackMin顶部数据就将新的最小值压入栈 golang实现 golang内置数据结构不包括栈，定义一个栈:\n支持NewStack()创建 支持Push(), Pop(), Peek(), Len(), Push支持任意类型(取值出来断言) 1package main 2 3import \u0026#34;sync\u0026#34; 4 5type ( 6\tStack struct { 7\ttop *node 8\tlength int 9\tlock *sync.RWMutex 10\t} 11\tnode struct { 12\tvalue interface{} 13\tprev *node 14\t} 15) 16 17// NewStack Create a new stack 18func NewStack() *Stack { 19\treturn \u0026amp;Stack{nil, 0, \u0026amp;sync.RWMutex{}} 20} 21 22// Len Return the number of items in the stack 23func (s *Stack) Len() int { 24\treturn s.length 25} 26 27// Peek View the top item on the stack 28func (s *Stack) Peek() interface{} { 29\tif s.length == 0 { 30\treturn nil 31\t} 32\treturn s.top.value 33} 34 35// Pop the top item of the stack and return it 36func (s *Stack) Pop() interface{} { 37\ts.lock.Lock() 38\tdefer s.lock.Unlock() 39\tif s.length == 0 { 40\treturn nil 41\t} 42\tn := s.top 43\ts.top = n.prev 44\ts.length-- 45\treturn n.value 46} 47 48// Push a value onto the top of the stack 49func (s *Stack) Push(value interface{}) { 50\ts.lock.Lock() 51\tdefer s.lock.Unlock() 52\tn := \u0026amp;node{value, s.top} 53\ts.top = n 54\ts.length++ 55} 具有GetMin()方法的栈代码实现：\n1package main 2 3import ( 4\t\u0026#34;fmt\u0026#34; 5) 6 7type GetMinStack struct { 8\tstackData *Stack 9\tstackMin *Stack 10} 11 12func NewGetMinStack() *GetMinStack { 13\treturn \u0026amp;GetMinStack{ 14\tstackData: NewStack(), 15\tstackMin: NewStack(), 16\t} 17} 18 19func (gms *GetMinStack) Push(newNumber int) { 20\tif gms.stackMin.length == 0 { 21\tgms.stackMin.Push(newNumber) 22\t} else if newNumber \u0026lt;= gms.GetMin().(int) { 23\tgms.stackMin.Push(newNumber) 24\t} 25\tgms.stackData.Push(newNumber) 26} 27 28func (gms *GetMinStack) Pop() int { 29\tif gms.stackMin.length == 0 { 30\t_ = fmt.Errorf(\u0026#34;your stack is empty\u0026#34;) 31\treturn 0 32\t} 33\tvalue := gms.stackData.Pop() 34\tif value == gms.GetMin() { 35\tgms.stackMin.Pop() 36\t} 37\treturn value.(int) 38} 39 40func (gms *GetMinStack) GetMin() interface{} { 41\tif gms.stackMin.length == 0 { 42\t_ = fmt.Errorf(\u0026#34;your stack is empty\u0026#34;) 43\treturn 0 44\t} else { 45\treturn gms.stackMin.Peek() 46\t} 47} 测试用例 1package main 2 3import ( 4\t\u0026#34;testing\u0026#34; 5 6\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 7) 8 9// TDD测试驱动开发 10func TestGetMinStack_GetMin(t *testing.T) { 11\t// define a getmin stack 12\ttests := []struct { 13\t// 名字 14\tname string 15 16\t// 输入部分 17\tval func(gms *GetMinStack) 18 19\t// 输出部分 20\twantRes interface{} 21\t}{ 22\t{ 23\tname: \u0026#34;getmin after push\u0026#34;, 24\tval: push, 25\twantRes: 1, 26\t}, 27\t{ 28\tname: \u0026#34;getmin after push and pop\u0026#34;, 29\tval: pushpop, 30\twantRes: 3, 31\t}, 32\t{ 33\tname: \u0026#34;getmin for empty GetMinStack\u0026#34;, 34\tval: empty, 35\t// default return 0 36\twantRes: 0, 37\t}, 38\t} 39\tfor _, tt := range tests { 40\tgms := NewGetMinStack() 41\tt.Run(tt.name, func(t *testing.T) { 42\ttt.val(gms) 43\tres := gms.GetMin() 44\tassert.Equal(t, tt.wantRes, res) 45\t}) 46\t} 47} 48 49func push(gms *GetMinStack) { 50\tgms.Push(5) 51\tgms.Push(3) 52\tgms.Push(1) 53\tgms.Push(8) 54} 55 56func pushpop(gms *GetMinStack) { 57\tgms.Push(5) 58\tgms.Push(3) 59\tgms.Push(1) 60\tgms.Push(8) 61\tgms.Pop() 62\tgms.Pop() 63} 64 65func empty(gms *GetMinStack) { 66} 测试结果:\n1$ go test -run ^TestGetMinStack_GetMin$ -v . 2=== RUN TestGetMinStack_GetMin 3=== RUN TestGetMinStack_GetMin/getmin_after_push 4=== RUN TestGetMinStack_GetMin/getmin_after_push_and_pop 5=== RUN TestGetMinStack_GetMin/getmin_for_empty_GetMinStack 62022/08/30 15:14:48 your stack is empty 7--- PASS: TestGetMinStack_GetMin (0.00s) 8 --- PASS: TestGetMinStack_GetMin/getmin_after_push (0.00s) 9 --- PASS: TestGetMinStack_GetMin/getmin_after_push_and_pop (0.00s) 10 --- PASS: TestGetMinStack_GetMin/getmin_for_empty_GetMinStack (0.00s) 11PASS 12ok command-line-arguments 0.008s 无论栈中有多少数据，每次压入、弹出的时候都是和stackMin顶部数据做一次比较，最小值永远在stackMin顶部，时间复杂度为O(1), 满足题目要求。\n","link":"https://zhangsiming-blyq.github.io/post/algorithm/1/","section":"post","tags":["algorithm"],"title":"【算法系列】设计一个有getMin功能的栈"},{"body":" 在使用client-go的watch接口时候碰到异常退出问题，查了一下google没有多少信息，于是扒了一下代码，把自己踩的坑记录下来方便以后自查自纠。\n使用client-go watch接口 💡 全局的mycluster都等于*kubernetes.Clientset 1. 如何watch 由于kubernetes整合了etcd的watch功能，我们可以通过watch操作去建立一个长连接，不断的接收数据；这种方式要优于普通的反复轮询请求，降低server端的压力;\n使用client-go调用对应对象的Watch()方法之后，会返回一个watch.Event对象，可以对其使用ResultChan()接受watch到的对象。\n1pod, err := mycluster.Clusterclientset.CoreV1().Pods(appNamespace).Watch(context.TODO(), metav1.ListOptions{LabelSelector: label}) 2if err != nil { 3 log.Error(err) 4} 5... 6event, ok := \u0026lt;-pod.ResultChan() 7if !ok { 8 log.Error(err) 9} 异常：watch接口自动断开 1. 现象 在使用过程中，watch操作持续一段时间就会自动断开\n2. 排查 我们进入watch包里面找到streamwatcher.go，其中节选了一些重要片段：\n1type StreamWatcher struct { 2\tsync.Mutex 3\tsource Decoder 4\treporter Reporter 5\tresult chan Event 6\tstopped bool 7} 8... 9func NewStreamWatcher(d Decoder, r Reporter) *StreamWatcher { 10\tsw := \u0026amp;StreamWatcher{ 11\tsource: d, 12\treporter: r, 13\t// It\u0026#39;s easy for a consumer to add buffering via an extra 14\t// goroutine/channel, but impossible for them to remove it, 15\t// so nonbuffered is better. 16\tresult: make(chan Event), 17\t} 18\tgo sw.receive() 19\treturn sw 20} 21... 22func (sw *StreamWatcher) receive() { 23\tdefer close(sw.result) 24\tdefer sw.Stop() 25\tdefer utilruntime.HandleCrash() 26\tfor { 27\taction, obj, err := sw.source.Decode() 28\tif err != nil { 29\t// Ignore expected error. 30\tif sw.stopping() { 31\treturn 32\t} 33\tswitch err { 34\tcase io.EOF: 35\t// watch closed normally 36\tcase io.ErrUnexpectedEOF: 37\tklog.V(1).Infof(\u0026#34;Unexpected EOF during watch stream event decoding: %v\u0026#34;, err) 38\tdefault: 39\tif net.IsProbableEOF(err) || net.IsTimeout(err) { 40\tklog.V(5).Infof(\u0026#34;Unable to decode an event from the watch stream: %v\u0026#34;, err) 41\t} else { 42\tsw.result \u0026lt;- Event{ 43\tType: Error, 44\tObject: sw.reporter.AsObject(fmt.Errorf(\u0026#34;unable to decode an event from the watch stream: %v\u0026#34;, err)), 45\t} 46\t} 47\t} 48\treturn 49\t} 50\tsw.result \u0026lt;- Event{ 51\tType: action, 52\tObject: obj, 53\t} 54\t} 55} 3. 原因 结合代码看一下，StreamWatcher实现了Watch()方法，我们上述调用ResultChan()的时候，实际上返回的是这里的sw.result;\n再往下看新建StreamWatcher的时候，有一个”go sw.receive()”, 也就是几乎在新建对象的同步就开始接受处理数据了，最后看到sw的receive()方法可以看到，在处理数据的时候(sw.source.Decode()), 如果err不为nil, 会switch集中error情况，最后会直接return，然后defer sw.Stop()；\n也就是说如果接受数据解码的时候(sw.source.Decode()), 如果解码失败，那么StreamWatcher就被关闭了，那自然数据通道也就关闭了，造成”watch一段时间之后自动关闭的现象”。\n解决办法(1)：forinfor 那么既然是这种情况会导致watch断开，那么我们首先想到的就是暴力恢复这个StreamWatcher，代码实现如下：\n1for { 2\tpod, err := mycluster.Clusterclientset.CoreV1().Pods(appNamespace).Watch(context.TODO(), metav1.ListOptions{LabelSelector: label}) 3\tif err != nil { 4\tlog.Error(err) 5\t} 6loopier: 7\tfor { 8\tevent, ok := \u0026lt;-pod.ResultChan() 9\tif !ok { 10\ttime.Sleep(time.Second * 5) 11\tlog.Info(\u0026#34;Restarting watcher...\u0026#34;) 12\tbreak loopier 13\t} 14\t// your process logic 15\t} 16} 我们定义一层for嵌套，因为在上述退出的时候会先defer close(sw.result)，所以我们接受数据的通道也就是上面代码里的pod.ResultChan()就会关闭，然后我们加一个错误处理，等待5s之后，break掉这个loopier循环，让外层的for循环继续新建StreamWatcher继续监听数据。以此达到持续监听的效果，好处是实现简单，坏处是缺少错误判断，不能针对错误类型分别处理，对于一直出错的场景也只是无脑重启。\n解决办法(2)：retrywatcher 在官方代码下client-go/tools/watch/retrywatcher.go中其实官方给了一个标准解法，用于解决watch异常退出的问题，下面我们看下这种实现方式：\n1type RetryWatcher struct { 2\tlastResourceVersion string 3\twatcherClient cache.Watcher 4\tresultChan chan watch.Event 5\tstopChan chan struct{} 6\tdoneChan chan struct{} 7\tminRestartDelay time.Duration 8} 9... 10func newRetryWatcher(initialResourceVersion string, watcherClient cache.Watcher, minRestartDelay time.Duration) (*RetryWatcher, error) { 11\tswitch initialResourceVersion { 12\tcase \u0026#34;\u0026#34;, \u0026#34;0\u0026#34;: 13\t// TODO: revisit this if we ever get WATCH v2 where it means start \u0026#34;now\u0026#34; 14\t// without doing the synthetic list of objects at the beginning (see #74022) 15\treturn nil, fmt.Errorf(\u0026#34;initial RV %q is not supported due to issues with underlying WATCH\u0026#34;, initialResourceVersion) 16\tdefault: 17\tbreak 18\t} 19 20\trw := \u0026amp;RetryWatcher{ 21\tlastResourceVersion: initialResourceVersion, 22\twatcherClient: watcherClient, 23\tstopChan: make(chan struct{}), 24\tdoneChan: make(chan struct{}), 25\tresultChan: make(chan watch.Event, 0), 26\tminRestartDelay: minRestartDelay, 27\t} 28 29\tgo rw.receive() 30\treturn rw, nil 31} 和普通的StreamWatcher很类似，这里面RetryWatcher多了一些结构体字段；lastResourceVersion、minRestartDelay用于出错之后重启Watcher的RV保存，以及重试时间；传入initialResourceVersion和watcherClient(cache.Watcher)即可创建一个RetryWatcher;\n同理，RetryWatcher也是在创建对象的同时就开始go rw.receive()接受数据。\n1func (rw *RetryWatcher) receive() { 2\tdefer close(rw.doneChan) 3\tdefer close(rw.resultChan) 4 5\tklog.V(4).Info(\u0026#34;Starting RetryWatcher.\u0026#34;) 6\tdefer klog.V(4).Info(\u0026#34;Stopping RetryWatcher.\u0026#34;) 7 8\tctx, cancel := context.WithCancel(context.Background()) 9\tdefer cancel() 10\tgo func() { 11\tselect { 12\tcase \u0026lt;-rw.stopChan: 13\tcancel() 14\treturn 15\tcase \u0026lt;-ctx.Done(): 16\treturn 17\t} 18\t}() 19 20\t// We use non sliding until so we don\u0026#39;t introduce delays on happy path when WATCH call 21\t// timeouts or gets closed and we need to reestablish it while also avoiding hot loops. 22\twait.NonSlidingUntilWithContext(ctx, func(ctx context.Context) { 23\tdone, retryAfter := rw.doReceive() 24\tif done { 25\tcancel() 26\treturn 27\t} 28 29\ttime.Sleep(retryAfter) 30 31\tklog.V(4).Infof(\u0026#34;Restarting RetryWatcher at RV=%q\u0026#34;, rw.lastResourceVersion) 32\t}, rw.minRestartDelay) 33} 34... 35func (rw *RetryWatcher) Stop() { 36\tclose(rw.stopChan) 37} 上面代码的receive()函数中，wait包起到核心重试逻辑作用，他会循环执行里面的函数，直到收到context Done 的信号才会往下走；而上面代码的ctx只有两种情况才会被关闭：\n有人调用了RetryWatcher的Stop()； 另外就是rw.doReceive()中返回了done, 也会直接调用cancel()结束wait部分。 而如果接受的done为false，则会正常等待time.Sleep(retryAfter)之后，进行重试，实现RetryWatcher！\n接下来就看下这个rw.doReceive()，也就是RetryWatcher的接收处理数据部分, 同时会根据err类型判断是否应该重试：\n1func (rw *RetryWatcher) doReceive() (bool, time.Duration) { 2\twatcher, err := rw.watcherClient.Watch(metav1.ListOptions{ 3\tResourceVersion: rw.lastResourceVersion, 4\tAllowWatchBookmarks: true, 5\t}) 6\t// We are very unlikely to hit EOF here since we are just establishing the call, 7\t// but it may happen that the apiserver is just shutting down (e.g. being restarted) 8\t// This is consistent with how it is handled for informers 9\tswitch err { 10... 11// 省略watch的一些错误处理，都会返回false，也就是继续wait重试 12... 13\t} 14 15\tif watcher == nil { 16\tklog.Error(\u0026#34;Watch returned nil watcher\u0026#34;) 17\t// Retry 18\treturn false, 0 19\t} 20 21\tch := watcher.ResultChan() 22\tdefer watcher.Stop() 23 24\tfor { 25\tselect { 26\tcase \u0026lt;-rw.stopChan: 27\tklog.V(4).Info(\u0026#34;Stopping RetryWatcher.\u0026#34;) 28\treturn true, 0 29\tcase event, ok := \u0026lt;-ch: 30\tif !ok { 31\tklog.V(4).Infof(\u0026#34;Failed to get event! Re-creating the watcher. Last RV: %s\u0026#34;, rw.lastResourceVersion) 32\treturn false, 0 33\t} 34 35\t// We need to inspect the event and get ResourceVersion out of it 36\tswitch event.Type { 37\tcase watch.Added, watch.Modified, watch.Deleted, watch.Bookmark: 38\tmetaObject, ok := event.Object.(resourceVersionGetter) 39\t... 40\tresourceVersion := metaObject.GetResourceVersion() 41\t... 42 // All is fine; send the non-bookmark events and update resource version. 43\tif event.Type != watch.Bookmark { 44\tok = rw.send(event) 45\tif !ok { 46\treturn true, 0 47\t} 48\t} 49\trw.lastResourceVersion = resourceVersion 50 51\tcontinue 52 53\tcase watch.Error: 54\t... 55\t} 56\t} 57\t} 58} 59... 60func (rw *RetryWatcher) send(event watch.Event) bool { 61\t// Writing to an unbuffered channel is blocking operation 62\t// and we need to check if stop wasn\u0026#39;t requested while doing so. 63\tselect { 64\tcase rw.resultChan \u0026lt;- event: 65\treturn true 66\tcase \u0026lt;-rw.stopChan: 67\treturn false 68\t} 69} 上面代码我已经非常清晰的展示出了doReceive()的处理逻辑，第一步会按照rw中定义的watcher开始真实的监听对应资源对象，这里返回错误的话也会进行rw的重试逻辑；然后会获取真实的watcher.ResultChan()也就是可以获取到真实对象的通道，套用for select模式，循环接受数据，如果数据一直是正常的，那么会通过rw.send(event)发送给rw.ResultChan，然后记录保存rw.lastResourceVersion然后继续接收，实现watch的功能；\n这里多说一句，由于rw.lastResourceVersion是保存在rw的，也就是及时重启(对应上面的任意一个case返回的是true)，会从rw.lastResourceVersion也就是最新的RV开始监听，这样实现根据特定原因重启故障的watcher，比较合理，也很巧妙，是官方的标准答案。\n个人版RetryWatcher代码实现： 说了那么多，那么具体要怎么使用这个RetryWatcher呢？我个人做了一个妥协方案可以参考：\n还记得RetryWatcher中定义的watcherClient类型吗，需要是cache.Watcher，然后我们看client-go/tools/cache里面定义的cache.Watcher接口，定义如下：\n1type Watcher interface { 2\t// Watch should begin a watch at the specified version. 3\tWatch(options metav1.ListOptions) (watch.Interface, error) 4} 也就是实现了Watch(xxxx)这个方法的就是符合的cache.Watcher; 而最开始我们代码正好是通过MyCluster.Clusterclientset.CoreV1().Pods()返回的接口v1.PodInterface中调用的Watch(xxxxx), 那么直接用“MyCluster.Clusterclientset.CoreV1().Pods()”来生成RetryWatcher是不是就行了！\n我们来看一下这个接口类型：\n1type PodInterface interface { 2\t... 3\tWatch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error) 4\t... 5} 答案是不行… 下面这个多了一个ctx参数 🥶，但是我们就是想从Clientset这里用怎么办，自己实现一个符合的结构吧：\n1type PodListWatch struct { 2\tMyCluster *initk8s.MyCluster 3\tAppNamespace string 4} 5// watch指定命名空间下的Pod例子 6func (p PodListWatch) Watch(options metav1.ListOptions) (watch.Interface, error) { 7\treturn p.MyCluster.Clusterclientset.CoreV1().Pods(p.AppNamespace).Watch(context.TODO(), options) 8} 这个PodListWatch的Watch(xxxx)方法正好满足cache.Watcher, 单后watch的方式还是按照我们原先Clientset的，然后生成RetryWatcher的方式如下：\n1func ResourceWatcher(mycluster *initk8s.MyCluster, appNamespace string) { 2\tpodWatcher := PodListWatch{MyCluster: mycluster, AppNamespace: appNamespace} 3\t// generate new retrywatcher, use podRetryWatcher.ResultChan() to keep a chronic watch operation 4\tpodRetryWatcher, err := retrywatcher.NewRetryWatcher(label, podWatcher) 5\tif err != nil { 6\tlog.Error(err) 7\t} 8FORWATCHER: 9\tfor { 10\tselect { 11\tcase \u0026lt;-podRetryWatcher.Done(): 12\tlog.Info(\u0026#34;Retrywatcher is exiting, please check...\u0026#34;) 13\tbreak FORWATCHER 14\tcase event, ok := \u0026lt;-podRetryWatcher.ResultChan(): 15\tif !ok { 16\tlog.Warn(\u0026#34;Retrywatcher is not open, please check...\u0026#34;) 17\tcontinue 18\t} 19\t... 上述只是一种思路提供，希望能够帮到使用client-go进行watch的同学；官方的实现RetryWatcher确实更加合理，还可以进行改造加减不同情况是否重试的策略。另外watch方法毕竟是直接和kubernetes中的apiserver沟通，如果想要减轻apiserver的压力kubernetes提供了更加常用的informer机制(sharedinformer也是众多controller使用的，同时也是我认为kubernetes最核心的功能)，至于使用informer就又有很多要说的了，以后有时间可能会更新~\n","link":"https://zhangsiming-blyq.github.io/post/retrywatcher/","section":"post","tags":["golang","kubernetes"],"title":"client-go watch接口隔一段时间自动退出怎么办？"},{"body":" 讲解python中迭代器、生成器、以及yield字段的常见使用场景。\nIterators python中对象实现了__iter__()和__next__()两个方法，我们就成它是可迭代对象(iterables)，通过__iter__()可以返回一个迭代器对象(iterators)\n__iter__()方法：返回迭代器对象本身 __next__()方法：返回容器的下一个元素，在结尾时引发一个StopIteration异常终止迭代器 1lst = [1, 2, 3] 2print(type(lst)) 3new_iter = lst.__iter__() 4print(type(new_iter)) 5 6# Output 7\u0026lt;class \u0026#39;list\u0026#39;\u0026gt; 8\u0026lt;class \u0026#39;list_iterator\u0026#39;\u0026gt; for循环实际上就是通过__iter__()获取iterators，然后进行__next__()取值，直到StopIteration\nGenerators\u0026amp;yield Generators是一种特殊的迭代器(iterators)。如果函数中任意位置存在yield字段，当你调用函数，函数不会直接执行，而是会返回一个生成器(generators)。另外生成器(generators)还支持生成器表达式（类似于列表，只是将[]换成了()）\n1def test(): 2 print(\u0026#34;for test\u0026#34;) 3 yield 0 4gen1 = test() 5print(type(gen1)) 6gen2 = (x*x for x in range(0, 3)) 7print(type(gen2)) 8 9# Output 10\u0026lt;class \u0026#39;generator\u0026#39;\u0026gt; 11\u0026lt;class \u0026#39;generator\u0026#39;\u0026gt; yield 区别于迭代器(iterators)会把所有的内容全部储存到内存中，生成器(generators)会随着__next__()的不断调用分配内存\n每次生成器(generators)会运行到yield字段，然后将生成器(generators)状态保存返回；下次调用__next__()会继续从当前位置继续，到达下一个yield字段。一直到StopIteration停止。看一下下面例子：\n1def test(): 2 print(\u0026#34;start\u0026#34;) 3 yield 0 4 print(\u0026#34;end\u0026#34;) 5 yield 1 6 7gen1 = test() 8print(gen1.__next__()) 9print(gen1.__next__()) 10 11print(\u0026#34;\u0026#34;) 12gen2 = (x*x for x in range(0, 3)) 13print(gen2.__next__()) 14print(gen2.__next__()) 15print(gen2.__next__()) 16print(gen2.__next__()) 17 18# Output 19start 200 21end 221 23 240 251 264 27Traceback (most recent call last): 28 File \u0026#34;/home/vagrant/aa.py\u0026#34;, line 16, in \u0026lt;module\u0026gt; 29 print(gen2.__next__()) 30StopIteration example 我们有一批数据100条存在一个list中，想分为10个10个一组 ，分别处理：\n1def chunks(lst, n): 2 \u0026#34;\u0026#34;\u0026#34;Yield successive n-sized chunks from lst.\u0026#34;\u0026#34;\u0026#34; 3 for i in range(0, len(lst), n): 4 yield lst[i:i + n] 5 6origin_lst = list(range(0, 100)) 7for i in chunks(origin_lst, 10): 8 print(i) 9 10# Output 11[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 12[10, 11, 12, 13, 14, 15, 16, 17, 18, 19] 13[20, 21, 22, 23, 24, 25, 26, 27, 28, 29] 14[30, 31, 32, 33, 34, 35, 36, 37, 38, 39] 15[40, 41, 42, 43, 44, 45, 46, 47, 48, 49] 16[50, 51, 52, 53, 54, 55, 56, 57, 58, 59] 17[60, 61, 62, 63, 64, 65, 66, 67, 68, 69] 18[70, 71, 72, 73, 74, 75, 76, 77, 78, 79] 19[80, 81, 82, 83, 84, 85, 86, 87, 88, 89] 20[90, 91, 92, 93, 94, 95, 96, 97, 98, 99] 参考链接：\nhttps://blog.csdn.net/Yuyh131/article/details/83310486 https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do ","link":"https://zhangsiming-blyq.github.io/post/python-yield/","section":"post","tags":["python"],"title":"python yield详解"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/apiserver/","section":"tags","tags":null,"title":"apiserver"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/controller-manager/","section":"tags","tags":null,"title":"controller-manager"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/etcd/","section":"tags","tags":null,"title":"etcd"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/finance/","section":"tags","tags":null,"title":"finance"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/finance/","section":"categories","tags":null,"title":"finance"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/fund/","section":"tags","tags":null,"title":"fund"},{"body":" 对于 kubernetes 集群的控制平面组件，监控是必要的, 他可以帮助我们获取到集群的整体负载压力，并在核心组件出问题的时候配合告警让管理员及时发现问题，及时处理，更稳定的保证集群的生命周期。\n一、Prometheus 如何自动发现 Kubernetes Metrics 接口? prometheus 收集 kubernetes 集群中的指标有两种方式，一种是使用 crd(servicemonitors.monitoring.coreos.com)的方式，主要通过标签匹配；另一种是通过 scrape_config，支持根据配置好的\u0026quot;relabel_configs\u0026quot;中的具体目标, 进行不断拉取(拉取间隔为\u0026quot;scrape_interval\u0026quot;)\n配置权限： k8s 中 RBAC 支持授权资源对象的权限，比如可以 get、list、watch 集群中的 pod，还支持直接赋予对象访问 api 路径的权限，比如获取/healthz, /api 等, 官方对于 non_resource_urls 的解释如下：\nnon_resource_urls - (Optional) NonResourceURLs is a set of partial urls that a user should have access to. *s are allowed, but only as the full, final step in the path Since non-resource URLs are not namespaced, this field is only applicable for ClusterRoles referenced from a ClusterRoleBinding. Rules can either apply to API resources (such as \u0026quot;pods\u0026quot; or \u0026quot;secrets\u0026quot;) or non-resource URL paths (such as \u0026quot;/api\u0026quot;), but not both.\n既然 prometheus 要主动抓取指标，就必须对他使用的 serviceaccount 提前进行 RBAC 授权：\n1# clusterrole.yaml 2apiVersion: rbac.authorization.k8s.io/v1 3kind: ClusterRole 4metadata: 5 name: monitor 6rules: 7- apiGroups: [\u0026#34;\u0026#34;] 8 resources: 9 - nodes 10 - pods 11 - endpoints 12 - services 13 verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] 14- nonResourceURLs: [\u0026#34;/metrics\u0026#34;] 15 verbs: [\u0026#34;get\u0026#34;] 16 17# clusterrolebinding 18apiVersion: rbac.authorization.k8s.io/v1 19kind: ClusterRoleBinding 20metadata: 21 name: prometheus-api-monitor 22roleRef: 23 apiGroup: rbac.authorization.k8s.io 24 kind: ClusterRole 25 name: monitor 26subjects: 27- kind: ServiceAccount 28 name: prometheus-operator-nx-prometheus 29 namespace: monitor 获取 apiserver 自身的 metric 信息： prometheus 中配置\u0026quot;scrape_config\u0026quot;, 或者 prometheus-operator 中配置\u0026quot;additionalScrapeConfigs\u0026quot;, 配置获取 default 命名空间下的 kubernetes endpoints\n1- job_name: \u0026#39;kubernetes-apiservers\u0026#39; 2 kubernetes_sd_configs: 3 - role: endpoints 4 scheme: https 5 tls_config: 6 ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt 7 bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token 8 relabel_configs: 9 - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] 10 action: keep 11 regex: default;kubernetes;https 获取 controller-manager、scheduler 的 metric 信息： controller-manager 和 scheduler 因为自身暴露 metric 接口，需要修改对应 manifests 下的静态 pod 文件，添加匹配的 annotations 即可完成抓取：\n1# prometheus端配置 2kubernetes_sd_configs: 3 - role: pod 4 relabel_configs: 5 - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] 6 action: \u0026#34;keep\u0026#34; 7 regex: \u0026#34;true\u0026#34; 8 - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] 9 action: replace 10 regex: ([^:]+)(?::\\d+)?;(\\d+) 11 replacement: $1:$2 12 target_label: __address__ 13 - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] 14 action: replace 15 target_label: __metrics_path__ 16 regex: \u0026#34;(.+)\u0026#34; 17 - action: labelmap 18 regex: __meta_kubernetes_pod_label_(.+) 19 - source_labels: [__meta_kubernetes_namespace] 20 action: replace 21 target_label: kubernetes_namespace 22 - source_labels: [__meta_kubernetes_pod_name] 23 action: replace 24 target_label: kubernetes_pod_name 25 26# controller-manager配置: 27metadata: 28 annotations: 29 prometheus_io_scrape: \u0026#34;true\u0026#34; 30 prometheus.io/port: \u0026#34;10252\u0026#34; 31 32# scheduler配置： 33metadata: 34 annotations: 35 prometheus_io_scrape: \u0026#34;true\u0026#34; 36 prometheus.io/port: \u0026#34;10251\u0026#34; 获取 etcd 的 metric 信息： etcd 是跑在物理机上的，所以我们先创建对应的 endpoints 绑定好 service，然后采用 servicemonitor 的方式去匹配获取 etcd 的监控指标：\n1# service.yaml 2apiVersion: v1 3kind: Service 4metadata: 5 name: etcd-k8s 6 namespace: kube-system 7 labels: 8 k8s-app: etcd 9spec: 10 type: ClusterIP 11 clusterIP: None 12 ports: 13 - name: port 14 port: 2379 15 protocol: TCP 16 17# endpoint.yaml 18apiVersion: v1 19kind: Endpoints 20metadata: 21 name: etcd-k8s 22 namespace: kube-system 23 labels: 24 k8s-app: etcd 25subsets: 26- addresses: 27 - ip: xx.xx.xx.xx 28 - ip: xx.xx.xx.xx 29 - ip: xx.xx.xx.xx 30 ports: 31 - name: port 32 port: 2379 33 protocol: TCP 34 35# servicemonitor.yaml(需要配置好相关的证书) 36apiVersion: monitoring.coreos.com/v1 37kind: ServiceMonitor 38metadata: 39 name: etcd-k8s 40 namespace: monitor 41 labels: 42 k8s-app: etcd-k8s 43 release: prometheus-operator-nx 44spec: 45 jobLabel: k8s-app 46 endpoints: 47 - port: port 48 interval: 30s 49 scheme: https 50 tlsConfig: 51 caFile: /ca.pem 52 certFile: /server.pem 53 keyFile: /server-key.pem 54 insecureSkipVerify: true 55 selector: 56 matchLabels: 57 k8s-app: etcd 58 namespaceSelector: 59 matchNames: 60 - kube-system 61 62# 最后附上etcd secret的创建方法，将etcd证书挂载进入提供连接使用 63apiVersion: v1 64data: 65 ca.pem: xx 66 server.pem: xx 67 server-key.pem: xx 68kind: Secret 69metadata: 70 name: etcd-certs 71 namespace: monitor 72type: Opaque 二、我该重点关注哪些 control plane 指标？ apiserver: 其中计算延迟可以采用\u0026quot;percentiles\u0026quot;而不是平均数去更好的展示延迟出现情况 apiserver_request_duration_seconds: 计算读(Non-LIST)请求，读(LIST)请求，写请求的平均处理时间\napiserver_request_total: 计算 apiserver 的 QPS、计算读请求、写请求的成功率; 还可以计算请求错误数量以及错误码\napiserver_current_inflight_requests: 计算正在处理的读、写请求\napiserver_dropped_requests_total: 计算失败的请求\ncontroller-manager: leader_election_master_status: 关注是否有 leader\nxxx_depth: 关注正在调和的控制队列深度\nscheduler: leader_election_master_status: 关注是否有 leader\nscheduler_schedule_attempts_total: 帮助查看是否调度器不能正常工作; Number of attempts to schedule pods, by the result. 'unschedulable' means a pod could not be scheduled, while 'error' means an internal scheduler problem\nscheduler_e2e_scheduling_duration_seconds_sum: scheduler 调度延迟(参数弃用)\nrest_client_requests_total: client 请求次数(次重要); Number of HTTP requests, partitioned by status code, method, and host\netcd: etcd_server_has_leader: etcd 是否有 leader\netcd_server_leader_changes_seen_total: etcd leader 切换的次数，如果太频繁可能是一些连接不稳定现象或者 etcd 集群负载过大\netcd_server_proposals_failed_total: 一个提议请求是需要完整走过 raft protocol 的，这个指标帮助我们提供请求出错次数，大多数情况是 etcd 选举 leader 失败或者集群缺乏选举的候选人\netcd_disk_wal_fsync_duration_seconds_sum/etcd_disk_backend_commit_duration_seconds_sum: etcd 磁盘存储了 kubernetes 的所有重要信息，如果磁盘同步有很大延迟会影响 kubernetes 集群的操作, 此指标提供了 etcd 磁盘同步的平均延迟\netcd_debugging_mvcc_db_total_size_in_bytes: etcd 各节点容量\netcd_network_peer_sent_bytes_total/etcd_network_peer_received_bytes_total: 可以计算 etcd 节点的发送/接收数据速率\ngrpc_server_started_total: 可以用于计算 etcd 各个方法的调用速率\n最后采用的 kubernetes 维护界面由：apiserver 专用仪表盘、etcd 仪表盘、综合控制平面仪表盘和证书监控组成；并且在维护使用过程中根据不同参数，不断调整, 够用就行。\n参考链接: https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/cluster_role\nhttps://prometheus.io/docs/prometheus/latest/configuration/configuration/\nhttps://www.datadoghq.com/blog/kubernetes-control-plane-monitoring/\nhttps://sysdig.com/blog/monitor-kubernetes-api-server/\nhttps://sysdig.com/blog/monitor-etcd/\n","link":"https://zhangsiming-blyq.github.io/post/monitor-control-plane/","section":"post","tags":["kubernetes","apiserver","etcd","controller-manager","scheduler"],"title":"prometheus 监测 kubernetes 控制平面"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/scheduler/","section":"tags","tags":null,"title":"scheduler"},{"body":" 股神巴菲特说：“通过定期投资指数基金，一个什么都不懂的业余投资者竟然往往能够战胜大部分专业投资者；指数基金追踪指数，对基金经理没有过多依赖；每个指数基金会同时投资几十只甚至上百只股票，可以最大化分散投资者的非市场风险；而且在所有同类基金里，指数基金的费率也是最低的。”\n宽基指数基金 上证50指数：上交所选出沪市规模最大、流动性最好、最具有代表性的50只股票组成样本股，反应沪市最具影响力的一批优秀大盘企业的整体状况 沪深300指数：由中证指数公司开发，从上交所和深交所挑选规模最大、流动性最好的300只股票，国内股市最具代表性的指数 中证500指数：将全部沪深300指数的300家公司排除，然后将最近一年日均总市值排名前300名的企业也排除，剩下的公司中选择日均总市值排名前500名的企业，作为中证500指数 创业板指数：放在深交所下面的，给达不到主板上市条件的小企业开放的门槛更低的市场；创业板综指指的是所有创业板上市公司的股价平均表现；创业板指数是反应创业板最主要的100家企业的平均表现 以上四种指数按照市值加权，股票规模越大，指数中权重越高。\n红利指数：是近十几年兴起的一类比较特殊的策略加权基金；我们都知道股票是会发放股息的，也就是现金分红；业绩比较好的公司会每年从净利润中拿出一部分，以现金分红的形式回馈股东；而红利指数就是按照分红进行加权，组成指数基金。常见的有上证红利指数、中证红利指数、深证红利指数和红利机会指数 基本面指数：基本面基本覆盖了一个公司运营的各个方面，比如营业收入、现金流、净资产、分红等，按照基本面进行加权的叫做基本面指数 央视财经50指数：专家们投票选出来的50只股票，和专家的选股能力息息相关 恒生指数：投资中国香港上市的公司中规模最大的50家企业 H股指数：公司在内地注册，但是在香港上市，这样的公司就是H股；H股指数的全程是恒生中国企业指数 上证50AH优选指数：很多公司同时在A股和H股上市，关系紧密；买入AH股中相对便宜的那个，卖出相对贵的那个，利用这一原理来获取超额收益的组成上证50AH优选指数 纳斯达克100指数：美股纳斯达克规模最大的100家大型企业 标普500指数：以大盘股为主，有500只成分股；标普500是一个附带主观判断的蓝筹股指数，包括公司各个行业分布、大小型公司的选择都比较均匀 上证综指：上交所全部的上市公司组成 中证100指数：沪深300中选择前100组成、还有类似的中证800、中证1000、中证全指 等权重指数：等权重指数则是分配给每个成分股完全相同的权重 行业指数基金 优秀行业： 必须消费：主要是维持我们正常生活所需要的各种消费品、例如饮料、酒、农副食品等 医药行业：每个人都离不开生老病死，医药是人类的基本需求 可选消费：饮料、烟草等常用日用消费品，一般单价比较低、消费频率大，也是刚需 养老产业：多行业混合的产业，覆盖了多个行业，包括医疗保健、信息技术、日常消费、可选消费、甚至包括保险公司这种金融行业 强周期性行业 银行业：银行的周期性受宏观经济的影响较大，经济处于下行周期，银行利差收益减少，利益就变差；经济好转时，银行效益就会变好 证券业 地产业 其他： 军工行业 环保行业 白酒行业 常见的估值指标 市盈率：公司市值/公司盈利，反映了我们愿意为了获取1元的净利润付出多少代价(市盈率10表示我们愿意为了1元盈利付出10元)；也可以反推估算市值(根据公司年净利润乘市盈率可以计算市值)；适用于流通性好，盈利稳定的公司 盈利收益率：公司盈利/公司市值，一般来说盈利收益率越高公司的估值就越低，公司越有可能被低估；适用于流通性好，盈利稳定的公司 市净率：公司市值/公司净资产，净资产就是资产减去负债；当企业的资产大多是比较容易衡量价值的有形资产，并且使长期保值的资产时，比较适合用市净率来估值 股息率：过去一年公司的现金分红/公司市值，股息率衡量的就是现金分红的收益率；盈利收益率*分红率=股息率；分红率一般是公司预先就设置好了，连续多年不会有什么改变；股息率会随之股价波动，股价越低，股息率越高 指数的估值查看”银行螺丝钉公众号”获取\n挑选指数基金 盈利收益率法： 投资方法：\n盈利收益率大于10%时，开始(坚持)定投 盈利收益率小于10%，大于6.4%的时候暂停定投、继续持有；可以定投其他盈利收益率大于10%的品种 盈利收益率小于6.4%时，分批卖出 适用范围：流通性好、盈利比较稳定的品种：\n指数：上证50、上证红利、中证红利、基本面50、央视50、上证50AH优选、恒生指数、H股指数\n博格公式法： 指数基金未来的年复合收益率，等于指数基金的投资初期股息率，加上指数基金每年的市盈率变化率，再加上指数基金每年的盈利变化率：\n股息率：从公证号估值获取 市盈率：市盈率呈周期性变化，如果看历史市盈率处于较低则未来大概率是会上涨的，这时候应该买入 盈利：一般情况国内经济上涨都会带来正收益 投资方法：\n市盈率处于历史底部区域的时候坚持定投 市盈率进入正常估值，暂停定投，继续持有；可以定投其他处于底部的产品 市盈率进入历史较高区域的时候卖出 适用范围：盈利呈高速增长姿态的指数，可以使用博格公式对其进行判断。如果指数当前的市盈率处于它历史市盈率波动范围的较低区域，就可以投资它\n指数：沪深300、中证500、创业板、红利机会指数、必需消费行业指数、医药行业指数、可选消费行业指数、养老产业指数\n博格公式法-变种： 指数基金未来的年复合收益率，等于指数基金每年市净率的变化率，加上指数基金每年净资产的变化率\n投资方法：\n市净率处于历史底部区域的时候坚持定投 市净率进入正常估值，暂停定投，继续持有；可以定投其他处于底部的产品 市净率进入历史较高区域的时候卖出 适用范围：盈利处于不稳定状态或者盈利呈周期性变化，但行业没有长期亏损记录的指数\n指数：证券行业指数、金融行业指数、非银金融行业指数、地产行业指数\n在指数基金低估、值的投资的时候再进行定投\n提高定投收益的五个小技巧 省下就是赚到：降低交易基金的费用 正确处理基金分红，能让长期收益上一个台阶(收益继续投入) 定投的频率选择：长期收益来看，一个月一投和一周一投收益几乎一致；如果月中看到大跌忍不住追加建议两周或者一周一投都可 定期不定额(盈利收益率法)：以盈利收益率首次达到10%以上开始投资基金额为基准，以后每个月定投的金额可以用下面公式计算：首次低谷时的定投资金*(当月的盈利收益率/首次的盈利收益率) 定期不定额(博格公式法)：首次低估时的定投资金*(首次的市盈率/当月的市盈率)；适用于博格公式变种的则是：首次低估时的定投资金*(首次的市净率/当月的市净率) 我应该投入多少资金到指数基金？ 对于手里有部分流动资产，可以用100减去自己当前的年龄，用得到的数值加上百分号就是适合投入到指数基金中的资金比例； 对于上班族：(每个月收入-各项开支)*0.5的资金作为指数基金定投是比较合理的；建议一部分指数基金，另一部分货币基金 巴菲特卖出的三个条件 基本面恶化：国家宏观经济如果走下坡路，那么指数基金是不值得持有的 过于昂贵：应当严格遵守盈利收益率低于6.4%卖出 有更好的品种 定投计划表 日期 操作(买/卖) 交易品种代码 买入或卖出金额 成交单价 买入或卖出份额 买入或卖出时的估值 ","link":"https://zhangsiming-blyq.github.io/post/indexfund/","section":"post","tags":["fund","finance"],"title":"指数基金投资指南摘记"},{"body":"+++ title = \u0026quot;Search\u0026quot; searchPage = true type = \u0026quot;search\u0026quot; +++\n","link":"https://zhangsiming-blyq.github.io/search/","section":"","tags":null,"title":""},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/about/","section":"tags","tags":null,"title":"about"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/baileysrock/","section":"tags","tags":null,"title":"BaileysRock"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/bailiyingqi/","section":"tags","tags":null,"title":"bailiyingqi"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/containerd/","section":"tags","tags":null,"title":"containerd"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/containerd/","section":"categories","tags":null,"title":"containerd"},{"body":"","link":"https://zhangsiming-blyq.github.io/categories/docker/","section":"categories","tags":null,"title":"docker"},{"body":"","link":"https://zhangsiming-blyq.github.io/tags/tips/","section":"tags","tags":null,"title":"tips"},{"body":"专注于IT互联网，包括但不限于kubernetes、Go语言(golang)、python、shell、linux等\n百里英骐 ♥我的instagram\n说点什么 Remember, it's never too late to start studying and making progress. Stay focused, stay motivated, and do your best 备注 本站所有内容本着传播正能量，促进互相学习的基本原则, 可能会引用一些互联网上公开的内容。 如有侵权，请联系1030728296@qq.com删除, 感谢理解与支持。\n","link":"https://zhangsiming-blyq.github.io/about/","section":"","tags":["about","bailiyingqi"],"title":"关于我"},{"body":" 本篇，用于记录那些解答了自己疑惑的小知识点，由于篇幅过短并没有必要单独写一个blog，所以记录于此； 内容源自随处看到的公众号，博客，网络等，均会注明出处用于分享学习。\nkubernetes常规问题 kubernetes的cordon打上的SchedulingDisabled仅仅影响调度，也就是直接打上nodeName不会受到该参数的影响 kubernetes的QosClass判断pod内的全部container，包括init-container，也就是如果init-container不进行限制，其他container无论怎么配置仍然不是Guaranteed kubernetes集群新版本如果cordon打上unschedule，会默认追加Taint；旧版本不会 kubernetes集群对于unschedule的节点不会走入调度环节，只有可以正常调度的节点才会走到后面判断Toleration，label等；特别地，对于daemonset的pod，schedulingDisable无效，但是tolerance等有效(v1.17版本中，Damoneset 的 pod 的调度从 daemonset controller 迁移到 kube-scheduler 来做调度，从而支持 PodAffnity、PodAntiAffinity 等能力) Error(不再重启)，Completed状态的podip会显示，但是实际不占用podip，真实podip已经分配给其他服务使用 kubernetes中，kubelet限制的max-pod数量是限制的具体的pod数量，超出会报错Outofpods 每次pod重启，kubelet会给他分配新的cgroup目录路径，而不会使用原来的；新的pod启动之后间隔一小段时间会删除旧的cgroup路径 kubernetes 推荐使用 systemd 来代替 cgroupfs; 因为systemd是kubernetes自带的cgroup管理器, 负责为每个进程分配cgroups; 但docker的cgroup driver默认是cgroupfs,这样就同时运行有两个cgroup控制管理器；可以使用docker info查看docker使用的cgroup driver，然后从\u0026quot;/etc/docker/daemon.json\u0026quot;中修改成systemd 中断绑定cpu核心问题 中断是什么？中断是一种电信号，由硬件产生并直接送到中断控制器上，再由中断控制器向CPU发送中断信号，CPU检测到信号后，中断当前工作转而处理中断信号；其实准确的说这种算硬中断 如果不像让这种中断，或者系统中断和网络中断和一些业务的中断在同一个cpu上面互相影响；可以把某个中断绑定到某几个特定的cpu核心，来达到目的 默认情况systemctl status irqbalance服务会平衡所有中断均衡地是用cpu 可以用echo cpu号 \u0026gt; /proc/irq/中断号/smp_affinity或者使用taskset来绑定中断到具体的cpu核心 ethtool -l eth0可以看到，一些通道信息(这个通道是可以触发网络中断的队列数量), 设置多了会影响内存等资源，设置小了可能会称为高流量瓶颈 参考链接：https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/interrupt_and_process_binding 在default命名空间下的svc, kubernetes 该svc作为集群内部服务连接api-server的媒介(这三个信息会被注入到每个集群内部的pod中: KUBERNETES_SERVICE_HOST=10.96.0.1、KUBERNETES_SERVICE_PORT=443、KUBERNETES_SERVICE_PORT_HTTPS=443) 永远使用\u0026quot;--service-cluster-ip-range\u0026quot;定义的CIDR的第一个ip地址 svc以及对应的endpoints都是由master controller(api-server二进制文件在最开始启动的controller之一)管控 RunKubernetesService()是一个循环，里面的逻辑包含支持UpdateKubernetesService()更新这个svc信息，ReconcileEndpoints(...endpointPorts []corev1.EndpointPort...)来更新endpoint，也就是一般3个master的信息；不过有时候看到的endpoint可能只有一个ip，这可能是云厂商传入的master lb层 参考链接: https://networkop.co.uk/post/2020-06-kubernetes-default/ git的cherry-pick操作 参考链接: https://www.atlassian.com/git/tutorials/cherry-pick/ git 的 cherry-pick 操作简单来讲就是可以把具体的commit从一个分支，直接嫁接(复制)到另一个分支, 下面看一个例子: 1$ git branch 2* feat/siming 3 master 4$ git log 5commit 1d7df64add47be9891efa6469f663e78acf3982f (HEAD -\u0026gt; feat/siming, origin/feat/siming) 6Author: zhangsiming \u0026lt;zhangsiming@360.cn\u0026gt; 7Date: Fri Feb 10 20:18:59 2023 +0800 8 9 test2 10 11commit a272f807df9f22c58aa2a970ff26a13a66abec4d 12Author: zhangsiming \u0026lt;zhangsiming@360.cn\u0026gt; 13Date: Fri Feb 10 19:59:06 2023 +0800 14 15 test 16... 17 18# 可以看到feat/siming分支最近两个commit一个是test，一个是test2，我们现在记录一下test的commitId，然后把他cherry-pick到master分支 19$ git checkout master 20$ git cherry-pick a272f807df9f22c58aa2a970ff26a13a66abec4d 21 22# 大功告成，test部分的变更已经追加到了master分支，我们看一下git log graph(注意看HEAD指针位置) 23$ git log --pretty=oneline --graph --decorate --all 24 25* 1d7df64add47be9891efa6469f663e78acf3982f (origin/feat/siming, feat/siming) test2 26* a272f807df9f22c58aa2a970ff26a13a66abec4d test 27* 627f296be9f64418d4f6dfe99d2fcf6881196f30 (HEAD -\u0026gt; master, origin/master, origin/HEAD) Merge branch \u0026#39;feat/siming\u0026#39; into \u0026#39;master\u0026#39; 28|\\ 29| * 3a64ef1e9c78dba97b775ac6fcf3a1ecf0c7e925 fix: 优化gpu-alarmer 30| * 08358da0a98490e9e87a990342d13dbeffb7758f add: k8s-weekly-report 31| * 7231fc0a739fae55a5800e883d2e811b6c58e7f3 fix: 修改eventsinformer时间展示 32 33# 如果不想要这个commit了，可以reset回退(HEAD后面有几个^就回退几个commit，或者采用HEAD~n) 34$ git reset --hard HEAD^ ","link":"https://zhangsiming-blyq.github.io/tips/","section":"","tags":["tips","BaileysRock"],"title":"学习摘记"},{"body":" 伴随着kubernetes对docker的弃用，containerd开始进入大众视野；相比于kubelet中集成docker-shim连接docker，docker再次条用containerd去管理容器，直接使用containerd可以通过原生CRI接口的调用实现容器runtime，简化了调用链路，更加的灵活可靠。\n一、安装使用 ctr 管理 containerd 1# Install Dependent Libraries 2$ sudo apt-get update 3$ sudo apt-get install libseccomp2 4 5# 下载 6# 目前是下载的1.5.2 7$ wget https://github.com/containerd/containerd/releases/download/v${VERSION}/cri-containerd-cni-${VERSION}-linux-amd64.tar.gz 8 9# 安装 10$ sudo tar --no-overwrite-dir -C / -xzf cri-containerd-cni-${VERSION}-linux-amd64.tar.gz 11# 初始化containerd配置 12$ containerd config default \u0026gt; /etc/containerd/config.toml 13# 修改默认的sandbox_image 14$ vim /etc/containerd/config.toml 15... 16sandbox_image = \u0026#34;registry.cn-beijing.aliyuncs.com/shannonai-k8s/pause:3.1\u0026#34; 17... 18 19# 启动服务 20sudo systemctl daemon-reload 21sudo systemctl start containerd 22 23# 查看版本 24$ ctr version 25Client: 26 Version: 1.4.3 27 Revision: 269548fa27e0089a8b8278fc4fc781d7f65a939b 28 Go version: go1.13.15 29 30Server: 31 Version: 1.4.3 32 Revision: 269548fa27e0089a8b8278fc4fc781d7f65a939b 33 UUID: b7e3b0e7-8a36-4105-a198-470da2be02f2 二、containerd 使用 2.1 运行一个 busybox 镜像： demo: 1# 拉取镜像 2$ ctr -n k8s.io i pull docker.io/library/busybox:latest 3# 创建一个container(此时还未运行) 4$ ctr -n k8s.io container create docker.io/library/busybox:latest busybox 5# 创建一个task 6$ ctr -n k8s.io task start -d busybox 7 8# 上述步骤也可以简写成如下 9$ ctr -n k8s.io run -d docker.io/library/busybox:latest busybox 查看容器在宿主机的 pid，及状态:\n1$ ctr -n k8s.io task ls 2TASK PID STATUS 3busybox 2356 RUNNING 进入容器：\n1$ ctr -n k8s.io t exec --exec-id $RANDOM -t busybox sh 杀死移除容器：\n1$ ctr -n k8s.io t kill -s SIGKILL busybox 2$ ctr -n k8s.io t rm busybox 3WARN[0000] task busybox exit with non-zero exit code 137 2.2 其他 ctr 命令 镜像标记:\n1$ ctr -n k8s.io i tag A B 2# 若新镜像reference 已存在, 需要先删除新reference, 或者如下方式强制替换 3$ ctr -n k8s.io i tag --force A B 删除镜像:\n1$ ctr -n k8s.io i rm A 拉取镜像:\n1$ ctr -n k8s.io i pull -k A 查看镜像:\n1$ ctr -n k8s.io i ls 推送镜像:\n1$ ctr -n k8s.io i push -k A 导出镜像:\n1$ ctr -n k8s.io i export A.tar A 导入镜像:\n1$ ctr -n k8s.io i import A.tar 读取日志: 导出到文件中进行读取:\n1$ ctr -n k8s.io run --log-uri file:///var/log/xx.log 三、containerd + docker 安装新版本的 docker-ce 默认，采用 containerd(--containerd=/run/containerd/containerd.sock)\n1$ systemctl status docker 2● docker.service - Docker Application Container Engine 3 Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) 4 Active: active (running) since 四 2021-06-10 15:46:44 CST; 1h 46min ago 5 Docs: https://docs.docker.com 6 Main PID: 4965 (dockerd) 7 CGroup: /system.slice/docker.service 8 ├─1830 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/e0a50564d9a23a85240f54f3bf 9 ├─4965 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock 10 ├─5402 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 8082 -container-ip 172.17.0.2 -container-port 80 11 └─5417 /usr/bin/docker-proxy -proto tcp -host-ip :: -host-port 8082 -container-ip 172.17.0.2 -container-port 80 注意： ctr 直接跑起来一个容器只有 lo 网卡，也就是无法与外网通信；如果想连接外网，请参考 其他参考链接:\nhttps://mp.weixin.qq.com/s/A9zU7gZH0liLjc-e-dhk8A\nhttps://blog.csdn.net/tongzidane/article/details/114587138\nhttps://github.com/containerd/containerd/blob/master/docs/cri/installation.md\n","link":"https://zhangsiming-blyq.github.io/post/containerd/","section":"post","tags":["kubernetes","containerd"],"title":"浅谈 containerd"},{"body":"","link":"https://zhangsiming-blyq.github.io/series/","section":"series","tags":null,"title":"Series"}]
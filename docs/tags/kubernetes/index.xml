<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on </title>
    <link>https://zhangsiming-blyq.github.io/tags/kubernetes/</link>
    <description>Recent content in kubernetes on </description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Sat, 21 Sep 2024 16:00:00 +0000</lastBuildDate><atom:link href="https://zhangsiming-blyq.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kubernetes 中 Cilium 网络架构详解与流量处理流程</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes/cilium/</link>
      <pubDate>Sat, 21 Sep 2024 16:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes/cilium/</guid>
      <description>
        
          
            1. Cilium 和 Cilium-Operator 的网络架构 在 Kubernetes 集群中，我们使用 Cilium 作为网络插件，基于 eBPF 提供高效的网络功能，如流量控制、负载均衡、网络安全策略等。同时，Cilium-Operator 负责管理集群范围的 Cilium 操作，包括 IP 地址管理、服务发现、BGP 宣告和配置同步等。
2. IP 分配与 Cilium-Host 的作用 2.1 Cilium 如何分配 IP 地址 Pod 创建时的 IP 分配：当 Kubernetes 中创建一个新的 Pod 时，Cilium 的 CNI 插件 负责为该 Pod 分配 IP 地址。这与 Kubernetes Controller Manager 协同完成。在我们公司的场景中，Cilium 使用的是公司内部的物理 IP 地址，这些地址是从公司内部的一个专门的 IP 地址池中分配的，以确保 Pod 在公司内部的网络中能够被识别和访问。
通过 Cilium CNI 插件将 IP 分配到 Pod：Cilium 作为 Kubernetes 的 CNI 插件，负责为新创建的 Pod 分配 IP 地址，并将该 IP 绑定到 Pod 的虚拟网络接口（veth）。Cilium 通过 CNI 接口向 Kubernetes 报告这些信息，确保 Kubernetes 的网络配置与 Cilium 的 eBPF 配置一致。
          
          
        
      </description>
    </item>
    
    <item>
      <title>Kubernetes Operator 新手开发一文入门</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes/operator/</link>
      <pubDate>Sun, 15 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes/operator/</guid>
      <description>
        
          
            概述 Kubernetes Operator 简介 Kubernetes Operator 是一类 Kubernetes 控制器，它能够自动化管理复杂的应用程序和其生命周期，通常被用来管理有状态应用（如数据库、缓存等）。通过扩展 Kubernetes API，Operator 可以将日常操作流程（如安装、升级、扩展、备份等）转换为 Kubernetes 原生对象，从而实现自动化和声明式管理。
为什么使用 Operator Operator 通过将 DevOps 团队日常管理应用的运维知识和流程编码化，使复杂的应用程序管理变得简单和自动化。在 Kubernetes 中，Operator 可以持续监控自定义资源，并自动进行相应操作，确保应用程序的状态与用户期望一致。
Operator 与 Controller 的关系 Operator 实际上是一个高级 Controller，它不仅负责监控和管理 Kubernetes 中的自定义资源 (CR)，还可以执行特定的业务逻辑。Controller 是 Kubernetes 架构中管理资源状态的核心组件，Operator 是对 Controller 的封装和扩展，专门用于复杂应用的生命周期管理。
核心概念 自定义资源 (CR) 和自定义资源定义 (CRD) 什么是 CR 自定义资源 (Custom Resource, CR) 是 Kubernetes 用户可以定义的扩展对象，用于描述某个具体的应用或资源的期望状态。每个 CR 对象的结构基于其相应的 CRD (Custom Resource Definition)，通过 CR，用户可以声明他们希望 Kubernetes 管理的特定应用或服务。
什么是 CRD CRD（自定义资源定义）是 Kubernetes 的一种扩展机制，允许用户向 Kubernetes API 添加新的对象类型。通过 CRD，用户可以定义新的资源种类（类似于内置的 Pod、Service 等），并指定这些资源的结构和行为。
          
          
        
      </description>
    </item>
    
    <item>
      <title>Deploy Kubernetes cluster via kubeadm</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes/kubeadm/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes/kubeadm/</guid>
      <description>
        
          
            1. Install kubeadm(kubelet kubectl) and docker https://docs.docker.com/engine/install/ubuntu/
Set up docker apt repository:
1# Add Docker&amp;#39;s official GPG key: 2sudo apt-get update 3sudo apt-get install ca-certificates curl 4sudo install -m 0755 -d /etc/apt/keyrings 5sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc 6sudo chmod a+r /etc/apt/keyrings/docker.asc 7 8# Add the repository to Apt sources: 9echo \ 10 &amp;#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \ 11 $(. /etc/os-release &amp;amp;&amp;amp; echo &amp;#34;$VERSION_CODENAME&amp;#34;) stable&amp;#34; | \ 12 sudo tee /etc/apt/sources.
          
          
        
      </description>
    </item>
    
    <item>
      <title>如何加节点到Kubernetes集群</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes/addnode/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes/addnode/</guid>
      <description>
        
          
            方法一：基于 Bootstrap Token 的加入方式 原理 Bootstrap Token 是一种临时的令牌，用于新节点在加入集群时进行身份验证。 新节点使用 kubeadm join 命令，提供 --token 和 --discovery-token-ca-cert-hash 参数，与控制平面 API 服务器建立安全连接。 控制平面验证 token 的有效性，并向新节点提供所需的证书和配置文件，使其能够加入集群。 具体步骤 1. 在控制平面节点上生成 Bootstrap Token 使用以下命令生成新的引导令牌：
1kubeadm token create --print-join-command 输出示例：
1kubeadm join 10.0.24.14:6443 --token abcdef.0123456789abcdef \ 2 --discovery-token-ca-cert-hash sha256:430cb53669a7fde6e44338968458d47f3fcdbeda4d73bda7435df34ed20ad5be --print-join-command 参数会直接输出用于加入集群的完整命令，包括 token 和 discovery-token-ca-cert-hash。 2. 在新节点上执行加入命令 在新节点上，以 root 或具有相应权限的用户身份执行上述输出的命令：
1kubeadm join 10.0.24.14:6443 --token abcdef.0123456789abcdef \ 2 --discovery-token-ca-cert-hash sha256:430cb53669a7fde6e44338968458d47f3fcdbeda4d73bda7435df34ed20ad5be 3. 加入过程解析 身份验证：新节点使用 token 与控制平面 API 服务器进行身份验证。 证书验证：使用 --discovery-token-ca-cert-hash 提供的哈希值，确保连接的 API 服务器是可信的。 获取配置：验证通过后，新节点从控制平面获取 kubelet 所需的配置文件和证书。 节点注册：kubelet 启动并与控制平面通信，节点被注册到集群中。 注意事项 Token 有效期：默认情况下，token 有效期为 24 小时。可以使用 --ttl 参数调整有效期。 Token 管理：使用 kubeadm token list 查看现有 token，kubeadm token delete &amp;lt;token-id&amp;gt; 删除 token。 方法二：使用 静态 Kubeconfig 文件 的方式(这种是最常规的, 最正确的) 原理 预先在控制平面节点上为新节点生成 kubeconfig 文件，包含必要的证书和配置信息。 将 kubeconfig 文件安全地传输到新节点。 新节点的 kubelet 使用该 kubeconfig 文件与控制平面通信，完成加入过程。 具体步骤 1.
          
          
        
      </description>
    </item>
    
    <item>
      <title>kubernetes常规问题</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes/kubernetes%E5%B8%B8%E8%A7%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes/kubernetes%E5%B8%B8%E8%A7%84%E9%97%AE%E9%A2%98/</guid>
      <description>
        
          
             kubernetes的cordon打上的SchedulingDisabled仅仅影响调度，也就是直接打上nodeName不会受到该参数的影响 kubernetes的QosClass判断pod内的全部container，包括init-container，也就是如果init-container不进行限制，其他container无论怎么配置仍然不是Guaranteed kubernetes集群新版本如果cordon打上unschedule，会默认追加Taint；旧版本不会 kubernetes集群对于unschedule的节点不会走入调度环节，只有可以正常调度的节点才会走到后面判断Toleration，label等；特别地，对于daemonset的pod，schedulingDisable无效，但是tolerance等有效(v1.17版本中，Damoneset 的 pod 的调度从 daemonset controller 迁移到 kube-scheduler 来做调度，从而支持 PodAffnity、PodAntiAffinity 等能力) Error(不再重启)，Completed状态的podip会显示，但是实际不占用podip，真实podip已经分配给其他服务使用 kubernetes中，kubelet限制的max-pod数量是限制的具体的pod数量，超出会报错Outofpods 每次pod重启，kubelet会给他分配新的cgroup目录路径，而不会使用原来的；新的pod启动之后间隔一小段时间会删除旧的cgroup路径 kubernetes 推荐使用 systemd 来代替 cgroupfs; 因为systemd是kubernetes自带的cgroup管理器, 负责为每个进程分配cgroups; 但docker的cgroup driver默认是cgroupfs,这样就同时运行有两个cgroup控制管理器；可以使用docker info查看docker使用的cgroup driver，然后从&amp;quot;/etc/docker/daemon.json&amp;quot;中修改成systemd 
          
          
        
      </description>
    </item>
    
    <item>
      <title>在default命名空间下的svc, kubernetes</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes/kubernetes-default-svc/</link>
      <pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes/kubernetes-default-svc/</guid>
      <description>
        
          
            
该svc作为集群内部服务连接api-server的媒介(这三个信息会被注入到每个集群内部的pod中: KUBERNETES_SERVICE_HOST=10.96.0.1、KUBERNETES_SERVICE_PORT=443、KUBERNETES_SERVICE_PORT_HTTPS=443) 永远使用&amp;quot;--service-cluster-ip-range&amp;quot;定义的CIDR的第一个ip地址 svc以及对应的endpoints都是由master controller(api-server二进制文件在最开始启动的controller之一)管控 RunKubernetesService()是一个循环，里面的逻辑包含支持UpdateKubernetesService()更新这个svc信息，ReconcileEndpoints(...endpointPorts []corev1.EndpointPort...)来更新endpoint，也就是一般3个master的信息；不过有时候看到的endpoint可能只有一个ip，这可能是云厂商传入的master lb层 参考链接: https://networkop.co.uk/post/2020-06-kubernetes-default/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>谈谈kubernetes 证书认证那些事儿</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes/kubernetes-certificate/</link>
      <pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes/kubernetes-certificate/</guid>
      <description>
        
          
            kubernetes各个组件都是加密通信的, 那么都有哪些证书、各个证书怎么交互、这些证书什么时候过期，这个就变得至关重要; 本文引用了一些其他网络内容(均已附上原文链接)，并适当补充完善，用于让新手完善熟悉kubernetes证书体系(如有侵权联系邮箱可以删除)。
一、数字证书原理 1.1 传统非对称加密 1message --&amp;gt; (公钥加密) --&amp;gt; || 传输 || --&amp;gt; (私钥解密) --&amp;gt; message 注意:
1.这里与数字证书认证相反，是公钥加密私钥解密
2.公钥私钥需要是一个秘钥对
1.2 哈希函数 1message --&amp;gt; H(message) --&amp;gt; Hash message 处理加入一个随机数，然后得出结果(加盐); 可以有效缓解在输入值是一个有效的集合，哈希值也是固定长度被别人‘试’出来的几率
1message --&amp;gt; H(R|message) --&amp;gt; Hash message 1.3 数字证书 1.3.1 数字签名 数字签名；把数据根据私钥/哈希进行加密，然后必须要对应的公钥来进行解密认证才能确保数据安全。前半句的加密过程就叫做 &#39;数字签名&#39;
1.3.2 数字证书认证过程 Alice 想要通过证书加密让 Bob 安全读到自己的信息流程如下：
Alice 在本地生成 Private Key 和 CSR（Certificate Signing Request）。CSR 中包含了 Alice 的公钥和姓名，机构、地址等身份信息。 Alice 使用该 CSR 向证书机构发起数字证书申请。 证书机构验证 Alice 的身份后，使用 CSR 中的信息生成数字证书，并使用自己的 CA 根证书对应的私钥对该证书签名。 Alice 使用自己的 Private Key 对合同进行签名，然后将签名后的合同和自己的证书一起并发送给 Bob。 Bob 使用操作系统中自带的证书机构根证书中的公钥来验证 Alice 证书中的签名，以确认 Alice 的身份和公钥。(使用内置根证书确认身份并获取Alice证书) Alice 的证书验证成功后，Bob 使用 Alice 证书中的公钥来验证合同中数字签名。(使用刚刚获取的Alice证书(公钥)解析Alice发送的内容) 合同数字签名通过验证，可以证明该合同为 Alice 本人发送，并且中间未被第三方篡改过。 注意：
          
          
        
      </description>
    </item>
    
    <item>
      <title>浅谈kubernetes ingress机制</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes/ingress-mechanism/</link>
      <pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes/ingress-mechanism/</guid>
      <description>
        
          
            ingress在将流量发往后端的时候是不经过kube-proxy的，ingress controller会直接和kube-apiserver进行交互，然后获取pod endpoints和service的对应关系，进行轮询，负载均衡到后端节点。
一、验证试验 从集群中删除kube-proxy，查看通过ingress的方式是否可以访问成功?
1# 实验中选择的是&amp;#34;iptables模式的kube-proxy&amp;#34; 2# 首先关闭kube-proxy 3$ sudo systemctl stop kube-proxy 4 5# 查看服务，ClusterIP的端口是8080，NodePort的端口是30948 6$ ks 7NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 8example-test NodePort 10.0.0.226 &amp;lt;none&amp;gt; 8080:30948/TCP 18h 9traefik NodePort 10.0.0.85 &amp;lt;none&amp;gt; 9000:30001/TCP,80:30002/TCP,443:31990/TCP 19h 10 11# 查看与&amp;#34;10.0.0.226&amp;#34;有关的iptables条目，得知访问&amp;#34;10.0.0.226&amp;#34;且目的端口是&amp;#34;8080&amp;#34;的流量会发送给KUBE-SVC-KNYNFDNL67C7KAZZ链 12$ sudo iptables -S -t nat | grep 10.0.0.226 13-A KUBE-SERVICES ! -s 10.0.0.0/24 -d 10.0.0.226/32 -p tcp -m comment --comment &amp;#34;traffic-dispatcher/example-test:port-8080 cluster IP&amp;#34; -m tcp --dport 8080 -j KUBE-MARK-MASQ 14-A KUBE-SERVICES -d 10.
          
          
        
      </description>
    </item>
    
    <item>
      <title>client-go watch接口隔一段时间自动退出怎么办？</title>
      <link>https://zhangsiming-blyq.github.io/post/golang/retrywatcher/</link>
      <pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/golang/retrywatcher/</guid>
      <description>
        
          
            在使用client-go的watch接口时候碰到异常退出问题，查了一下google没有多少信息，于是扒了一下代码，把自己踩的坑记录下来方便以后自查自纠。
使用client-go watch接口 💡 全局的mycluster都等于*kubernetes.Clientset 1. 如何watch 由于kubernetes整合了etcd的watch功能，我们可以通过watch操作去建立一个长连接，不断的接收数据；这种方式要优于普通的反复轮询请求，降低server端的压力;
使用client-go调用对应对象的Watch()方法之后，会返回一个watch.Event对象，可以对其使用ResultChan()接受watch到的对象。
1pod, err := mycluster.Clusterclientset.CoreV1().Pods(appNamespace).Watch(context.TODO(), metav1.ListOptions{LabelSelector: label}) 2if err != nil { 3 log.Error(err) 4} 5... 6event, ok := &amp;lt;-pod.ResultChan() 7if !ok { 8 log.Error(err) 9} 异常：watch接口自动断开 1. 现象 在使用过程中，watch操作持续一段时间就会自动断开
2. 排查 我们进入watch包里面找到streamwatcher.go，其中节选了一些重要片段：
1type StreamWatcher struct { 2	sync.Mutex 3	source Decoder 4	reporter Reporter 5	result chan Event 6	stopped bool 7} 8... 9func NewStreamWatcher(d Decoder, r Reporter) *StreamWatcher { 10	sw := &amp;amp;StreamWatcher{ 11	source: d, 12	reporter: r, 13	// It&amp;#39;s easy for a consumer to add buffering via an extra 14	// goroutine/channel, but impossible for them to remove it, 15	// so nonbuffered is better.
          
          
        
      </description>
    </item>
    
    <item>
      <title>prometheus 监测 kubernetes 控制平面</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes/monitor-control-plane/</link>
      <pubDate>Mon, 11 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes/monitor-control-plane/</guid>
      <description>
        
          
            对于 kubernetes 集群的控制平面组件，监控是必要的, 他可以帮助我们获取到集群的整体负载压力，并在核心组件出问题的时候配合告警让管理员及时发现问题，及时处理，更稳定的保证集群的生命周期。
一、Prometheus 如何自动发现 Kubernetes Metrics 接口? prometheus 收集 kubernetes 集群中的指标有两种方式，一种是使用 crd(servicemonitors.monitoring.coreos.com)的方式，主要通过标签匹配；另一种是通过 scrape_config，支持根据配置好的&amp;quot;relabel_configs&amp;quot;中的具体目标, 进行不断拉取(拉取间隔为&amp;quot;scrape_interval&amp;quot;)
配置权限： k8s 中 RBAC 支持授权资源对象的权限，比如可以 get、list、watch 集群中的 pod，还支持直接赋予对象访问 api 路径的权限，比如获取/healthz, /api 等, 官方对于 non_resource_urls 的解释如下：
non_resource_urls - (Optional) NonResourceURLs is a set of partial urls that a user should have access to. *s are allowed, but only as the full, final step in the path Since non-resource URLs are not namespaced, this field is only applicable for ClusterRoles referenced from a ClusterRoleBinding.
          
          
        
      </description>
    </item>
    
    <item>
      <title>浅谈 containerd</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes/containerd/</link>
      <pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes/containerd/</guid>
      <description>
        
          
            伴随着kubernetes对docker的弃用，containerd开始进入大众视野；相比于kubelet中集成docker-shim连接docker，docker再次条用containerd去管理容器，直接使用containerd可以通过原生CRI接口的调用实现容器runtime，简化了调用链路，更加的灵活可靠。
一、安装使用 ctr 管理 containerd 1# Install Dependent Libraries 2$ sudo apt-get update 3$ sudo apt-get install libseccomp2 4 5# 下载 6# 目前是下载的1.5.2 7$ wget https://github.com/containerd/containerd/releases/download/v${VERSION}/cri-containerd-cni-${VERSION}-linux-amd64.tar.gz 8 9# 安装 10$ sudo tar --no-overwrite-dir -C / -xzf cri-containerd-cni-${VERSION}-linux-amd64.tar.gz 11# 初始化containerd配置 12$ containerd config default &amp;gt; /etc/containerd/config.toml 13# 修改默认的sandbox_image 14$ vim /etc/containerd/config.toml 15... 16sandbox_image = &amp;#34;registry.cn-beijing.aliyuncs.com/shannonai-k8s/pause:3.1&amp;#34; 17... 18 19# 启动服务 20sudo systemctl daemon-reload 21sudo systemctl start containerd 22 23# 查看版本 24$ ctr version 25Client: 26 Version: 1.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>

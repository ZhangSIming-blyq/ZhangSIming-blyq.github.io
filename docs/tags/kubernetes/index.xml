<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on bailiyingqi&#39;s blog</title>
    <link>https://zhangsiming-blyq.github.io/tags/kubernetes/</link>
    <description>Recent content in kubernetes on bailiyingqi&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Mon, 12 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://zhangsiming-blyq.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>谈谈kubernetes 证书认证那些事儿</title>
      <link>https://zhangsiming-blyq.github.io/post/kubernetes-certificate/</link>
      <pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/kubernetes-certificate/</guid>
      <description>
        
          
            kubernetes各个组件都是加密通信的, 那么都有哪些证书、各个证书怎么交互、这些证书什么时候过期，这个就变得至关重要; 本文引用了一些其他网络内容(均已附上原文链接)，并适当补充完善，用于让新手完善熟悉kubernetes证书体系(如有侵权联系邮箱可以删除)。
一、数字证书原理 1.1 传统非对称加密 1message --&amp;gt; (公钥加密) --&amp;gt; || 传输 || --&amp;gt; (私钥解密) --&amp;gt; message 注意:
1.这里与数字证书认证相反，是公钥加密私钥解密
2.公钥私钥需要是一个秘钥对
1.2 哈希函数 1message --&amp;gt; H(message) --&amp;gt; Hash message 处理加入一个随机数，然后得出结果(加盐); 可以有效缓解在输入值是一个有效的集合，哈希值也是固定长度被别人‘试’出来的几率
1message --&amp;gt; H(R|message) --&amp;gt; Hash message 1.3 数字证书 1.3.1 数字签名 数字签名；把数据根据私钥/哈希进行加密，然后必须要对应的公钥来进行解密认证才能确保数据安全。前半句的加密过程就叫做 &#39;数字签名&#39;
1.3.2 数字证书认证过程 Alice 想要通过证书加密让 Bob 安全读到自己的信息流程如下：
Alice 在本地生成 Private Key 和 CSR（Certificate Signing Request）。CSR 中包含了 Alice 的公钥和姓名，机构、地址等身份信息。 Alice 使用该 CSR 向证书机构发起数字证书申请。 证书机构验证 Alice 的身份后，使用 CSR 中的信息生成数字证书，并使用自己的 CA 根证书对应的私钥对该证书签名。 Alice 使用自己的 Private Key 对合同进行签名，然后将签名后的合同和自己的证书一起并发送给 Bob。 Bob 使用操作系统中自带的证书机构根证书中的公钥来验证 Alice 证书中的签名，以确认 Alice 的身份和公钥。(使用内置根证书确认身份并获取Alice证书) Alice 的证书验证成功后，Bob 使用 Alice 证书中的公钥来验证合同中数字签名。(使用刚刚获取的Alice证书(公钥)解析Alice发送的内容) 合同数字签名通过验证，可以证明该合同为 Alice 本人发送，并且中间未被第三方篡改过。 注意：
          
          
        
      </description>
    </item>
    
    <item>
      <title>浅谈kubernetes ingress机制</title>
      <link>https://zhangsiming-blyq.github.io/post/ingress-mechanism/</link>
      <pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/ingress-mechanism/</guid>
      <description>
        
          
            ingress在将流量发往后端的时候是不经过kube-proxy的，ingress controller会直接和kube-apiserver进行交互，然后获取pod endpoints和service的对应关系，进行轮询，负载均衡到后端节点。
一、验证试验 从集群中删除kube-proxy，查看通过ingress的方式是否可以访问成功?
1# 实验中选择的是&amp;#34;iptables模式的kube-proxy&amp;#34; 2# 首先关闭kube-proxy 3$ sudo systemctl stop kube-proxy 4 5# 查看服务，ClusterIP的端口是8080，NodePort的端口是30948 6$ ks 7NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 8example-test NodePort 10.0.0.226 &amp;lt;none&amp;gt; 8080:30948/TCP 18h 9traefik NodePort 10.0.0.85 &amp;lt;none&amp;gt; 9000:30001/TCP,80:30002/TCP,443:31990/TCP 19h 10 11# 查看与&amp;#34;10.0.0.226&amp;#34;有关的iptables条目，得知访问&amp;#34;10.0.0.226&amp;#34;且目的端口是&amp;#34;8080&amp;#34;的流量会发送给KUBE-SVC-KNYNFDNL67C7KAZZ链 12$ sudo iptables -S -t nat | grep 10.0.0.226 13-A KUBE-SERVICES ! -s 10.0.0.0/24 -d 10.0.0.226/32 -p tcp -m comment --comment &amp;#34;traffic-dispatcher/example-test:port-8080 cluster IP&amp;#34; -m tcp --dport 8080 -j KUBE-MARK-MASQ 14-A KUBE-SERVICES -d 10.
          
          
        
      </description>
    </item>
    
    <item>
      <title>client-go watch接口隔一段时间自动退出怎么办？</title>
      <link>https://zhangsiming-blyq.github.io/post/retrywatcher/</link>
      <pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/retrywatcher/</guid>
      <description>
        
          
            在使用client-go的watch接口时候碰到异常退出问题，查了一下google没有多少信息，于是扒了一下代码，把自己踩的坑记录下来方便以后自查自纠。
使用client-go watch接口 💡 全局的mycluster都等于*kubernetes.Clientset 1. 如何watch 由于kubernetes整合了etcd的watch功能，我们可以通过watch操作去建立一个长连接，不断的接收数据；这种方式要优于普通的反复轮询请求，降低server端的压力;
使用client-go调用对应对象的Watch()方法之后，会返回一个watch.Event对象，可以对其使用ResultChan()接受watch到的对象。
1pod, err := mycluster.Clusterclientset.CoreV1().Pods(appNamespace).Watch(context.TODO(), metav1.ListOptions{LabelSelector: label}) 2if err != nil { 3 log.Error(err) 4} 5... 6event, ok := &amp;lt;-pod.ResultChan() 7if !ok { 8 log.Error(err) 9} 异常：watch接口自动断开 1. 现象 在使用过程中，watch操作持续一段时间就会自动断开
2. 排查 我们进入watch包里面找到streamwatcher.go，其中节选了一些重要片段：
1type StreamWatcher struct { 2	sync.Mutex 3	source Decoder 4	reporter Reporter 5	result chan Event 6	stopped bool 7} 8... 9func NewStreamWatcher(d Decoder, r Reporter) *StreamWatcher { 10	sw := &amp;amp;StreamWatcher{ 11	source: d, 12	reporter: r, 13	// It&amp;#39;s easy for a consumer to add buffering via an extra 14	// goroutine/channel, but impossible for them to remove it, 15	// so nonbuffered is better.
          
          
        
      </description>
    </item>
    
    <item>
      <title>prometheus 监测 kubernetes 控制平面</title>
      <link>https://zhangsiming-blyq.github.io/post/monitor-control-plane/</link>
      <pubDate>Mon, 11 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/monitor-control-plane/</guid>
      <description>
        
          
            对于 kubernetes 集群的控制平面组件，监控是必要的, 他可以帮助我们获取到集群的整体负载压力，并在核心组件出问题的时候配合告警让管理员及时发现问题，及时处理，更稳定的保证集群的生命周期。
一、Prometheus 如何自动发现 Kubernetes Metrics 接口? prometheus 收集 kubernetes 集群中的指标有两种方式，一种是使用 crd(servicemonitors.monitoring.coreos.com)的方式，主要通过标签匹配；另一种是通过 scrape_config，支持根据配置好的&amp;quot;relabel_configs&amp;quot;中的具体目标, 进行不断拉取(拉取间隔为&amp;quot;scrape_interval&amp;quot;)
配置权限： k8s 中 RBAC 支持授权资源对象的权限，比如可以 get、list、watch 集群中的 pod，还支持直接赋予对象访问 api 路径的权限，比如获取/healthz, /api 等, 官方对于 non_resource_urls 的解释如下：
non_resource_urls - (Optional) NonResourceURLs is a set of partial urls that a user should have access to. *s are allowed, but only as the full, final step in the path Since non-resource URLs are not namespaced, this field is only applicable for ClusterRoles referenced from a ClusterRoleBinding.
          
          
        
      </description>
    </item>
    
    <item>
      <title>浅谈 containerd</title>
      <link>https://zhangsiming-blyq.github.io/post/containerd/</link>
      <pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zhangsiming-blyq.github.io/post/containerd/</guid>
      <description>
        
          
            伴随着kubernetes对docker的弃用，containerd开始进入大众视野；相比于kubelet中集成docker-shim连接docker，docker再次条用containerd去管理容器，直接使用containerd可以通过原生CRI接口的调用实现容器runtime，简化了调用链路，更加的灵活可靠。
一、安装使用 ctr 管理 containerd 1# Install Dependent Libraries 2$ sudo apt-get update 3$ sudo apt-get install libseccomp2 4 5# 下载 6# 目前是下载的1.5.2 7$ wget https://github.com/containerd/containerd/releases/download/v${VERSION}/cri-containerd-cni-${VERSION}-linux-amd64.tar.gz 8 9# 安装 10$ sudo tar --no-overwrite-dir -C / -xzf cri-containerd-cni-${VERSION}-linux-amd64.tar.gz 11# 初始化containerd配置 12$ containerd config default &amp;gt; /etc/containerd/config.toml 13# 修改默认的sandbox_image 14$ vim /etc/containerd/config.toml 15... 16sandbox_image = &amp;#34;registry.cn-beijing.aliyuncs.com/shannonai-k8s/pause:3.1&amp;#34; 17... 18 19# 启动服务 20sudo systemctl daemon-reload 21sudo systemctl start containerd 22 23# 查看版本 24$ ctr version 25Client: 26 Version: 1.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
